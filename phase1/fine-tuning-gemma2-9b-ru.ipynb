{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":205088,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":56633,"modelId":78150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":15088.105996,"end_time":"2024-12-28T00:50:41.125384","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-27T20:39:13.019388","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Gemma for Russian Language\nThis notebook demonstrates the fine-tuning of the Gemma model on Russian datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the Russian dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_ru)","metadata":{}},{"cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used gemma2_9b_en","metadata":{}},{"cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:41:06.592716Z","iopub.status.busy":"2024-12-27T20:41:06.592453Z","iopub.status.idle":"2024-12-27T20:41:15.097439Z","shell.execute_reply":"2024-12-27T20:41:15.096370Z"},"papermill":{"duration":8.511693,"end_time":"2024-12-27T20:41:15.099114","exception":false,"start_time":"2024-12-27T20:41:06.587421","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"cell_type":"code","source":"import os\n# from kaggle_secrets import UserSecretsClient\n# from google.colab import userdata\n#import getpass\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:41:15.108816Z","iopub.status.busy":"2024-12-27T20:41:15.108476Z","iopub.status.idle":"2024-12-27T20:41:15.112640Z","shell.execute_reply":"2024-12-27T20:41:15.111867Z"},"papermill":{"duration":0.011096,"end_time":"2024-12-27T20:41:15.114417","exception":false,"start_time":"2024-12-27T20:41:15.103321","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:41:15.123030Z","iopub.status.busy":"2024-12-27T20:41:15.122829Z","iopub.status.idle":"2024-12-27T20:41:28.414884Z","shell.execute_reply":"2024-12-27T20:41:28.412729Z"},"papermill":{"duration":13.298387,"end_time":"2024-12-27T20:41:28.416305","exception":false,"start_time":"2024-12-27T20:41:15.117918","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Load and Explore Russian Dataset\nWe are using the `allenai/c4` dataset with Russian (`ru`) data. The dataset is loaded in streaming mode for efficient handling of large datasets.\n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"cell_type":"markdown","source":"Since we want to fine-tune the Gemma 2 9b model for adapting to the Russian language, we need a good amount of high-quality Russian text corpus. For that, we use the 'C4' dataset, which is a multilingual text dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/allenai/c4)  \n\n**Dataset Summary (from the original dataset page):**  \nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on the Common Crawl dataset: [https://commoncrawl.org](https://commoncrawl.org).\n\nThis is the processed version of Google's C4 dataset.","metadata":{}},{"cell_type":"code","source":"data = load_dataset(\"allenai/c4\", \"ru\", streaming=True)","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:41:28.427791Z","iopub.status.busy":"2024-12-27T20:41:28.427542Z","iopub.status.idle":"2024-12-27T20:42:09.766722Z","shell.execute_reply":"2024-12-27T20:42:09.765676Z"},"papermill":{"duration":41.348252,"end_time":"2024-12-27T20:42:09.769118","exception":false,"start_time":"2024-12-27T20:41:28.420866","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data = []\nfor i, example in enumerate(iter(data[\"train\"])):\n    if i >= 2:  # Change this number to get more examples\n        break\n    sample_data.append(example[\"text\"])\n\nprint(\"Sample Russian Data:\")\nfor i, text in enumerate(sample_data):\n    print(f\"Example {i + 1}:\", text[:500])  # Print the first 500 characters to get a preview","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:42:09.779162Z","iopub.status.busy":"2024-12-27T20:42:09.778907Z","iopub.status.idle":"2024-12-27T20:42:22.216329Z","shell.execute_reply":"2024-12-27T20:42:22.215182Z"},"papermill":{"duration":12.445,"end_time":"2024-12-27T20:42:22.218391","exception":false,"start_time":"2024-12-27T20:42:09.773391","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the maximum number of examples for training and validation\nmax_train_examples = 5000\nmax_val_examples = 100\n\n# Create a plain-text list from a subset of the dataset\n# Load data subsets\ntrain_text_data = [example[\"text\"] for example in itertools.islice(data[\"train\"], max_train_examples)]\nval_text_data = [example[\"text\"] for example in itertools.islice(data[\"validation\"], max_val_examples)]\n\n# Check the first example to ensure loading is correct\n#print(\"First training example:\", train_text_data[0])\n#print(\"First validation example:\", val_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:42:22.228925Z","iopub.status.busy":"2024-12-27T20:42:22.228662Z","iopub.status.idle":"2024-12-27T20:42:23.542702Z","shell.execute_reply":"2024-12-27T20:42:23.541631Z"},"papermill":{"duration":1.321759,"end_time":"2024-12-27T20:42:23.544537","exception":false,"start_time":"2024-12-27T20:42:22.222778","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(val_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:42:23.555713Z","iopub.status.busy":"2024-12-27T20:42:23.555450Z","iopub.status.idle":"2024-12-27T20:42:24.123672Z","shell.execute_reply":"2024-12-27T20:42:24.122562Z"},"papermill":{"duration":0.575382,"end_time":"2024-12-27T20:42:24.125589","exception":false,"start_time":"2024-12-27T20:42:23.550207","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training\nWe configure model parallelism using TPUs to handle the large-scale Gemma 9b model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n","metadata":{}},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_en/3\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:42:24.136054Z","iopub.status.busy":"2024-12-27T20:42:24.135767Z","iopub.status.idle":"2024-12-27T20:44:28.481296Z","shell.execute_reply":"2024-12-27T20:44:28.480352Z"},"papermill":{"duration":124.35303,"end_time":"2024-12-27T20:44:28.483295","exception":false,"start_time":"2024-12-27T20:42:24.130265","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### testing if model is loaded correctly:","metadata":{}},{"cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<48}  {str(variable.shape):<14}  {str(variable.value.sharding.spec)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:44:28.494994Z","iopub.status.busy":"2024-12-27T20:44:28.494756Z","iopub.status.idle":"2024-12-27T20:44:28.499899Z","shell.execute_reply":"2024-12-27T20:44:28.498645Z"},"papermill":{"duration":0.012589,"end_time":"2024-12-27T20:44:28.501226","exception":false,"start_time":"2024-12-27T20:44:28.488637","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:44:28.512978Z","iopub.status.busy":"2024-12-27T20:44:28.512753Z","iopub.status.idle":"2024-12-27T20:44:28.517507Z","shell.execute_reply":"2024-12-27T20:44:28.516103Z"},"papermill":{"duration":0.012294,"end_time":"2024-12-27T20:44:28.518660","exception":false,"start_time":"2024-12-27T20:44:28.506366","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"Привет! Как дела сегодня? Расскажи что-нибудь интересное, что ты недавно узнал.\", # Greeting and request for recent information\n    \"Что ты знаешь об истории Ренессанса в Италии? Можешь объяснить его влияние на искусство и науку?\", # Request for historical knowledge and cultural impact\n    \"Напиши небольшое стихотворение на русском языке об осеннем пейзаже.\", # Request for poetic creativity\n    \"Объясни простыми словами, как работает искусственный интеллект и каковы его наиболее распространенные применения в России.\", # Request for technical explanation and geographical context\n    \"Если кто-то скажет: 'За двумя зайцами погонишься, ни одного не поймаешь', что это будет значить? В какой ситуации можно использовать это выражение?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:44:28.529861Z","iopub.status.busy":"2024-12-27T20:44:28.529642Z","iopub.status.idle":"2024-12-27T20:48:29.868459Z","shell.execute_reply":"2024-12-27T20:48:29.867442Z"},"papermill":{"duration":241.351248,"end_time":"2024-12-27T20:48:29.874594","exception":false,"start_time":"2024-12-27T20:44:28.523346","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:48:29.887653Z","iopub.status.busy":"2024-12-27T20:48:29.887351Z","iopub.status.idle":"2024-12-27T20:48:30.748012Z","shell.execute_reply":"2024-12-27T20:48:30.746434Z"},"papermill":{"duration":0.869748,"end_time":"2024-12-27T20:48:30.750060","exception":false,"start_time":"2024-12-27T20:48:29.880312","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20,\n)\n\nwandb.init(project = \"fine-tuning-gemma2_9b_ru\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-27T20:48:30.765023Z","iopub.status.busy":"2024-12-27T20:48:30.764787Z","iopub.status.idle":"2024-12-27T20:48:32.611536Z","shell.execute_reply":"2024-12-27T20:48:32.610621Z"},"papermill":{"duration":1.856821,"end_time":"2024-12-27T20:48:32.613670","exception":false,"start_time":"2024-12-27T20:48:30.756849","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 9: Plotting the results:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 5))\n\n# Plotting Loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T00:30:04.541135Z","iopub.status.busy":"2024-12-28T00:30:04.540782Z","iopub.status.idle":"2024-12-28T00:30:04.969516Z","shell.execute_reply":"2024-12-28T00:30:04.968369Z"},"papermill":{"duration":1.979063,"end_time":"2024-12-28T00:30:04.971048","exception":false,"start_time":"2024-12-28T00:30:02.991985","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NOTE:\nSince we are using a small size for our ```val_data```, the metrics obtained from it may not be very reliable.\nAs you can see the train loss and accuracy keep improving but the validation metrics are not, this suggest possible overfitting, How ever in the next cell we test the model and we see that its performance actually improved, So we can say that there are other reasons behind the difference between val and train loss.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"cell_type":"code","source":"test_prompts = [\n    \"Привет! Как дела сегодня? Расскажи что-нибудь интересное, что ты недавно узнал.\", # Greeting and request for recent information\n    \"Что ты знаешь об истории Ренессанса в Италии? Можешь объяснить его влияние на искусство и науку?\", # Request for historical knowledge and cultural impact\n    \"Напиши небольшое стихотворение на русском языке об осеннем пейзаже.\", # Request for poetic creativity\n    \"Объясни простыми словами, как работает искусственный интеллект и каковы его наиболее распространенные применения в России.\", # Request for technical explanation and geographical context\n    \"Если кто-то скажет: 'За двумя зайцами погонишься, ни одного не поймаешь', что это будет значить? В какой ситуации можно использовать это выражение?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T00:30:08.032691Z","iopub.status.busy":"2024-12-28T00:30:08.032316Z","iopub.status.idle":"2024-12-28T00:34:59.794826Z","shell.execute_reply":"2024-12-28T00:34:59.793767Z"},"papermill":{"duration":294.868853,"end_time":"2024-12-28T00:35:01.371055","exception":false,"start_time":"2024-12-28T00:30:06.502202","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma model and used plain text in target language, we can expect some randomness and other things from its answers, as it has not been fine-tuned on instruct datasets(We will look into this in the next phase).","metadata":{}},{"cell_type":"markdown","source":"### Step 11: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_9b_ru\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_9b_ru\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T00:35:04.544095Z","iopub.status.busy":"2024-12-28T00:35:04.543687Z","iopub.status.idle":"2024-12-28T00:36:18.979613Z","shell.execute_reply":"2024-12-28T00:36:18.978228Z"},"papermill":{"duration":77.563434,"end_time":"2024-12-28T00:36:20.575748","exception":false,"start_time":"2024-12-28T00:35:03.012314","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_9b_ru\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Russian text generation. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training","metadata":{}}]}