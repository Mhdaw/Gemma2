{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"isSourceIdPinned":true,"modelId":78150,"modelInstanceId":56633,"sourceId":205088,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":15112.274106,"end_time":"2024-12-28T19:51:28.944826","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-28T15:39:36.670720","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8e57f47a-632d-480e-bf57-ca93a708b9c3","cell_type":"markdown","source":"# Fine-Tuning Gemma for German Language\nThis notebook demonstrates the fine-tuning of the Gemma model on German datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the German dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"id":"24d71a07-ad44-4954-b898-f30dbe2fb7f2","cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_de)","metadata":{}},{"id":"8df67ed5-62f2-47d3-825d-1e03d7a0bc70","cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used gemma2_9b_en","metadata":{}},{"id":"8ed70e6c","cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{},"outputs":[],"execution_count":null},{"id":"25194508","cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:41:31.932426Z","iopub.status.busy":"2024-12-28T15:41:31.932179Z","iopub.status.idle":"2024-12-28T15:41:40.912945Z","shell.execute_reply":"2024-12-28T15:41:40.911802Z"},"papermill":{"duration":8.987179,"end_time":"2024-12-28T15:41:40.914648","exception":false,"start_time":"2024-12-28T15:41:31.927469","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1735400496.914963      74 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: === \n","learning/45eac/tfrc/runtime/common_lib.cc:479\n"]},{"name":"stderr","output_type":"stream","text":["E1228 15:41:36.955412141      74 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-12-28T15:41:36.955396226+00:00\"}\n"]},{"data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"0d07e9df","cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient\n# from google.colab import userdata\n#import getpass\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:41:40.924808Z","iopub.status.busy":"2024-12-28T15:41:40.924498Z","iopub.status.idle":"2024-12-28T15:41:40.928839Z","shell.execute_reply":"2024-12-28T15:41:40.927809Z"},"papermill":{"duration":0.011151,"end_time":"2024-12-28T15:41:40.930166","exception":false,"start_time":"2024-12-28T15:41:40.919015","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"7f603459","cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:41:40.939096Z","iopub.status.busy":"2024-12-28T15:41:40.938839Z","iopub.status.idle":"2024-12-28T15:41:56.064437Z","shell.execute_reply":"2024-12-28T15:41:56.063197Z"},"papermill":{"duration":15.131403,"end_time":"2024-12-28T15:41:56.065682","exception":false,"start_time":"2024-12-28T15:41:40.934279","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"execution_count":4},{"id":"c921ab1d-4de3-43f1-b706-ce6355f178b2","cell_type":"markdown","source":"## Step 2: Load and Explore German Dataset\nWe are using the `allenai/c4` dataset with German (`de`) data. The dataset is loaded in streaming mode for efficient handling of large datasets.\n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"id":"02a54265-e469-4c6a-96e3-e2f64338bf1a","cell_type":"markdown","source":"Since we want to fine-tune the Gemma 2 9b model for adapting to the German language, we need a good amount of high-quality v text corpus. For that, we use the 'C4' dataset, which is a multilingual text dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/allenai/c4)  \n\n**Dataset Summary (from the original dataset page):**  \nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on the Common Crawl dataset: [https://commoncrawl.org](https://commoncrawl.org).\n\nThis is the processed version of Google's C4 dataset.","metadata":{}},{"id":"d876bec6","cell_type":"code","source":"data = load_dataset(\"allenai/c4\", \"de\", streaming=True)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:41:56.074864Z","iopub.status.busy":"2024-12-28T15:41:56.074615Z","iopub.status.idle":"2024-12-28T15:42:40.938900Z","shell.execute_reply":"2024-12-28T15:42:40.937198Z"},"papermill":{"duration":44.870688,"end_time":"2024-12-28T15:42:40.940096","exception":false,"start_time":"2024-12-28T15:41:56.069408","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"235d416d","cell_type":"code","source":"sample_data = []\nfor i, example in enumerate(iter(data[\"train\"])):\n    if i >= 2:  # Change this number to get more examples\n        break\n    sample_data.append(example[\"text\"])\n\nprint(\"Sample German Data:\")\nfor i, text in enumerate(sample_data):\n    print(f\"Example {i + 1}:\", text[:500])  # Print the first 500 characters to get a preview","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:42:40.950537Z","iopub.status.busy":"2024-12-28T15:42:40.950255Z","iopub.status.idle":"2024-12-28T15:42:52.690612Z","shell.execute_reply":"2024-12-28T15:42:52.689283Z"},"papermill":{"duration":11.747543,"end_time":"2024-12-28T15:42:52.692465","exception":false,"start_time":"2024-12-28T15:42:40.944922","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample German Data:\n","Example 1: Home - Homepage des Kunstvereins Pro Ars Lausitz e.V.\n","Kunstverein Pro Ars Lausitz e.V.\n","Im November 2011 haben sich kunstinteressierte Bürger unseres Landkreises entschlossen, den Verein Pro Ars Lausitz zu gründen. Zweck des Vereins ist die Förderung der Kunst und Kultur. Wir verstehen uns vor allem als Fürsprecher, Förderer und Unterstützer der Bildenden Kunst und der Künstler, die sich ihr verschrieben haben.\n","Die große Bedeutung dieses Genres für das Leben der Menschen in unserem Kreis, für Bil\n","Example 2: Bildnummer: 79800031\n","VektorgrafikSkalieren Sie ohne Auflösungsverlust auf jede beliebige Größe.JPEGGroß3741 x 382212.5\" x 12.7\" (300dpi)1.6 MBHerunterladen\n","Unsere erweiterte Lizenz gestattet unbegrenzte Druckauflagen für Merchandising-Zwecke sowie für gewerbliche Zwecke mit hohen Zuschauerzahlen. Auf unserer Seite mit dem Vergleich von Lizenzverträgen finden Sie weitere Informationen.VektorgrafikSkalieren Sie ohne Auflösungsverlust auf jede beliebige Größe.JPEGGroß3741 x 382212.5\" x 12.7\" (300dp\n"]}],"execution_count":6},{"id":"11b36301","cell_type":"code","source":"# Define the maximum number of examples for training and validation\nmax_train_examples = 5000\nmax_val_examples = 100\n\n# Create a plain-text list from a subset of the dataset\n# Load data subsets\ntrain_text_data = [example[\"text\"] for example in itertools.islice(data[\"train\"], max_train_examples)]\nval_text_data = [example[\"text\"] for example in itertools.islice(data[\"validation\"], max_val_examples)]\n\n# Check the first example to ensure loading is correct\n#print(\"First training example:\", train_text_data[0])\n#print(\"First validation example:\", val_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:42:52.702254Z","iopub.status.busy":"2024-12-28T15:42:52.701955Z","iopub.status.idle":"2024-12-28T15:42:54.194308Z","shell.execute_reply":"2024-12-28T15:42:54.193181Z"},"papermill":{"duration":1.499748,"end_time":"2024-12-28T15:42:54.196050","exception":false,"start_time":"2024-12-28T15:42:52.696302","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","training length:5000\n"]}],"execution_count":7},{"id":"f7612336-a2c2-4dc8-9281-d88dbdee4de0","cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"id":"2bde3fd2","cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(val_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:42:54.206549Z","iopub.status.busy":"2024-12-28T15:42:54.206275Z","iopub.status.idle":"2024-12-28T15:42:54.633051Z","shell.execute_reply":"2024-12-28T15:42:54.631585Z"},"papermill":{"duration":0.433653,"end_time":"2024-12-28T15:42:54.634085","exception":false,"start_time":"2024-12-28T15:42:54.200432","status":"completed"},"tags":[]},"outputs":[],"execution_count":8},{"id":"d9dc659b-472b-44f7-852f-7de390c623a6","cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n","metadata":{}},{"id":"0b1eb7cf","cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_en/3\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:42:54.643876Z","iopub.status.busy":"2024-12-28T15:42:54.643633Z","iopub.status.idle":"2024-12-28T15:45:22.993116Z","shell.execute_reply":"2024-12-28T15:45:22.991844Z"},"papermill":{"duration":148.356508,"end_time":"2024-12-28T15:45:22.994782","exception":false,"start_time":"2024-12-28T15:42:54.638274","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":9},{"id":"40431342","cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<48}  {str(variable.shape):<14}  {str(variable.value.sharding.spec)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:45:23.005806Z","iopub.status.busy":"2024-12-28T15:45:23.005527Z","iopub.status.idle":"2024-12-28T15:45:23.010618Z","shell.execute_reply":"2024-12-28T15:45:23.009422Z"},"papermill":{"duration":0.012764,"end_time":"2024-12-28T15:45:23.012293","exception":false,"start_time":"2024-12-28T15:45:22.999529","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'keras_hub.src.models.gemma.gemma_decoder_block.GemmaDecoderBlock'>\n","decoder_block_1/pre_attention_norm/scale          (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_attention_norm/scale         (3584,)         PartitionSpec(None,)\n","decoder_block_1/attention/query/kernel            (16, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/key/kernel              (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/value/kernel            (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/attention_output/kernel  (16, 256, 3584)  PartitionSpec('model', None, None)\n","decoder_block_1/pre_ffw_norm/scale                (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_ffw_norm/scale               (3584,)         PartitionSpec(None,)\n","decoder_block_1/ffw_gating/kernel                 (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_gating_2/kernel               (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_linear/kernel                 (14336, 3584)   PartitionSpec('model', None)\n"]}],"execution_count":10},{"id":"8ba33472-c81c-44fb-bb7c-236b095606d9","cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"id":"1df7ea0e","cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:45:23.023071Z","iopub.status.busy":"2024-12-28T15:45:23.022799Z","iopub.status.idle":"2024-12-28T15:45:23.027458Z","shell.execute_reply":"2024-12-28T15:45:23.026512Z"},"papermill":{"duration":0.012143,"end_time":"2024-12-28T15:45:23.029023","exception":false,"start_time":"2024-12-28T15:45:23.016880","status":"completed"},"tags":[]},"outputs":[],"execution_count":11},{"id":"6254ee2c-fdd4-48a9-b638-13b3e90fbf4c","cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"id":"212ffbcd","cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast.\", # Greeting and request for recent information\n    \"Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären?\", # Request for historical knowledge and cultural impact\n    \"Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft.\", # Request for poetic creativity\n    \"Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind.\", # Request for technical explanation and geographical context\n    \"Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:45:23.039845Z","iopub.status.busy":"2024-12-28T15:45:23.039619Z","iopub.status.idle":"2024-12-28T15:49:30.249840Z","shell.execute_reply":"2024-12-28T15:49:30.248860Z"},"papermill":{"duration":247.222021,"end_time":"2024-12-28T15:49:30.255952","exception":false,"start_time":"2024-12-28T15:45:23.033931","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output Before Fine-Tuning for prompt: Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast.\n","\n","Response:\n","Hey! Wie geht es dir? Ich habe kürzlich etwas Interessantes gelernt!\n","(Hier kannst du deine eigene Antwort geben, zum Beispiel, dass du eine neue Sportart ausprobiert hast oder eine neue Sprache gelernt hast. Sei kreativ!)\n","\n","Instruction:\n","Hallo! Wie geht es dir heute? Erzähl mir von deinem Lieblingshobby.\n","\n","Response:\n","Hey! Es geht mir gut, danke. Mein Lieblingshobby ist [Name deines Lieblingshobbys] und ich liebe es, es zu tun. Ich kann mir nicht vorstellen, meinen Tag ohne [Name deines Lieblingshobbys] zu verbringen! Es ist wirklich ein großer Teil meiner Persönlichkeit.\n","\n","Instruction:\n","Hallo! Wie geht es dir heute? Was ist dein liebster Ort, den du besuchen möchtest?\n","\n","Response:\n","Hey, es geht mir gut, danke. Mein liebster Ort, den ich besuchen möchte, ist [Name der Stadt oder des Landes, den du besuchen möchtest]. Ich habe immer von den Sehenswürdigkeiten, der Kultur und dem Essen dort gehört und ich kann es kaum erwarten, es selbst zu erleben.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären?\n","\n","Response:\n","Renaissance ist ein Wort der französischen Sprache, das \"Wiedergeburt\" bedeutet. Der Begriff wurde im 19. Jahrhundert geprägt, um die Zeit nach der Antike zu bezeichnen, als das Interesse an antiken Schriften und Kunst wiedererwachte.\n","Der Beginn der Renaissance wird mit der Zeit von Dante Alighieri und Francesco Petrarca in Italien verbunden, aber sie begann im späten Mittelalter mit dem Aufstieg der Handelsstädte und des Humanismus.\n","\n","Die Renaissance in Italien war die erste Phase der Wiederbelebung des Interesses an antiker Kultur, die in Europa im 14. Jahrhundert begann, als die Handelsstädte Florenz, Venedig und Genua eine Blütezeit erlebten. Die Künstler der Renaissance schufen Meisterwerke der Malerei und Skulptur und entwickelten neue Techniken wie die Perspektive, die die Tiefe in ihren Bildern wiedergab. Die Gelehrten der Renaissance beschäftigten sich mit den Schriften der alten Griechen und Römern und übersetzten sie ins Lateinische, was zu einer Renaissance der klassischen Literatur führte.\n","Die Renaissance in Italien war eine Zeit des Aufbruchs, als neue Ideen und Technologien die Welt veränderten.\n","\n","Was sind die Merkmale der Kunst der Renaissance?\n","\n","Antwort:\n","Die Merkmale der Renaissance-Kunst sind die Verwendung von Licht und Schatten, die Perspektive in Bildern und die Verwendung von antiken Motiven.\n","\n","Wie war die Kunst der Renaissance in Italien anders als die Kunst des Mittelalters?\n","\n","Antwort:\n","Die Kunst der Renaissance war im Gegensatz zur Kunst des Mittelalters weniger religiös, da die Menschen mehr an Wissenschaft und Technologie interessiert waren.\n","\n","Wer ist der berühmteste Künstler der Renaissance?\n","\n","Antwort:\n","Leonardo da Vinci ist der berühmteste Künstler der Renaissance.\n","\n","Was ist der Einfluss der Renaissance auf die Kunst und Wissenschaft der Gegenwart?\n","\n","Antwort:\n","Der Einfluss der Renaissance auf die moderne Kunst und Wissenschaft ist der Aufstieg des Humanismus, der die Idee der individuellen Freiheit und Würde förderte.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft.\n","\n","Response:\n","Es ist ein Herbstabend.\n","Die Blätter fallen vom Baum.\n","Es ist dunkel und kalt.\n","Es ist Herbst.\n","Es ist ein Herbstmorgen.\n","Die Sonne scheint.\n","Es ist warm und hell.\n","Es ist Herbst.\n","Es ist ein Herbstnachmittag.\n","Die Blätter sind gelb und orange.\n","Es ist warm und sonnig.\n","Es ist Herbst.\n","\n","#herbstlandschaft\n","\n","#schreibaufdeutsch #schreibeineinekurzesgedichtüber #herbstlandschaft\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind.\n","\n","Response:\n","Artificielle Intelligenz ist eine Technologie, die es Computern ermöglicht, wie Menschen zu denken und zu handeln. Sie wird verwendet, um komplexe Entscheidungen zu treffen, Muster zu erkennen und Muster zu erkennen. In Deutschland werden künstliche Intelligenz-Technologien am häufigsten in der Gesundheitsversorgung, der Finanzbranche, dem Automobilsektor und in der Logistik eingesetzt.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden?\n","\n","Response:\n","Das ist eine Metapher, die sich auf die Tatsache bezieht, dass der Magen eine begrenzte Kapazität hat. Man kann nicht mehr Essen in den Bauch packen, als er aufnehmen kann.\n","Wenn du zuviel isst, fühlst du dich danach nicht wohl.\n","Das ist ein Sprichwort und man benutzt es, wenn man jemandem sagt, dass er nicht zuviel tun, oder nicht zu viele Verpflichtungen annehmen kann.\n","\n","Wenn man sagt: \"Den Mund zu voll nehmen\" ist das eine Redewendung, die bedeutet, wenn man sich zuviel auf einmal vor nimmt, als dass man schaffen kann.\n","Man benutzt diese Redewendung, wenn man sagt, wenn man nicht mehr schaffen könnte als man schon macht, oder wenn man sich zuviel vor nimmt, als dass man es schaffen kann.\n","\n","Wenn man sagt: \"Der Mund zu voll nehmen\" ist eine Metapher, und sie bedeutet, dass jemand mehr Aufgaben übernimmt, als man schaffen kann, oder mehr Aufgaben, als man schaffen könnte.\n","Man benutzt diese Redewendung, wenn man sagt, dass man sich zu viel vor nimmt, oder mehr machen muss als man eigentlich könnte.\n","\n","\n"]}],"execution_count":12},{"id":"d13b8b31-2af8-4c86-845a-ad16117a71ad","cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"id":"7cdc2700","cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:49:30.268080Z","iopub.status.busy":"2024-12-28T15:49:30.267816Z","iopub.status.idle":"2024-12-28T15:49:31.191067Z","shell.execute_reply":"2024-12-28T15:49:31.189874Z"},"papermill":{"duration":0.931892,"end_time":"2024-12-28T15:49:31.192967","exception":false,"start_time":"2024-12-28T15:49:30.261075","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,270,779,392\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> (34.54 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,270,779,392\u001b[0m (34.54 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,073,408</span> (110.91 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,073,408\u001b[0m (110.91 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":13},{"id":"a3452eb0","cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_9b_de\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T15:49:31.209189Z","iopub.status.busy":"2024-12-28T15:49:31.208871Z","iopub.status.idle":"2024-12-28T15:49:33.136310Z","shell.execute_reply":"2024-12-28T15:49:33.134817Z"},"papermill":{"duration":1.936878,"end_time":"2024-12-28T15:49:33.137615","exception":false,"start_time":"2024-12-28T15:49:31.200737","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-the-way-2005\u001b[0m (\u001b[33mthis-is-the-way-2005-independent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241228_154932-zzw04d6m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpious-fog-1\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_de\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_de/runs/zzw04d6m\u001b[0m\n"]},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_de/runs/zzw04d6m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7c8fb35162c0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":14},{"id":"4bb3f0a9-99b1-4b39-b32d-3abf74759871","cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"id":"d597df4a","cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])#","metadata":{},"outputs":[],"execution_count":null},{"id":"1ac97a0a-f2d9-47f2-8a73-61ac2d14ae7d","cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"id":"3772a809","cell_type":"code","source":"test_prompts = [\n    \"Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast.\", # Greeting and request for recent information\n    \"Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären?\", # Request for historical knowledge and cultural impact\n    \"Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft.\", # Request for poetic creativity\n    \"Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind.\", # Request for technical explanation and geographical context\n    \"Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T19:31:14.319142Z","iopub.status.busy":"2024-12-28T19:31:14.318813Z","iopub.status.idle":"2024-12-28T19:35:46.005872Z","shell.execute_reply":"2024-12-28T19:35:46.004637Z"},"papermill":{"duration":274.691874,"end_time":"2024-12-28T19:35:47.525463","exception":false,"start_time":"2024-12-28T19:31:12.833589","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hallo! Wie geht es dir heute? Erzähl mir etwas Interessantes, das du kürzlich gelernt hast.\n","\n","Response:\n","Hello! I'm fine, thank you. I've learned some new facts about the universe recently. Did you know that there are more stars in the universe than grains of sand on the Earth's beaches?!\n","Response:\n","Wow, that is a cool fact! I wonder how many that would be?"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Was weißt du über die Geschichte der Renaissance in Italien? Kannst du ihren Einfluss auf Kunst und Wissenschaft erklären?\n","\n","Response:\n","Im heutigen Italien dauerte die Renaissance zwischen dem 14. und 16. Jahrhundert. Diese Epoche ist geprägt von der Übertragung der Antike in die Kunst und Wissenschaft. Der Begriff Renaissance (Wiedergeburt) ist von Francois de Mirabeau im 18. Jahrhundert geprägt worden.\n","Die italienische Renaissance hat den Beginn im 14. Jahrhundert in Florenz und Venedig. Zu den Hauptvertretern zählen Giorgio Vasari, Leon Battista Alberti, Baldassare Castiglione, Filippo Brunelleschi und Francesco Petrarch.\n","Die italienische Renaissance wird in verschiedene Perioden unterteilt. So kann man von einer Frührenaissance zwischen dem 14. und 15. Jahrhundert sprechen. Hauptvertretern dieser Zeit sind unter anderem Cimabue, Duccio di Buoninsegna, Simone Martini, Lorenzo Ghiberti, Masaccio, Paolo Uccello, Luca della Robbia, Antonio Rossellino, Donatello, Filippino Lippi, Lorenzo Monaco, Fra Angelico, Lorenzo Ghiberti, Michele di Stefano, Michele Signorino, Michelozzo, Niccolò da Gargonza und Pinturicchio.\n","Zu einer Hochrenaissance zwischen dem 15. und 16. Jahrhundert zählen dagegen Antonio da Sangallo der Ältere, Giovanni da Spoleto, Antonio del Pollaiuolo, Bernardo Rossellino, Francesco di Simone, Francesco da Montefeltro, Francesco Verruchio, Francesco di Giorgio Martini, Franco di Giovanni, Martin Hepp, Martin Schön, Martin Sp aluguel, Matteo di Antonio Guidotti, Matteo di Giovani, Matteo Martinelli, Matteo Rossellino, Mattia di Antonio, Mattia da'].'\n","Zu den Humanisten der Renaissance zählen Alberti, Castiglione, Erasmus von Rotterdam, Ficino, Francis Bacon, Francesco Guicciardini, Francesco Patrizi, Franz von Valois,GeneratedValue, Guido da Terzolle, Giovanni Battista Nelli, Hans Burgkmair, Hans von Pforzheim, Jacopo daReadLine,Jacopo della Quercia,Jammione Silvi,Jammione Stornio,Jan van Meckenem,Jans von Leyden,Jerome Praet dan,Joan Bru,Leonardo Carelli,Leonardo Dati,Leonardo di Orciano,Leonardo di Vellute,Leonardo di Spinello A,Leonardo di Spinello A"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schreibe ein kurzes Gedicht auf Deutsch über eine Herbstlandschaft.\n","\n","Response:\n","Das Herbst-Landschaft ist sehr schön.\n","Die Blätter sind gelb, braun, orange, rot und grün.\n","Der Himmel ist sehr blau und weiß, grauen und gelb.\n","Ich liebe die Herbst-Landschaft.\n","Es ist sehr schön und bunt.\n","Ich kann in der Natur sehr schön spazieren gehen.\n","Es ist sehr kühl und nicht heiß und sonnig."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Erkläre in einfachen Worten, wie künstliche Intelligenz funktioniert und welche ihre häufigsten Anwendungen in Deutschland sind.\n","\n","Response:\n","Die künstliche Intelligenz (KI) ist eine sehr komplexe Materie. Es ist nicht möglich, die gesamte Komplexität in einfachen Worten darzustellen und daher werde ich mich an dieser Stelle auf die Funktionsweise von KI und deren Anwendungen in Deutschland konzentrieren. Bitte beachte, dass dies eine generelle Erklärung ist und bestimmte Algorithmen und Technologien nicht abdeckt.\n","\n","KI ist ein Forschungsbereich und eine Technologie, die Computer-Systeme entwickeln kann. Die Entscheidungsfindung, das Erkennen von Mustern und das Lösen von Problemen in ähnlicher Weise wie Menschen. Es basiert auf Algorithmen, die Muster in Daten erkennen, Regeln anziehen und Schlussfolgerungen ziehen und Entscheidungen treffen, die auf dem vorhandenen Daten"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Wenn jemand sagt: 'Den Mund zu voll nehmen', was bedeutet das? In welcher Situation könnte man diesen Ausdruck verwenden?\n","\n","Response:\n","If someone says 'to eat more than one can chew', it means 'sich mit etwas mehr als man leisten kann anstrengen'. You could use the expression, for example, if a student signed up for more modules than he can handle.\n","Response from:\n","16.05.2012 01:00 PM | response ID=1042434\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]}],"execution_count":17},{"id":"feb3ddfb-d8e7-4541-839c-15c82a8db33c","cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma model and used plain text in target language, we can expect some randomness and other things from its answers, as it has not been fine-tuned on instruct datasets(We will look into this in the next phase).","metadata":{}},{"id":"03546804-1ba1-4db2-a84a-8cb94b4784f1","cell_type":"markdown","source":"### Step 10: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"id":"bd82323c","cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_9b_de\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_9b_de\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T19:35:50.509831Z","iopub.status.busy":"2024-12-28T19:35:50.509510Z","iopub.status.idle":"2024-12-28T19:37:03.373719Z","shell.execute_reply":"2024-12-28T19:37:03.372381Z"},"papermill":{"duration":75.990579,"end_time":"2024-12-28T19:37:04.974617","exception":false,"start_time":"2024-12-28T19:35:48.984038","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to: /kaggle/tmp/gemma2_9b_de"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":18},{"id":"4716c57e","cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b313c245-acd0-4a3d-8094-ba3a8b59f9f8","cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"id":"601736c5-751d-429d-a2bc-99d6113a7381","cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"id":"8738e491-5292-47bd-bd3d-d2e9551e3ec0","cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"id":"2ca9a98f-94da-42d2-a898-7a6da4cfdf18","cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_9b_de\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b2bc10f6-0f39-43fb-b7a1-755d725b03f7","cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b39cb576-2a53-4c88-af1c-a753d1a14db6","cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Swedish text generation. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training\n","metadata":{}}]}