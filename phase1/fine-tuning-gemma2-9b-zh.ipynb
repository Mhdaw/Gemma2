{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":205088,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":56633,"modelId":78150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":14936.338232,"end_time":"2024-12-29T21:16:56.329611","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-29T17:07:59.991379","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Gemma for chinese Language\nThis notebook demonstrates the fine-tuning of the Gemma model on chinese datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the chinese dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_zh)","metadata":{}},{"cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used gemma2_9b_en","metadata":{}},{"cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:09:54.926085Z","iopub.status.busy":"2024-12-29T17:09:54.925811Z","iopub.status.idle":"2024-12-29T17:10:04.364932Z","shell.execute_reply":"2024-12-29T17:10:04.363634Z"},"papermill":{"duration":9.445686,"end_time":"2024-12-29T17:10:04.366658","exception":false,"start_time":"2024-12-29T17:09:54.920972","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient\n# from google.colab import userdata\n#import getpass\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:10:04.376630Z","iopub.status.busy":"2024-12-29T17:10:04.376286Z","iopub.status.idle":"2024-12-29T17:10:04.380760Z","shell.execute_reply":"2024-12-29T17:10:04.379699Z"},"papermill":{"duration":0.011222,"end_time":"2024-12-29T17:10:04.382153","exception":false,"start_time":"2024-12-29T17:10:04.370931","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:10:04.391498Z","iopub.status.busy":"2024-12-29T17:10:04.391278Z","iopub.status.idle":"2024-12-29T17:10:20.146561Z","shell.execute_reply":"2024-12-29T17:10:20.145666Z"},"papermill":{"duration":15.762434,"end_time":"2024-12-29T17:10:20.148677","exception":false,"start_time":"2024-12-29T17:10:04.386243","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Load and Explore Chinese Dataset\nWe are using the `allenai/c4` dataset with Chinese (`zh`) data. The dataset is loaded in streaming mode for efficient handling of large datasets.\n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"cell_type":"code","source":"data = load_dataset(\"allenai/c4\", \"zh\", streaming=True)","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:10:20.157452Z","iopub.status.busy":"2024-12-29T17:10:20.157226Z","iopub.status.idle":"2024-12-29T17:10:59.234482Z","shell.execute_reply":"2024-12-29T17:10:59.233708Z"},"papermill":{"duration":39.08399,"end_time":"2024-12-29T17:10:59.236507","exception":false,"start_time":"2024-12-29T17:10:20.152517","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data = []\nfor i, example in enumerate(iter(data[\"train\"])):\n    if i >= 2:  # Change this number to get more examples\n        break\n    sample_data.append(example[\"text\"])\n\nprint(\"Sample Chinese Data:\")\nfor i, text in enumerate(sample_data):\n    print(f\"Example {i + 1}:\", text[:500])  # Print the first 500 characters to get a preview","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:10:59.246738Z","iopub.status.busy":"2024-12-29T17:10:59.246502Z","iopub.status.idle":"2024-12-29T17:11:12.598955Z","shell.execute_reply":"2024-12-29T17:11:12.597105Z"},"papermill":{"duration":13.358742,"end_time":"2024-12-29T17:11:12.600067","exception":false,"start_time":"2024-12-29T17:10:59.241325","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the maximum number of examples for training and validation\nmax_train_examples = 5000\nmax_val_examples = 100\n\n# Create a plain-text list from a subset of the dataset\n# Load data subsets\ntrain_text_data = [example[\"text\"] for example in itertools.islice(data[\"train\"], max_train_examples)]\nval_text_data = [example[\"text\"] for example in itertools.islice(data[\"validation\"], max_val_examples)]\n\n# Check the first example to ensure loading is correct\n#print(\"First training example:\", train_text_data[0])\n#print(\"First validation example:\", val_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:11:12.609580Z","iopub.status.busy":"2024-12-29T17:11:12.609312Z","iopub.status.idle":"2024-12-29T17:11:14.049360Z","shell.execute_reply":"2024-12-29T17:11:14.047972Z"},"papermill":{"duration":1.447149,"end_time":"2024-12-29T17:11:14.051109","exception":false,"start_time":"2024-12-29T17:11:12.603960","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"cell_type":"code","source":"batch_size = 4 \n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(val_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:11:14.061403Z","iopub.status.busy":"2024-12-29T17:11:14.061113Z","iopub.status.idle":"2024-12-29T17:11:14.342349Z","shell.execute_reply":"2024-12-29T17:11:14.341298Z"},"papermill":{"duration":0.288987,"end_time":"2024-12-29T17:11:14.344446","exception":false,"start_time":"2024-12-29T17:11:14.055459","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n","metadata":{}},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_en/3\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:11:14.354098Z","iopub.status.busy":"2024-12-29T17:11:14.353818Z","iopub.status.idle":"2024-12-29T17:13:57.473108Z","shell.execute_reply":"2024-12-29T17:13:57.471930Z"},"papermill":{"duration":163.126236,"end_time":"2024-12-29T17:13:57.474729","exception":false,"start_time":"2024-12-29T17:11:14.348493","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<48}  {str(variable.shape):<14}  {str(variable.value.sharding.spec)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:13:57.486846Z","iopub.status.busy":"2024-12-29T17:13:57.486593Z","iopub.status.idle":"2024-12-29T17:13:57.492803Z","shell.execute_reply":"2024-12-29T17:13:57.491384Z"},"papermill":{"duration":0.014772,"end_time":"2024-12-29T17:13:57.494811","exception":false,"start_time":"2024-12-29T17:13:57.480039","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:13:57.511518Z","iopub.status.busy":"2024-12-29T17:13:57.511240Z","iopub.status.idle":"2024-12-29T17:13:57.516098Z","shell.execute_reply":"2024-12-29T17:13:57.515002Z"},"papermill":{"duration":0.014884,"end_time":"2024-12-29T17:13:57.517918","exception":false,"start_time":"2024-12-29T17:13:57.503034","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"你好！你今天怎么样？告诉我一些你最近学到的有趣的事情。\", # Greeting and request for recent information\n    \"你对意大利文艺复兴的历史了解多少？你能解释它对艺术和科学的影响吗？\", # Request for historical knowledge and cultural impact\n    \"用中文写一首关于秋景的短诗。\", # Request for poetic creativity\n    \"用简单的语言解释一下人工智能是如何工作的，以及它在中国最常见的应用是什么。\", # Request for technical explanation and geographical context\n    \"如果有人说“力不从心”，那是什么意思？在什么情况下可以使用这个表达？\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:13:57.529019Z","iopub.status.busy":"2024-12-29T17:13:57.528788Z","iopub.status.idle":"2024-12-29T17:18:05.366683Z","shell.execute_reply":"2024-12-29T17:18:05.365612Z"},"papermill":{"duration":247.850182,"end_time":"2024-12-29T17:18:05.373034","exception":false,"start_time":"2024-12-29T17:13:57.522852","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:18:05.385540Z","iopub.status.busy":"2024-12-29T17:18:05.385244Z","iopub.status.idle":"2024-12-29T17:18:06.342247Z","shell.execute_reply":"2024-12-29T17:18:06.341129Z"},"papermill":{"duration":0.965228,"end_time":"2024-12-29T17:18:06.343946","exception":false,"start_time":"2024-12-29T17:18:05.378718","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_9b_zh\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-29T17:18:06.358092Z","iopub.status.busy":"2024-12-29T17:18:06.357810Z","iopub.status.idle":"2024-12-29T17:18:08.211470Z","shell.execute_reply":"2024-12-29T17:18:08.210113Z"},"papermill":{"duration":1.862905,"end_time":"2024-12-29T17:18:08.213426","exception":false,"start_time":"2024-12-29T17:18:06.350521","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])#","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 9: Plotting the results:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 5))\n\n# Plotting Loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-29T20:59:56.060566Z","iopub.status.busy":"2024-12-29T20:59:56.060109Z","iopub.status.idle":"2024-12-29T20:59:56.548514Z","shell.execute_reply":"2024-12-29T20:59:56.547284Z"},"papermill":{"duration":1.992144,"end_time":"2024-12-29T20:59:56.550561","exception":false,"start_time":"2024-12-29T20:59:54.558417","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NOTE:\nSince we are using a small size for our ```val_data```, the metrics obtained from it may not be very reliable.\nAs you can see the train loss and accuracy keep improving but the validation metrics are not, this suggest possible overfitting, How ever in the next cell we test the model and we see that its performance actually improved, So we can say that there are other reasons behind the difference between val and train loss.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"cell_type":"code","source":"test_prompts = [\n    \"你好！你今天怎么样？告诉我一些你最近学到的有趣的事情。\", # Greeting and request for recent information\n    \"你对意大利文艺复兴的历史了解多少？你能解释它对艺术和科学的影响吗？\", # Request for historical knowledge and cultural impact\n    \"用中文写一首关于秋景的短诗。\", # Request for poetic creativity\n    \"用简单的语言解释一下人工智能是如何工作的，以及它在中国最常见的应用是什么。\", # Request for technical explanation and geographical context\n    \"如果有人说“力不从心”，那是什么意思？在什么情况下可以使用这个表达？\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-29T20:59:59.593383Z","iopub.status.busy":"2024-12-29T20:59:59.592970Z","iopub.status.idle":"2024-12-29T21:04:39.328045Z","shell.execute_reply":"2024-12-29T21:04:39.326645Z"},"papermill":{"duration":282.813512,"end_time":"2024-12-29T21:04:40.909995","exception":false,"start_time":"2024-12-29T20:59:58.096483","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma model and used plain text in target language, we can expect some randomness and other things from its answers, as it has not been fine-tuned on instruct datasets(We will look into this in the next phase).","metadata":{}},{"cell_type":"markdown","source":"### Step 11: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_9b_zh\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_9b_zh\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2024-12-29T21:04:43.932505Z","iopub.status.busy":"2024-12-29T21:04:43.932111Z","iopub.status.idle":"2024-12-29T21:05:57.549796Z","shell.execute_reply":"2024-12-29T21:05:57.548900Z"},"papermill":{"duration":76.687457,"end_time":"2024-12-29T21:05:59.122829","exception":false,"start_time":"2024-12-29T21:04:42.435372","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_9b_zh\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Chinese text generation. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training\n","metadata":{}}]}