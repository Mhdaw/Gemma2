{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85984,"sourceType":"modelInstanceVersion","modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Gemma2 for Italian Language","metadata":{}},{"cell_type":"markdown","source":"**This notebook demonstrates fine-tuning Google’s Gemma 2 model for the Italian language using Keras NLP. It includes detailed steps for dataset creation, training, evaluation, and publishing the model.**\r\n","metadata":{}},{"cell_type":"markdown","source":"**Overview**                                                                                   \nIn this notebook, we will:\n\n1. Load and process Italian text data from the C4 dataset.                                                                            \n2. Configure and fine-tune the Gemma2 language model for Italian.                                      \n3. Evaluate the model’s performance before and after fine-tuning.                                      \n4. Publish the fine-tuned model to Kaggle Models for further use.                                      ","metadata":{}},{"cell_type":"markdown","source":"#### Device:\n1x Nvidia P100\n#### Base model:\nGemma2 2b base","metadata":{}},{"cell_type":"markdown","source":"**First, install necessary libraries including keras-nlp, datasets, keras_hub, kagglehub**","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:39:52.911215Z","iopub.execute_input":"2024-12-09T15:39:52.911613Z","iopub.status.idle":"2024-12-09T15:40:05.642098Z","shell.execute_reply.started":"2024-12-09T15:39:52.911559Z","shell.execute_reply":"2024-12-09T15:40:05.640916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we set up environment variables for Kaggle authentication and configure the backend for optimal memory allocation.","metadata":{}},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:30.651104Z","iopub.execute_input":"2024-12-09T15:40:30.651433Z","iopub.status.idle":"2024-12-09T15:40:32.273924Z","shell.execute_reply.started":"2024-12-09T15:40:30.651404Z","shell.execute_reply":"2024-12-09T15:40:32.273066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n\nos.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"KAGGLE_USERNAME\")\nos.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:41.652580Z","iopub.execute_input":"2024-12-09T15:40:41.653473Z","iopub.status.idle":"2024-12-09T15:40:42.241256Z","shell.execute_reply.started":"2024-12-09T15:40:41.653435Z","shell.execute_reply":"2024-12-09T15:40:42.240307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Importing Libraries**                                                             \nNow, import TensorFlow, Keras NLP, and other libraries required for model loading and dataset handling.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:43.659842Z","iopub.execute_input":"2024-12-09T15:40:43.660448Z","iopub.status.idle":"2024-12-09T15:40:53.670814Z","shell.execute_reply.started":"2024-12-09T15:40:43.660408Z","shell.execute_reply":"2024-12-09T15:40:53.670085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a MirroredStrategy.\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:56.265336Z","iopub.execute_input":"2024-12-09T15:40:56.265962Z","iopub.status.idle":"2024-12-09T15:40:56.676750Z","shell.execute_reply.started":"2024-12-09T15:40:56.265929Z","shell.execute_reply":"2024-12-09T15:40:56.675875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"Since we want to fine-tune the Gemma 2 2b model for adapting to the Italian language, we need a good amount of high-quality Italian text corpus. For that, we use the 'C4' dataset, which is a multilingual text dataset.\r\n\r\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/allenai/c4)  \r\n\r\n**Dataset Summary (from the original dataset page):**  \r\nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on the Common Crawl dataset: [https://commoncrawl.org](https://commoncrawl.org).\r\n\r\nThis is the processed version of Googleataset.set.\r\net","metadata":{}},{"cell_type":"markdown","source":"**Note**                                                                                            \nsince this is a very large dataset, We use the \"streaming=True\" to avoid memory problems.\n","metadata":{}},{"cell_type":"code","source":"italian_data = load_dataset(\"allenai/c4\", \"it\", streaming=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:40:59.755864Z","iopub.execute_input":"2024-12-09T15:40:59.756196Z","iopub.status.idle":"2024-12-09T15:41:39.321949Z","shell.execute_reply.started":"2024-12-09T15:40:59.756167Z","shell.execute_reply":"2024-12-09T15:41:39.320978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The** data is in this format:\r\n\r\nAn example:\r\n```json\r\n{\r\n  \"url\": \"https://klyq.com/beginners-bbq-class-taking-place-in-missoula/\",\r\n  \"text\": \"Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity; put this on your calendar now. On Thursday, September 22nd, join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner-level class for everyone who wants to improve their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators, it is free. Included in the cost will be either a t-shirt or apron, and you will be tasting samples of each meat that is prepared.\",\r\n  \"timestamp\": \"2019-04-25T12:57:54Z\"\r\n}\r\n'\r\n}","metadata":{}},{"cell_type":"markdown","source":"**Here we take a look inside the dataset and print some examples.**","metadata":{}},{"cell_type":"code","source":"sample_data = []\nfor i, example in enumerate(iter(italian_data[\"train\"])):\n    if i >= 2:  # Change this number to get more examples\n        break\n    sample_data.append(example[\"text\"])\n\nprint(\"Sample Italian Data:\")\n\nfor i, text in enumerate(sample_data):\n    print(f\"Example {i + 1}:\", text[:50])  # Print the first 50 characters to get a preview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:42:00.275612Z","iopub.execute_input":"2024-12-09T15:42:00.276363Z","iopub.status.idle":"2024-12-09T15:42:19.454514Z","shell.execute_reply.started":"2024-12-09T15:42:00.276328Z","shell.execute_reply":"2024-12-09T15:42:19.453524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now** it's time to prepare the dataset for the model. We need to convert the dataset into a TensorFlow dataset, and we will use a fraction of the original dataset to save memory and time. (If you have better hardware available, you are welcome to try with a larger number of examples.)\r\n","metadata":{}},{"cell_type":"code","source":"# Define the maximum number of examples for training and validation\nmax_train_examples = 3000\nmax_val_examples = 100\n\n# Create a plain-text list from a subset of the dataset\n# Load data subsets\ntrain_text_data = [example[\"text\"] for example in itertools.islice(italian_data[\"train\"], max_train_examples)]\nval_text_data = [example[\"text\"] for example in itertools.islice(italian_data[\"validation\"], max_val_examples)]\n\n# Check the first example to ensure loading is correct\n#print(\"First training example:\", train_text_data[0])\n#print(\"First validation example:\", val_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:42:19.455861Z","iopub.execute_input":"2024-12-09T15:42:19.456136Z","iopub.status.idle":"2024-12-09T15:42:20.677021Z","shell.execute_reply.started":"2024-12-09T15:42:19.456112Z","shell.execute_reply":"2024-12-09T15:42:20.676136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 1\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(val_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:42:20.678185Z","iopub.execute_input":"2024-12-09T15:42:20.678522Z","iopub.status.idle":"2024-12-09T15:42:20.919206Z","shell.execute_reply.started":"2024-12-09T15:42:20.678493Z","shell.execute_reply":"2024-12-09T15:42:20.918519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"**Now we load the Gemma 2 model. For this notebook, we use the 2b version since we are working with limited hardware.**\n","metadata":{}},{"cell_type":"code","source":"model_id = \"gemma2_2b_en\"\n\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n    \ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:42:23.202547Z","iopub.execute_input":"2024-12-09T15:42:23.203265Z","iopub.status.idle":"2024-12-09T15:43:23.406846Z","shell.execute_reply.started":"2024-12-09T15:42:23.203231Z","shell.execute_reply":"2024-12-09T15:43:23.406103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Testing the Model:**\r\nWe can test the model by passing it an input to compare its responses before and after fine-tuning.\r\n","metadata":{}},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:36:29.084420Z","iopub.execute_input":"2024-12-09T17:36:29.085239Z","iopub.status.idle":"2024-12-09T17:36:29.090669Z","shell.execute_reply.started":"2024-12-09T17:36:29.085204Z","shell.execute_reply":"2024-12-09T17:36:29.089797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"Ciao! Come stai oggi? Raccontami qualcosa di interessante che hai imparato di recente.\",\n    \"Che cosa sai della storia del Rinascimento in Italia? Puoi spiegare il suo impatto sull'arte e sulla scienza?\",\n    \"Scrivi una breve poesia in italiano su un paesaggio autunnale.\",\n    \"Spiegare, in termini semplici, come funziona l'intelligenza artificiale e quali sono i suoi utilizzi più comuni in Italia.\",\n    \"Se qualcuno dicesse: 'Hai fatto il passo più lungo della gamba', cosa significherebbe? In quale situazione potrebbe essere usata questa espressione?\",\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:34:04.593517Z","iopub.execute_input":"2024-12-09T15:34:04.593892Z","iopub.status.idle":"2024-12-09T15:36:46.594325Z","shell.execute_reply.started":"2024-12-09T15:34:04.593859Z","shell.execute_reply":"2024-12-09T15:36:46.593380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LoRA\r\n\r\nThis is a large model with more than 2 billion trainable parameters. Full fine-tuning is very computationally expensive and time-consuming, so we choose the next best thing: the LoRA method.\r\n\r\n## What is LoRA?  \r\nLoRA (Low-Rank Adaptation) is a technique used to efficiently fine-tune large language models (LLMs) like Gemma 2.2b. It works by introducing trainable rank-decomposition matrices to the attention layers of the pre-trained model.\r\n","metadata":{}},{"cell_type":"code","source":"LoRA_rank = 2 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:43:34.182663Z","iopub.execute_input":"2024-12-09T15:43:34.183143Z","iopub.status.idle":"2024-12-09T15:43:34.424509Z","shell.execute_reply.started":"2024-12-09T15:43:34.183100Z","shell.execute_reply":"2024-12-09T15:43:34.423593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*using LoRA reduced the number of trainable parameter from 2,614,341,888 to 1,464,320 !*","metadata":{}},{"cell_type":"markdown","source":"**Now** lets prepare the model for fine-tuning                                          \ntaken from [here](https://ai.google.dev/gemma/docs/lora_tuning)","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom wandb.integration.keras import WandbMetricsLogger\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:43:38.526948Z","iopub.execute_input":"2024-12-09T15:43:38.527661Z","iopub.status.idle":"2024-12-09T15:43:39.183220Z","shell.execute_reply.started":"2024-12-09T15:43:38.527630Z","shell.execute_reply":"2024-12-09T15:43:39.182430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.05,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 1,\n    learning_rate = 5e-5,\n    weight_decay = 0.05,\n    sequence_length = 256,\n    epochs = 5\n)\n\nwandb.init(project = \"fine-tuning-gemma2_2b_it\",\n    config=configs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:43:52.079764Z","iopub.execute_input":"2024-12-09T15:43:52.080105Z","iopub.status.idle":"2024-12-09T15:43:59.412825Z","shell.execute_reply.started":"2024-12-09T15:43:52.080075Z","shell.execute_reply":"2024-12-09T15:43:59.411957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training:**","metadata":{}},{"cell_type":"code","source":"# Inspect dataset element types\nfor element in train_data.take(1):\n    print(type(element))\n    print(element[0].dtype if hasattr(element[0], 'dtype') else \"No dtype found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:29:03.128821Z","iopub.execute_input":"2024-12-09T15:29:03.129159Z","iopub.status.idle":"2024-12-09T15:29:03.171023Z","shell.execute_reply.started":"2024-12-09T15:29:03.129131Z","shell.execute_reply":"2024-12-09T15:29:03.170216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = gemma_lm.fit(train_data, validation_data=val_data, epochs=5, callbacks=[WandbMetricsLogger()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:44:07.029077Z","iopub.execute_input":"2024-12-09T15:44:07.029424Z","iopub.status.idle":"2024-12-09T17:33:35.035095Z","shell.execute_reply.started":"2024-12-09T15:44:07.029391Z","shell.execute_reply":"2024-12-09T17:33:35.034184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plotting the loss and accuracy:**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 6))\n\n# Plotting Loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plotting Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:36:10.171734Z","iopub.execute_input":"2024-12-09T17:36:10.172097Z","iopub.status.idle":"2024-12-09T17:36:10.801588Z","shell.execute_reply.started":"2024-12-09T17:36:10.172066Z","shell.execute_reply":"2024-12-09T17:36:10.800776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we see that the fine-tuning on Italian language in fact had a good effect since it is making a more meaningful response.","metadata":{}},{"cell_type":"code","source":"test_prompts = [\n    \"Ciao! Come stai oggi? Raccontami qualcosa di interessante che hai imparato di recente.\",\n    \"Che cosa sai della storia del Rinascimento in Italia? Puoi spiegare il suo impatto sull'arte e sulla scienza?\",\n    \"Scrivi una breve poesia in italiano su un paesaggio autunnale.\",\n    \"Spiegare, in termini semplici, come funziona l'intelligenza artificiale e quali sono i suoi utilizzi più comuni in Italia.\",\n    \"Se qualcuno dicesse: 'Hai fatto il passo più lungo della gamba', cosa significherebbe? In quale situazione potrebbe essere usata questa espressione?\",\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:36:35.095199Z","iopub.execute_input":"2024-12-09T17:36:35.095507Z","iopub.status.idle":"2024-12-09T17:39:16.732588Z","shell.execute_reply.started":"2024-12-09T17:36:35.095480Z","shell.execute_reply":"2024-12-09T17:39:16.731789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Uploading the fine-tuned model to kaggle","metadata":{}},{"cell_type":"markdown","source":"**For uploading the model to kaggle, First we need to save it:**","metadata":{}},{"cell_type":"code","source":"os.makedirs('gemma2_2b_it')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:47:56.898578Z","iopub.execute_input":"2024-12-09T17:47:56.899216Z","iopub.status.idle":"2024-12-09T17:47:56.904268Z","shell.execute_reply.started":"2024-12-09T17:47:56.899182Z","shell.execute_reply":"2024-12-09T17:47:56.903301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npreset_dir = \"/kaggle/working/gemma2_2b_it\"\ngemma_lm.save_to_preset(preset_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:48:35.525072Z","iopub.execute_input":"2024-12-09T17:48:35.525665Z","iopub.status.idle":"2024-12-09T17:49:09.977286Z","shell.execute_reply.started":"2024-12-09T17:48:35.525630Z","shell.execute_reply":"2024-12-09T17:49:09.976077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preset_dir = \"gemma2_2b_it\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:50:07.723906Z","iopub.execute_input":"2024-12-09T17:50:07.724251Z","iopub.status.idle":"2024-12-09T17:50:07.729586Z","shell.execute_reply.started":"2024-12-09T17:50:07.724219Z","shell.execute_reply":"2024-12-09T17:50:07.728773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:47:01.542648Z","iopub.execute_input":"2024-12-09T17:47:01.543536Z","iopub.status.idle":"2024-12-09T17:47:02.304956Z","shell.execute_reply.started":"2024-12-09T17:47:01.543502Z","shell.execute_reply":"2024-12-09T17:47:02.304082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, preset_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:50:10.195680Z","iopub.execute_input":"2024-12-09T17:50:10.196528Z","iopub.status.idle":"2024-12-09T17:57:17.582583Z","shell.execute_reply.started":"2024-12-09T17:50:10.196494Z","shell.execute_reply":"2024-12-09T17:57:17.581753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:57:43.484661Z","iopub.execute_input":"2024-12-09T17:57:43.485058Z","iopub.status.idle":"2024-12-09T17:57:46.593269Z","shell.execute_reply.started":"2024-12-09T17:57:43.485027Z","shell.execute_reply":"2024-12-09T17:57:46.592560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T17:57:58.275303Z","iopub.execute_input":"2024-12-09T17:57:58.275652Z","iopub.status.idle":"2024-12-09T17:57:58.280442Z","shell.execute_reply.started":"2024-12-09T17:57:58.275620Z","shell.execute_reply":"2024-12-09T17:57:58.279513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \r\na built-in preset identifier like 'bert_base_e\n* 2. '\r\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\r\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\r\na path to a local preset directory like './bert_base_en'","metadata":{}},{"cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_2b_it\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After the model is loaded, You can use it to generate French:)**","metadata":{}},{"cell_type":"code","source":"test_prompt = # your prompt.\n# Generate output after fine-tuning\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Thats it, If you have any suggestion, I would apperciate it**","metadata":{}}]}