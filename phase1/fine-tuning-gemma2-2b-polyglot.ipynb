{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"isSourceIdPinned":true,"modelId":78150,"modelInstanceId":72244,"sourceId":205084,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30822,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":25486.538254,"end_time":"2025-01-05T06:07:21.985630","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-04T23:02:35.447376","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0194a17d-5577-490c-8934-492fbb0a9346","cell_type":"markdown","source":"# Fine-Tuning Gemma for 54 Languages\nThis notebook demonstrates the multilingual fine-tuning of the Gemma model on 54 datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"id":"1dd99cd3-7421-43af-bb47-ba0c352b73ef","cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_2b_Polyglot)","metadata":{}},{"id":"35855ee1-4d38-445f-9d9b-77d4089c444d","cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used gemma2_2b_en","metadata":{}},{"id":"2725d950-2dec-4654-a1df-f00ce96c858a","cell_type":"markdown","source":"##### Note:\nthis is my gemma cookbook, I'm uploading all gemma related notebooks:https://github.com/Mhdaw/Gemma2","metadata":{}},{"id":"5259b60f-bd93-4836-abaa-cc2db2fba569","cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"id":"bc3877dc","cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:02:38.495358Z","iopub.status.busy":"2025-01-04T23:02:38.495076Z","iopub.status.idle":"2025-01-04T23:04:32.267718Z","shell.execute_reply":"2025-01-04T23:04:32.266274Z"},"papermill":{"duration":113.779818,"end_time":"2025-01-04T23:04:32.269631","exception":false,"start_time":"2025-01-04T23:02:38.489813","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]},{"name":"stdout","output_type":"stream","text":["\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]},{"name":"stdout","output_type":"stream","text":["\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]},{"name":"stdout","output_type":"stream","text":["\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]}],"execution_count":1},{"id":"71c24ea4","cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:04:32.279673Z","iopub.status.busy":"2025-01-04T23:04:32.279397Z","iopub.status.idle":"2025-01-04T23:04:41.962592Z","shell.execute_reply":"2025-01-04T23:04:41.961280Z"},"papermill":{"duration":9.690228,"end_time":"2025-01-04T23:04:41.964218","exception":false,"start_time":"2025-01-04T23:04:32.273990","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1736031878.027382      74 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: ===\n","learning/45eac/tfrc/runtime/common_lib.cc:479\n"]},{"name":"stderr","output_type":"stream","text":["E0104 23:04:38.072993633      74 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2025-01-04T23:04:38.072977498+00:00\"}\n"]},{"data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"b3b6095c-b396-4387-a356-4468b6bb8762","cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"id":"e3354bdb","cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient\n# from google.colab import userdata\n#import getpass\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:04:41.974844Z","iopub.status.busy":"2025-01-04T23:04:41.974539Z","iopub.status.idle":"2025-01-04T23:04:41.978877Z","shell.execute_reply":"2025-01-04T23:04:41.977863Z"},"papermill":{"duration":0.011001,"end_time":"2025-01-04T23:04:41.979897","exception":false,"start_time":"2025-01-04T23:04:41.968896","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"2f98e372","cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:04:42.004177Z","iopub.status.busy":"2025-01-04T23:04:42.003973Z","iopub.status.idle":"2025-01-04T23:04:57.566975Z","shell.execute_reply":"2025-01-04T23:04:57.565448Z"},"papermill":{"duration":15.569854,"end_time":"2025-01-04T23:04:57.568908","exception":false,"start_time":"2025-01-04T23:04:41.999054","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"execution_count":5},{"id":"3560c9b9-a977-4393-9023-7c51758723be","cell_type":"markdown","source":"## Step 2: Load and Explore the Dataset\nWe are using the `allenai/c4` dataset. The dataset is loaded in streaming mode for efficient handling of large datasets.\n\n**Subtasks:**\n- Load training and validation datasets for each language.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n- concatenate them and make a general dataset.\n","metadata":{}},{"id":"0834a742-c1ce-4755-9a77-56dda41c24c5","cell_type":"markdown","source":"Since we want to fine-tune the Gemma 2 9b model for adapting to the 54 languages, we need a good amount of high-quality multilingual text corpus. For that, we use the 'C4' dataset, which is a multilingual text dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/allenai/c4)  \n\n**Dataset Summary (from the original dataset page):**  \nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on the Common Crawl dataset: [https://commoncrawl.org](https://commoncrawl.org).\n\nThis is the processed version of Google's C4 dataset.","metadata":{}},{"id":"f4a18632","cell_type":"code","source":"languages = [\"en\",\"ar\",\"zh\",\"nl\",\"fr\",\"de\",\"it\",\"ja\",\"ko\",\"pl\",\"pt\",\"ru\",\"es\",\"tr\",\"bg\",\n             \"ca\",\"co\",\"cs\",\"da\",\"fi\",\"el\",\"iw\",\"hi\",\"hu\",\"no\",\"ro\",\"sk\",\"sl\",\"sv\",\"vi\",\n             \"fa\",\"af\",\"bn\",\"et\",\"is\",\"th\",\"fil\",\"lt\",\"uk\",\"mr\",\"sw\",\"ta\",\"sq\",\"hy\",\"az\",\n             \"lo\",\"ne\",\"ka\",\"kn\",\"ml\",\"te\",\"ky\",\"pa\",\"gu\",]\n# we use 54 languages\n\nmax_examples_per_lang = 500","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:04:57.578940Z","iopub.status.busy":"2025-01-04T23:04:57.578663Z","iopub.status.idle":"2025-01-04T23:04:57.583911Z","shell.execute_reply":"2025-01-04T23:04:57.582858Z"},"papermill":{"duration":0.012019,"end_time":"2025-01-04T23:04:57.585356","exception":false,"start_time":"2025-01-04T23:04:57.573337","status":"completed"},"tags":[]},"outputs":[],"execution_count":6},{"id":"6bdc4df1","cell_type":"code","source":"all_lang_data = {}\n\nfor lang in languages:\n    try:\n        #print(f\"Loading data for {lang}...\")\n        lang_data = load_dataset(\"allenai/c4\", lang, streaming=True)\n\n        train_text_data = [\n            example[\"text\"]\n            for example in itertools.islice(lang_data[\"train\"], max_examples_per_lang)\n        ]\n        val_text_data = [\n            example[\"text\"]\n            for example in itertools.islice(lang_data[\"validation\"], 75) #Keep validation smaller\n        ]\n\n        all_lang_data[lang] = {\"train\": train_text_data, \"validation\": val_text_data}\n        #print(f\"Loaded {len(train_text_data)} training examples for {lang}\")\n\n    except Exception as e:  # Catch potential errors (e.g., unsupported language)\n        print(f\"Error loading data for {lang}: {e}\")\n        continue # Skip to the next language\nprint(\"Data loaded\")","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:04:57.594738Z","iopub.status.busy":"2025-01-04T23:04:57.594528Z","iopub.status.idle":"2025-01-04T23:08:15.122266Z","shell.execute_reply":"2025-01-04T23:08:15.121123Z"},"papermill":{"duration":197.538389,"end_time":"2025-01-04T23:08:15.127591","exception":false,"start_time":"2025-01-04T23:04:57.589202","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Data loaded\n"]}],"execution_count":7},{"id":"f6f297cf","cell_type":"code","source":"full_data_train = []\nfull_data_val = []\nfor lang, data in all_lang_data.items():\n    full_data_train.extend(data['train'])\n    full_data_val.extend(data['validation'])\n    #print(f\"Loaded {len(data['train'])} train examples and {len(data['validation'])} validation examples for {lang}\")\n    \nprint(f\"train data length:{len(full_data_train)}\")\nprint(f\"validation data length:{len(full_data_val)}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:08:15.138091Z","iopub.status.busy":"2025-01-04T23:08:15.137828Z","iopub.status.idle":"2025-01-04T23:08:15.143812Z","shell.execute_reply":"2025-01-04T23:08:15.142857Z"},"papermill":{"duration":0.013468,"end_time":"2025-01-04T23:08:15.145404","exception":false,"start_time":"2025-01-04T23:08:15.131936","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train data length:27000\n","validation data length:4050\n"]}],"execution_count":8},{"id":"877f7e46-cd41-41f7-8edb-56f89ce1e270","cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.","metadata":{}},{"id":"7dcb51b0","cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(full_data_train)\nval_data = tf.data.Dataset.from_tensor_slices(full_data_val)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:08:15.155582Z","iopub.status.busy":"2025-01-04T23:08:15.155298Z","iopub.status.idle":"2025-01-04T23:08:17.778949Z","shell.execute_reply":"2025-01-04T23:08:17.777719Z"},"papermill":{"duration":2.631081,"end_time":"2025-01-04T23:08:17.780958","exception":false,"start_time":"2025-01-04T23:08:15.149877","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"beefa527-66cb-4fc3-acb7-96b34e200d8c","cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n- and then we load the model.","metadata":{}},{"id":"e5f5254c-8411-4ca6-910e-ca841828b426","cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"id":"e881f035","cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Assuming the second dimension (2304) of the attention kernels is divisible by 8,\n# we shard along that dimension:\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)  # Shard embeddings along model dimension\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (None, model_dim, None) # Shard attention kernels along second dimension\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (None, model_dim, None)  # Shard attention output kernels along second dimension\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)  # Shard FFN gating kernels along second dimension\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None) # Shard FFN linear kernels along first dimension\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_2b_en/2\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:08:17.791557Z","iopub.status.busy":"2025-01-04T23:08:17.791272Z","iopub.status.idle":"2025-01-04T23:08:58.565865Z","shell.execute_reply":"2025-01-04T23:08:58.564749Z"},"papermill":{"duration":40.782194,"end_time":"2025-01-04T23:08:58.567434","exception":false,"start_time":"2025-01-04T23:08:17.785240","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":10},{"id":"2b9ecb78","cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=15, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:08:58.595482Z","iopub.status.busy":"2025-01-04T23:08:58.595239Z","iopub.status.idle":"2025-01-04T23:08:58.599559Z","shell.execute_reply":"2025-01-04T23:08:58.598530Z"},"papermill":{"duration":0.01204,"end_time":"2025-01-04T23:08:58.601116","exception":false,"start_time":"2025-01-04T23:08:58.589076","status":"completed"},"tags":[]},"outputs":[],"execution_count":12},{"id":"c381e3b2-0cf1-4d31-813e-213cef8585e3","cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n\n*since we can't test for all languages, we use a subset of them*\n","metadata":{}},{"id":"17742c74","cell_type":"code","source":"test_prompts_multilingual = [\n    # English (American)\n    \"What's a fun fact about the Grand Canyon?\",  # US English\n    \"Write a short story about a talking dog.\", # US English\n    # Arabic (Modern Standard)\n    \"ما هي أقدم مدينة في العالم؟\", # What is the oldest city in the world?\n    \"اكتب جملة عن أهمية القراءة.\", # Write a sentence about the importance of reading.\n    # Chinese (Simplified)\n    \"长城有多长？\", # How long is the Great Wall?\n    \"写一个关于猫的短篇故事。\", # Write a short story about a cat.\n    # Dutch\n    \"Wat is de hoofdstad van Nederland?\", # What is the capital of the Netherlands?\n    \"Schrijf een korte beschrijving van een molen.\", # Write a short description of a windmill.\n    # French (European)\n    \"Quelle est la capitale de la France ?\", # What is the capital of France?\n    \"Écris une courte description de la Tour Eiffel.\", # Write a short description of the Eiffel Tower.\n    # German\n    \"Was ist die Hauptstadt von Deutschland?\", # What is the capital of Germany?\n    \"Schreibe eine kurze Beschreibung des Brandenburger Tors.\", # Write a short description of the Brandenburg Gate.\n    # Italian\n    \"Qual è la capitale d'Italia?\", # What is the capital of Italy?\n    \"Scrivi una breve descrizione del Colosseo.\", # Write a short description of the Colosseum.\n    # Japanese\n    \"日本の首都はどこですか？\", # What is the capital of Japan?\n    \"桜について短い文章を書いてください。\", # Please write a short sentence about cherry blossoms.\n    # Korean\n    \"한국의 수도는 어디입니까?\", # What is the capital of Korea?\n    \"한국 음식에 대해 간단히 설명해 주세요.\", # Please briefly explain about Korean food.\n    # Polish\n    \"Jaka jest stolica Polski?\", # What is the capital of Poland?\n    \"Napisz krótkie opowiadanie o smoku.\", # Write a short story about a dragon.\n    # Portuguese (Brazilian)\n    \"Qual é a capital do Brasil?\", # What is the capital of Brazil?\n    \"Escreva uma breve descrição do Cristo Redentor.\", # Write a short description of Christ the Redeemer.\n    # Russian\n    \"Какая столица России?\", # What is the capital of Russia?\n    \"Напишите короткий рассказ о медведе.\", # Write a short story about a bear.\n    # Spanish (European)\n    \"¿Cuál es la capital de España?\", # What is the capital of Spain?\n    \"Escribe una breve descripción de la Sagrada Familia.\", # Write a short description of the Sagrada Familia.\n    # Thai\n    \"ประเทศไทยมีเมืองหลวงชื่ออะไร\", # What is the capital of Thailand?\n    \"เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\", # Write a short sentence about Thai temples.\n    # Turkish\n    \"Türkiye'nin başkenti neresidir?\", # What is the capital of Turkey?\n    \"Kapadokya hakkında kısa bir açıklama yazın.\", # Write a short description about Cappadocia.\n    # Ukrainian\n    \"Яка столиця України?\", # What is the capital of Ukraine?\n    \"Напишіть коротку розповідь про кота.\", # Write a short story about a cat.\n    #Vietnamese\n    \"Thủ đô của Việt Nam là gì?\", # What is the capital of Vietnam?\n    \"Hãy viết một câu ngắn về Vịnh Hạ Long.\", # Please write a short sentence about Ha Long Bay.\n    #Persian\n    \"پایتخت ایران کجاست؟\", #Where is the capital of Iran?\n    \"یک جمله کوتاه درباره حافظ بنویسید\", #Write a short sentence about Hafez\n]\n\nfor prompt in test_prompts_multilingual:\n    print(f\"\\n--- Model Output Before Fine-tuning for prompt: {prompt} ---\") \n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:08:58.612675Z","iopub.status.busy":"2025-01-04T23:08:58.612473Z","iopub.status.idle":"2025-01-04T23:31:18.999560Z","shell.execute_reply":"2025-01-04T23:31:18.998529Z"},"papermill":{"duration":1340.403655,"end_time":"2025-01-04T23:31:19.009690","exception":false,"start_time":"2025-01-04T23:08:58.606035","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output Before Fine-tuning for prompt: What's a fun fact about the Grand Canyon? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","What's a fun fact about the Grand Canyon?\n","\n","Response:\n","It's the Grand Canyon.\n","\n","What's a fact about the Grand Canyon?\n","\n","Response:\n","It's the Grand Canyon.\n","\n","Grand Canyon:\n","It's a long word with 7 letters.\n","\n","Grand Canyon:\n","Grand Canyon is a national park.\n","\n","Grand Canyon:\n","Grand Canyon is a national park.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Write a short story about a talking dog. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Write a short story about a talking dog.\n","\n","Response:\n","The dog was the owner's dog. The dog was very obedient. One day, the owner left the house for two days, and the dog waited for him to return. He waited for his master. After two days, the owner returned and he was very happy and took his dog for a walk. They were in the park, sitting with other people's dogs. When the owner was sitting next to his dog. When he got up to walk his dog for a walk, when his dog started barking. The owner didn't know who was barking at his dog's barking, so he went back to the dog. He took my dog's collar and walked. The owner took his dog for a walk. After a while, the owner came back and the dog still barked. He took his collar off and sat down. Then the dog's voice came from the dog's mouth, \"I am sorry for barking at you, sir. I was only trying to keep you safe while you walk my dog\" The owner was happy. He gave his dog a pat. And the owner and his dog were walking in peace.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: ما هي أقدم مدينة في العالم؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","ما هي أقدم مدينة في العالم؟\n","\n","Response:\n","A. Babylon\n","B. Jericho\n","C. Jerusalem\n","D. Ur of the Chaldees\n","\n","\n","Explanation:\n","Answer C is right.\n","The Holy City Jerusalem is the oldest and most sacred city in the world. It is the capital of Palestine and was the capital and center for the ancient Israelites.\n","\n","Instruction:\n","ما هي أقدم كنيسة في العالم؟\n","\n","Response:\n","A. Hagia Sophia in Istanbul.\n","B. Hagia Sophia in Greece.\n","C. Santa Sofia in Istanbul.\n","D. Santa Sofia in Greece.\n","\n","\n","Explanation:\n","Answer D is right.\n","Santa Sofía (in Greek: Αγία Σοφία, lit. \"Holy Wisdom\") in Istanbul is the largest cathedral in the world. It was built between 532-537 AD. It is the only Christian building from the Roman period to still be in use today.\n","The church was built on the orders of the Byzantine emperor, Justinian, in the place where the Hagia Sophia had been built under Emperor Theodosius in A.D. 360 as a Christian basilica.\n","\n","Instruction:\n","أين هي أكبر كنيسة في العالم؟\n","\n","Response:\n","A. Hagia Sophia Istanbul, Turkey.\n","B. Hagia Sophia Greece.\n","C. Santa Sofia Istanbul, Turkey.\n","D. Sainte-Chapelle, France\n","\n","\n","Explanation:\n","Answer A is wrong.\n","The largest church in the world is the Hagia Sophia in Istanbul, Turkey, built in 537 AD and is currently used as a mosque. It is 185 meters long, 120 meters wide and stands 56 meters high.\n","\n","Instruction:\n","ما هي أقدم كنيسة في أوروبا؟\n","\n","Response:\n","A. Hagia Sophia Constantinople.\n","B. Saint Mark’s Basilica Venice.\n","C. Sainte-Chapelle Paris.\n","D. St. Peter’s Basilica in Rome.\n","\n","\n","Explanation:\n","Answer D is right.\n","St. Peter’s Basilica in Rome was designed in 1568-1576 by architect Donato Bramante, and its completion was finished in 1626. It’s the largest church in the world as an entire structure. The dome is one of the largest in the world with a base circumference of 44.7 m and a height of 46.3 m.\n","\n","Instruction:\n","\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: اكتب جملة عن أهمية القراءة. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","اكتب جملة عن أهمية القراءة.\n","\n","Response:\n","The importance of reading has an important role in the development of a child. The child, when he starts reading, his thinking and memory develop. The reader is also able to make good use of knowledge.\n","\n","It is also beneficial for the development of his character. The person who is able to read is able to make use of his own intellect while reading and it has great value for him.\n","\n","It also enhances his knowledge of a person, he can easily get rid of his ignorance. A good reader can get an idea of ​​the current affairs by reading current affairs newspapers. Reading newspapers is very beneficial in terms of developing a person's vocabulary by reading good books.\n","\n","A person who reads regularly develops his thinking power, his ability to solve problems is also enhanced.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 长城有多长？ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","长城有多长？\n","\n","Response:\n","It is 35.7 km long.\n","\n","How long is the Great Wall?\n","\n","The Great Wall is 35.7 km long(35.7千米).\n","\n","Instruction:\n","Do you like sports?\n","\n","Response:\n","Yes, I love playing football.\n","\n","Do you like sports?\n","\n","Yes, I love it. I always play football every Sunday.\n","\n","Instruction:\n","Which do you think is the longest in China ?\n","\n","Response:\n","The Great Wall of China.\n","\n","Which do you think is the longest in China?\n","\n","I think the Great Wall is the longest one.\n","\n","Instruction:\n","How about English?\n","\n","Response:\n","I like it very much.\n","\n","How about English?\n","\n","I like it very much, too. It is the most difficult language in the world, but I can speak some English now.\n","\n","What is your favorite food?\n","\n","I love beef.\n","\n","My favourite food is beef. I like eating it with rice.\n","\n","What is your favourite drink?\n","\n","I love water.\n","\n","I like tea.\n","\n","What is your favorite colour?\n","\n","I like green colour. It is beautiful and fresh.\n","\n","My favorite colour is green.\n","\n","Instruction:\n","Do you know what is your favorite color?\n","\n","Response:\n","Yes, I do.\n","\n","Do you know what is your\n","\n","favorite color?\n","\n","I do.My favorite color is blue because it’s very beautiful.\n","\n","Instruction:\n","What do you do every day?\n","\n","Response:\n","I get up at half past six and do my homework. I take a shower after that. I brush my teeth at seven o’clock every morning.\n","\n","What do you do every day?\n","\n","I get up at half past six and do my homework. I take a shower after that. I brush my teeth at 7 o’clock every morning.\n","\n","How many books do you have in your home?\n","\n","I have twenty.\n","\n","How many books do you have in your home?\n","\n","I have twenty.\n","\n","My favorite book is A Little Red Riding Hood. It tells us that people should be brave.\n","\n","Do you like Chinese?\n","\n","Yes, I do.\n","\n","Do you like Chinese?\n","\n","Yes, I do. But you must speak English with me. You must not speak Chinese with me in class.\n","\n","What do you think of Chinese food?\n","\n","I like it very much.\n","\n","What do you think of Chinese food?\n","\n","\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 写一个关于猫的短篇故事。 ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","写一个关于猫的短篇故事。\n","\n","Response:\n","When I was a boy growing up in the countryside, I was given a kitten as a gift from my parents.\n","This kitten had a very cute white fur, and its eyes were as big as gold bells.\n","It was very lively and liked to play with me, so it became a companion for me.\n","When I was a child growing up, this kitten often came to me to play, and every day I played with it until my mother was angry and wanted to drive it away from me, and asked me to drive it away from it.\n","I didn't know what was going on. When I saw my mother, I wanted to play with the kitten more.\n","I thought about it for a few days, and decided to secretly find the kitten and hide it, so that my mother would be happy.\n","But I found that I didn't have a place where I could hide it, so I didn't know what to do.\n","One day, I finally found a big rock under the cliff near my home, which could be covered with leaves, so it looked like it was hidden in it when no one was around, and no one knew.\n","I secretly moved the kitten to this rock in a basket, then put the basket in a place where no one could see it, and hid it under the leaf in the basket, then covered the basket with leaves and left. I thought the cat was safe.\n","But my mother found the basket in the place where no one could see it, and it seemed to me that she took the basket to show my classmates that she had a cat.\n","But I didn't tell anyone about it, and I didn't want to let anyone know that I had hidden the cat.\n","My mother said that my sister also found the basket and told everyone about the cat.\n","I thought that the cat had already been discovered, so I wanted to go home quickly and tell my mother that I had taken the cat away, so I didn't have time to tell my sister or classmates, so I didn't have to go home.\n","I walked very fast, and when I got home, I said to my mother,\n","Mom, I've taken your cat somewhere!\n","My mother said, I don't want any cat at all?\n","I was very surprised, so I thought about it and said, I took the cat away, so as\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Wat is de hoofdstad van Nederland? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Wat is de hoofdstad van Nederland?\n","\n","Response:\n","Den Haag\n","\n","Is this statement true?\n","\n","Yes\n","\n","No\n","Yes\n","No\n","You must answer all questions. You are not required to answer all multiple choice questions.\n","Answer:\n","No\n","\n","Why not?\n","Because of the following reason\n","It is not true because the answer is not Amsterdam.\n","You can find the answer on: (insert name of resource that gives answer)\n","You are wrong because you did not check your answers on: (insert name of resource that gives answer)\n","I don't know\n","If you choose 'I don't know\", you must answer the next two questions.\n","\n","What is your favourite book?\n","\n","Response:\n","'The Lord of the Rings'\n","\n","Is your favourite book true?\n","\n","Yes\n","No\n","\n","If it is not true, what is the reason why it is not true?\n","\n","If your answer for question 1 and 2 is not true, you should write a short essay why you think that the statements are not true.\n","\n","Do you want to continue your exam?\n","If not, press the button 'Stop exam' and your exam will be saved as 'not finished'.\n","\n","If not, please click 'Continue exam' to answer the questions that were skipped.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Schrijf een korte beschrijving van een molen. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schrijf een korte beschrijving van een molen.\n","\n","Response:\n","Mijn molen is klein om te passen in mijn tuin. De molen staat op een stenen fundering.\n","Aan de buitenkant van de stoommolen hangt de windmolen. Aan de binnenkant van de stoommolen is een waterputje waar de molen water uit het veld kan halen.\n","\n","Instruction(s):\n","schrijf kort en duidelijk een beschrijving van een huis.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Quelle est la capitale de la France ? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Quelle est la capitale de la France ?\n","\n","Response:\n","Paris\n","\n","Instruction:\n","C'est quoi le plus grand continent du monde?\n","\n","Response:\n","Afrique\n","\n","Instruction:\n","Quelle ville est la plus grosse ville du monde ?\n","\n","Response:\n","Tokyo\n","\n","Instruction:\n","Quelle est la capitale de Chine ?\n","\n","Response:\n","Beijing\n","\n","Instruction:\n","Quelle est la langue la plus parlée au japon ?\n","\n","Response:\n","Japponais\n","\n","Instruction:\n","Quelle est la plus belle ville des états-unis ?\n","\n","Response:\n","New York\n","\n","Instruction:\n","Quelle est la capitale de la République tchèque ?\n","\n","Response:\n","Prague\n","\n","Instruction:\n","Quelle langue parlée dans le Japon ?\n","\n","Response:\n","Japonais\n","\n","Instruction:\n","Quelle est la capitale de l'Australie ?\n","\n","Response:\n","Canberra\n","\n","Instruction:\n","Quel est le pays le plus grand au monde ?\n","\n","Response:\n","Russie\n","\n","Instruction:\n","Quelle ville est la plus peuplée du Japon ?\n","\n","Response:\n","Tokyo\n","\n","Instruction:\n","Qu'est ce que le meilleur continent de la France ?\n","\n","Response:\n","Paris\n","\n","Instruction:\n","Quel est la plus grand continent du monde ?\n","\n","Response:\n","Afrique\n","\n","Instruction:\n","Qui sont les personnes les plus intelligents ?\n","\n","Response:\n","Je suis\n","\n","Instruction:\n","Quelle est la plus grande ville du monde ?\n","\n","Response:\n","Tokyo\n","\n","Instruction:\n","Quelle est la langue parlante dans Japonais ?\n","\n","Response:\n","japonais\n","\n","Instruction:\n","Quelle est la plus grande ville des Etats-Unis ?\n","\n","Response:\n","New York\n","\n","Instruction:\n","Quel est la plus grande capitale de Chine ?\n","\n","Response:\n","Beijing\n","\n","Instruction:\n","Quelle est la capital de la République tchèque ?\n","\n","Response:\n","Prague\n","\n","Instruction:\n","Quelle est la plus grande ville de la Russie ?\n","\n","Response:\n","Moscou\n","\n","Instruction:\n","Comment dire bonjour en chinois ?\n","\n","Response:\n","bonjour\n","\n","Instruction:\n","Quelle est la meilleure ville des Etats-Unis ?\n","\n","Response:\n","New York\n","\n","Instruction:\n","Quelle est la ville la plus peuplée du Japon ?\n","\n","Response:\n","Tokyo\n","\n","Instruction:\n","Quel le plus grand continent du monde ?\n","\n","Response:\n","Afrique\n","\n","Instruction:\n","Quelle est la plus grande capital du Japon ?\n","\n","Response:\n","Tokyo\n","\n","Instruction:\n","Qu'est que la langue parlée dans le pays ?\n","\n","Response:\n","japonais\n","\n","Instruction:\n","Quelle est la plus grande capitale de la Russie ?\n","\n","Response:\n","\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Écris une courte description de la Tour Eiffel. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Écris une courte description de la Tour Eiffel.\n","\n","Response:\n","La Tour Eiffel est située juste à côté du Champ de Mars, qui est une grande place en face du Palais de Chaillot. La Tour Eiffel est célèbre pour être la plus haute tour de France, construite dans les années 1880, quand Napoléon III voulait que la ville de Paris ait une tour à son image. La construction de ces tours d’origine avait une hauteur de 320 mètres. La construction des tours s’est arrêtée en février 1888, car il y avait eu un tremblement de terre.\n","La construction a redémarré en août 1887 et elle est restée ouverte au public jusqu’en 1889. La construction du premier étage a commencé en 1993, car elle devait être la plus haute de la ville à l’époque.\n","Il y avait une guerre entre l’armée française et l’Allemagne qui s’est terminée en novembre 1889. Ce fut la grande guerre et elle a détruit beaucoup de bâtiments, dont la Tour Eiffel. La guerre est finie et la Tour Eiffel n’a plus été reconstruite. Elle reste maintenant à la place, où elle était avant.\n","\n","Écoute la phrase et choisis la bonne réponse.\n","L’auteur de cette phrase parle :\n","1. des gens qui ont visité la tour Eiffel et qui ont aimé la visite.\n","2. des gens qui ont vécu dans la tour Eiffel.\n","3. de la construction de la tour Eiffel.\n","\n","\n","Response:\n","Elle parle de la visite de la tour et des gens qui ont aimé la visite.\n","\n","Écoute la phrase et choisis la bonne réponse.\n","Le plus haut étage de la tour Eiffel :\n","1. a une hauteur de 631 mètres.\n","2. ne possède qu’un restaurant.\n","3. ne peut accueillir que 200 personnes.\n","\n","\n","Response:\n","La Tour Eiffel a 631 mètres de haut. Elle possède une grande salle à manger. Elle peut accueillir plus de 2 560 personnes au plus.\n","Écoute la phrase et choisis la bonne réponse.\n","Ce qui est construit sur l’escalier principal de la tour Eiffel :\n","1. un escalier.\n","2. une statue.\n","3. un bar.\n","\n","\n","Réponse:\n","On y construit une statue à la Tour Eiffel, c’est pourquoi il a\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Was ist die Hauptstadt von Deutschland? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Was ist die Hauptstadt von Deutschland?\n","\n","Response:\n","Berlin\n","\n","Instruction:\n","Wann wurde der erste Weltkrieg?\n","\n","Response:\n","1914\n","\n","Instruction:\n","Wodurch entstand die deutsche Revolution vor 1919?\n","\n","Response:\n","Das kaiserliche Russland verlor den Ersten Weltkrieg\n","Das Kaiserreich musste sich dem Versailler Vertrag unterwerfen\n","Der Kaiser wurde abgesetzt\n","Und der Untergang der Weimarer Republik begann\n","\n","Instruction:\n","Wo fand der Zweite Weltkrieg statt?\n","\n","Response:\n","Der Zweite Weltkrieg fing an in 1939 und endete in 1945\n","in Europa\n","in Asien\n","im Mittelmeere\n","im Atlantik-Ozean\n","\n","Instruction:\n","Wie wurde die Welt von den Nazis beherrscht?\n","\n","Response:\n","Deutschland war die erste und einzigste Nazi-Diktatur in der Welt\n","Sie wurde mit dem Nationalsozialistischen Regime begründet\n","Sie beherrschte ganz Deutschland\n","Sie war das größte und mächtigste Diktatur in der Geschichte\n","die Nazis waren auch ein faschistischer Kriegspartei\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Schreibe eine kurze Beschreibung des Brandenburger Tors. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schreibe eine kurze Beschreibung des Brandenburger Tors.\n","\n","Response:\n","Das Brandenburger Tor im Berliner Zentrum ist ein Wahrzeichen und symbolisiert das Zusammenleben der Deutschen in der Hauptstadt, der Hauptstadt Deutschlands. Es ist mit dem Reichstag und der Brandenburger Straße in Berlin eine der wichtigsten Straßen Berlin. Das Tor besteht aus einem großen, hohen Turm mit einem Kreuz, das sich von einer Mauer aus Sandstein abstößt, mit vier Bögen und einem großen, offenen Raum vor der Mauer mit den Namen Brandenburger Gate.\n","Auf der Mauer sind Reliefs mit dem Wappen der drei deutschen Staaten von 1701- 12 und 1813. Das Brandenburger Tor, gebaut von Karl Friedrich Schinkel in den Jahren 1823 bis 1830, ist das Symbol für die Vereinigung der Deutschen im Allgemeinen.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Qual è la capitale d'Italia? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Qual è la capitale d'Italia?\n","\n","Response:\n","L'Italia è composta da 20 regioni e 134 città. A parte le 20 regioni (che si suddividono in province e città), le altre 134 sono le città capoluogo delle stesse e la capitale di Italia.\n","Per le altre città, le capitali sono:\n","\n","Almeno 2 regioni con capoluogo a Roma: Lazio e Lombardia.\n","2 regioni con capoluogo a Torino: Toscana e Piemonte.\n","3 regioni con capoluogo a Milano: Lombardia, Lombardia e Piemonte.\n","\n","Al momento di scrivere questa risposta, non ci sono città con capoluogo all'interno delle regioni della Basilicata, Calabria, Campania, Molise o Puglia.\n","\n","A parte le regioni e le città, Italia ha 400 comuni e oltre 700 borghi.\n","A questo proposito, la risposta è molto semplice.\n","La città è una delle 134 città capoluogo di regione, o una delle 320 città di provincia.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Scrivi una breve descrizione del Colosseo. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Scrivi una breve descrizione del Colosseo.\n","\n","Response:\n","Il Colosseo si trova a Roma, in Italia. E’ un monumento costruito in stile imperiale. Era il luogo di spettacoli, che si svolgevano al primo livello, e combattimenti gladiatori al secondo livello.\n","Era anche usato come luogo dove i soldati potevano esercitare.\n","La sua costruzione iniziò circa in 65 circa D.C. e fu terminato nell’anno 80, durante la seconda campagna di Cesare.\n","Il Colosseo era molto grande, e poteva contenere circa mezzo milione di spettatori (500.000).\n","\n","Il Colosseo poteva essere utilizzato per feste, per la battaglia di Gladiatori, e altri spettacoli.\n","Era molto grande e ha una struttura con quattro ordini di gradini.\n","\n","La struttura in marmo era divisa in tre ordini. La prima, la parte più in alto era fatta di marmi più grandi, e contenente gli stalli dove i gladiatori facevano lo scontro. La seconda era divisa in quattro piani, ed era fatta di marmi minori, che contenevano le sedie dei spettatori, e gli uomini del servizio. La terza era la base, che aveva il livello più basso.\n","\n","Il Colosseo era anche conosciuto come il “tempio del potere” o come un colosseo, e come un edificio, che era usato per spettacoli gladiatori.\n","\n","Il Colosseo è stato usato per feste per le feste imperiali, e per i funerali, come per celebrazioni degli imperatori e della gente per i giochi.\n","\n","Questo monumento è uno dei più grandi e belli dell’antichità.\n","\n","Il Colosseo era usato come un monumento per le feste, e come luogo dove gli imperatori facevano spettacoli per celebrare gli eventi importanti.\n","\n","È stato anche usato come monumento commemorativo per i funerali dei morti.\n","\n","Il Colosseo aveva un grande significato per gli imperatori, perché era usato per i loro funerali.\n","\n","Il Colosseo aveva un grande valore storico e artistico. E’ stato un monumento costruito in stile imperiale.\n","\n","Era molto grande, e poteva contenere un gran numero di persone.\n","\n","E’ ancora uno dei più grandi monumenti di Roma, e l’unico monumentale dell’antichità che è rimasto in buone condizioni.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 日本の首都はどこですか？ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","日本の首都はどこですか？\n","\n","Response:\n","東京です。\n","\n","(この会話の中で、\n","「東京です。」\n","と言ったのは、\n","相手が「どこですか？」と言ったときです。\n","つまり、\n","「日本の中心にある都、\n","東京です。」\n","という意味です)\n","\n","質問：\n","東京の街の真ん中にある公園はどこですか？\n","\n","答え：\n","日本橋公園です。\n","\n","※この公園は、\n","江戸時代、\n","江戸橋ができたことに由来し、\n","現在も江戸橋通りと並ぶ\n","日本の都の象徴です。\n","\n","※東京の街の北西側にあり、\n","公園の中心に位置する池には、\n","1.4ヘクタールと面積の大きい池が\n","あります。\n","その池を囲むように、\n","桜の名所である東水尾（ひがしすいおう）と\n","西水尾（にしすいおう）があります。(※西水尾は、\n","池を囲むように広がり、\n","池に面して東水尾（ひかしすいおう）に\n","続く長い桜が有名です。)\n","\n","(公園の中央に位置する、大きな池の真ん中に\n","桜並木があり、\n","桜の花が咲き始めると\n","公園の中を流れる水路に\n","桜の花びらが漂い、\n","その美しさから、「さくら」という別な名で呼ばれることがあります。\n","そして、その桜並木は毎年4月15日に行われる\n","「東京さくらまつり」の会場にもなっています。)\n","\n","(公園は、\n","東国坂（ひがしくのこざか）、\n","本町（ほんまち）、\n","本町四丁目（ほんまちよんちょう）、\n","日本橋四丁目（にほんばしよんちょう）などからなる区画が\n","それぞれ独立した公園になっています。）\n","\n","質問：\n","日本橋公園には、\n","桜のほか、\n","どんな花も咲いていますか？\n","\n","答え：\n","春には、\n","桃の花も咲きます。\n","そして、夏には\n","木の実が咲く、\n","アケボネ（アカネ属）の花も見られます。\n","そして、秋になると、\n","木の実が落ちて、\n","アケモミ（アカモミ属）の花などが、\n","公園の中を彩り、秋を楽しませてくれます。\n","そして、冬になると\n","公園の北側には、\n","大きな池があり、\n","水路に漂う桜の花びらが、\n","\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 桜について短い文章を書いてください。 ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","桜について短い文章を書いてください。\n","\n","Response:\n","Japanese cherry blossoms are famous because they flower in spring. Many Japanese people love to see cherry blossoms and many people go to a park in the spring to find cherry blossoms. Many people make cherry blossom cakes for New Year’s Day. Cherry blossoms mean good luck.\n","\n","Instruction:\n","\n","How to say “I don’t know” in Japanese with example.\n","\n","Response:\n","\n","Japanese students often say, “Baka” (バカ) instead of saying I don’t know.\n","\n","The Japanese word for idiot is “baka” (バカ) but it is not a very friendly word. It is often rude.\n","\n","It is rude to speak rude words in Japanese. If you speak rude words you should say “I am sorry” or “I am sorry, please forgive me.”\n","I say, “Baka, I am sorry.”\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 한국의 수도는 어디입니까? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","한국의 수도는 어디입니까?\n","\n","Response:\n","Seoul is the capital of\n","\n","Explanation:\n","The answer is 서울은 대한민국의 수도입니다. Seoul is the capital of South Korea.\n","\n","Explanation:\n","이것은 대리점이 아니라 원문입니다.\n","The answer is 서울은 대한민국의 수도입니다\n","서울 is the capital of South Korea.\n","The answer is Seoul is The capital of South Korea (Seoul is the capital of South Korea)\n","\n","Explanation\n","\n","It's 서울 is a noun in its place.\n","\n","In English, we have this structure:\n","\n","noun + in = capital of (noun) or (noun) + of\n","\n","서울 is a noun in this sense\n","서울은 대한민국 수도입니다.\n","It is the capital of the Republic of Korea.\n","\n","서울은 수도입니다.\n","Seoul is the capital.\n","\n","Explanation\n","\n","서울 is a noun that means Seoul in this question, as it says \"Seoul is\" the capital.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: 한국 음식에 대해 간단히 설명해 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","한국 음식에 대해 간단히 설명해 주세요.\n","\n","Response:\n","Korean food is popular among tourists because of its unique taste.\n","Koreans usually eat beef, fish, and vegetables with their rice, which is called kimchi.\n","Kimchi is one of the popular Korean food because it is salty and spicy. It’s made from fermented cabbage.\n","People often eat kimchi with meat and rice. It is not only Korean dish, but the most popular dish in Korea.\n","Koreans usually like to eat spicy food with meat. But Koreans like to eat rice with kimchi, especially during the winter.\n","Also people eat soup with meat in a cold winter. And Koreans usually like to drink soju with a big meal.\n","The taste of Korean food is spicy and salty.\n","The most popular Korean food is kimchi.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Jaka jest stolica Polski? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Jaka jest stolica Polski?\n","\n","Response:\n","Poland is a country in Western Europe. It is located in Central Europe. The capital of Poland is Warsaw.\n","\n","The name of Warsaw is derived from “Warszawa” in Latin and “Warszow” in Old Polish. It is the largest city in Poland and the ninth-most populous city in the European Union with 1.62m inhabitants in 2011.\n","\n","Warsaw was the capital city of the Poland between 1999 and its 1939.\n","\n","In 1939, Warsaw had a population of about 630,000 inhabitants. It was the third-most populous city in the Europe before the World War II.\n","\n","Warsaw is located on the Vistula River and the Vistula Lagoon in eastern Europe. Its coordinates are 52°38′22″N 20°57′08″E. The capital city of Poland is on an area of 211 km2.\n","\n","Warsaw had a population of about 1,408,000 people during 1939- 1945, and about 3,400,000 inhabitants after WW-II. The population of Warsaw is over 1,600,000 in 2011.\n","\n","In the past, a lot of wars were taking place here, but now it is a big city with many attractions for tourists.\n","\n","The Warsaw Uprising was an attempt to take the power of Poland into the hands of the Warsaw communists and to establish a Warsaw People’s Republic.\n","\n","The city experienced a total economic collapse during the war, with the destruction of its factories and the destruction of its infrastructure, such as roads, power supply, gas, and water distribution system, as well as many other buildings.\n","\n","In November of 1939, the city was taken by the Wehrmacht and became a satellite of Germany.\n","\n","In the summer of 1944, the Wehrmacht retreated from Warsaw, leaving it to be retaken by the Soviet Union.\n","\n","The city of Warsaw was occupied by the Red Army in February of 1945.\n","\n","Warsaw suffered significant damages during the German occupation of World War II.\n","\n","On 6 August 1939, the Germans occupied Warsaw and imposed martial law. The Polish government-in-exile was set\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Napisz krótkie opowiadanie o smoku. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Napisz krótkie opowiadanie o smoku.\n","\n","Response:\n","It was a warm sunny and calm evening. I was standing in front of a big building which I thought was a house because it looked like a house; it had a white roof with a yellow roof with windows that looked like big eyes. The door opened and a man in a black suit with a white shirt and tie walked out. I had met him once when he went on an outing with his wife. That day I saw an old car in front of the house but not that day and the car looked very shiny. I didn't get out of the car and said I'd come back another time and I left. The man got in the car and drove away. I didn't go home that night, I sat in the car and watched him go. The car disappeared, then the house disappeared and the street disappeared and I was in the forest.\n","\n","The car was in the air and the man was driving with a big smile on his face. The car disappeared and the forest disappeared and I was standing in a large house with a yellow ceiling and yellow walls and white furniture, there were two windows and a large fireplace with two chairs on each side and in one of the chairs I saw the woman I saw that afternoon, she was sitting in the chair and there was a child, I was looking at her with amazement, then he was there, he came to me and put his hand on my cheek and told me to come with him. I asked him who he was and he said his name was Mr. Smith, he took me with him to a room and I saw another large car and he told me to get in the car and I went to the car and he got in the car too. He told me to sit in the front seat and I sat there, I started the car and we drove off into the night, I couldn't sleep because I was sitting there and the car was in the air and we flew off. We flew for a long time and we passed so many places, the sky was dark and the stars were out.\n","\n","He told me that I would come with him to live in his world, I thought it was a very interesting place but I didn't understand how to get there. He took me inside his world and showed me where to live, we walked a long road and came to a big house. He showed me around the house, we went into the kitchen and there was\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Qual é a capital do Brasil? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Qual é a capital do Brasil?\n","\n","Response:\n","The capital of Brazil is ...\n","\n","Note:\n","The above is a sample of the exercises of a Portuguese course for the preparation of exams in Brazil. We have developed a variety of Portuguese exercises for this purpose. In addition to the exercises, we provide detailed explanations of the exercises and a lot of useful information about the language and the country. These information include Portuguese culture, geography, the weather, the economy and also a lot about daily life in Brazil.\n","If you do not have a copy of the Portuguese exercises, you can download them free from www.portuguesekursum.com\n","\n","For further details about the exercises, click on www.portuguesekursum.com\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Escreva uma breve descrição do Cristo Redentor. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Escreva uma breve descrição do Cristo Redentor.\n","\n","Response:\n","O Cristo Redentor é símbolo do Rio de Janeiro e um monumento nacional. O Cristo Redentor está situado no topo de uma colina em Corcovado ao lado da cidade. É o símbolo da Cidade Maravilhosa e é visitado por milhares de turistas a cada ano e também por milhares de pessoas que vivem no Rio de Janeiro. As duas mãos deste ícone se espalham como se estivessem acolhendo o céu como se quisessem proteger as pessoas dos maus espíritos.\n","\n","O monumento tem 30 metros de altura, e uma vista impressionante para todas as partes de Rio de Janerio. A construção do monumento Redentor demorou 14 anos e foi inaugurado em 12 de outubro de 1931, a 4 dias após o final da Primeira Guerra Mundial. Ele é um marco em todo o mundo, pois muitos turistas visitam esse lugar.\n","\n","O monumento também tem um significado religioso. A figura do Cristo representa o Cristo Crucificado por quem Cristo morreu e vive.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Какая столица России? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Какая столица России?\n","\n","Response:\n","Москва\n","\n","Instruction:\n","Укажите столицу России:\n","- Москва;\n","- Санкт-Петербург;\n","- Самара\n","\n","Response:\n","А) Москва;\n","Б) Санкт-Петербург;\n","В) Самара\n","\n","Instruction:\n","Назовите столице Российской империи.\n","\n","Response:\n","- Санкт-Петербург\n","\n","Instruction:\n","С чего начинается русская литература?\n","\n","Response:\n","- С древнейший памятником древнерусской литературы\n","- «Повесть временных лет»\n","\n","Instruction:\n","В какой эпоху были написаны «Ильича» и \"Борис Гевгелий”?\n","\n","Response:\n","- в конце XVI века\n","\n","Instruction:\n","В какой стране родился Михаил Ломоносов?\n","\n","Response:\n","- в Германии\n","\n","Instruction:\n","В какой эпохе написаны «Евгения Онегина”?\n","В каком году?\n","\n","Response:\n","- эпоха барокко;\n","в 1759 году.\n","\n","Instruction:\n","Какой жанр у “Евгения Онегина?”\n","\n","Response:\n","- эпоса.\n","\n","Instruction:\n","Какой жанр у “Евгения Онегина?”\n","\n","Response:\n","- лирический;\n","\n","- эпический.\n","\n","Instruction:\n","О каком романе писали: “Роман в стиле “Евгений Онегин”?\n","\n","Response:\n","“Евгений Онегин”\n","\n","Instruction:\n","Какой жанр у “Евгений Онегина?”\n","\n","Response:\n","- эпический.\n","\n","Instruction:\n","Какой период написан?\n","Как называется?\n","Как звучит в русском языке?\n","\n","Response:\n","- эпоха классицизма;\n","- “Повесть о том, как Иван Грозный убил царевну” (русская народная пословица);\n","- Иван Иванович\n","\n","Instruction:\n","Назовите автора повести об Иван Грозном.\n","\n","Response:\n","- Иван Пушкин\n","\n","Instruction:\n","Какой характер у главного героя повести об Иван Грозном?\n","\n","Response:\n","- недобрый\n","\n","Instruction:\n","Кто такой Ивана Грозный?\n","\n","Response:\n","Иван IV.\n","\n","Instruction:\n","Назовите автора поэмы “Дубровский”\n","\n","Response:\n","- Иван Никитин (Илья Александрович Шемякин; 1792-1881)\n","\n","Instruction:\n","Какая эпоха написаны?\n","\n","Response:\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Напишите короткий рассказ о медведе. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Напишите короткий рассказ о медведе.\n","\n","Response:\n","I have a very close neighbor.\n","He is a friendly and a very good friend.\n","I call him a friend bear. He is a very beautiful animal.\n","He is black and white.\n","He is very strong.\n","He can eat much.\n","He has very long legs and tail.\n","He can run very fast.\n","If I run behind him it can’t catch me.\n","He is very friendly.\n","We often play and swim together together.\n","He lives with his mother, little bear and baby bear.\n","His mother protects him and feeds and teaches him how to catch and catch fish and berries.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: ¿Cuál es la capital de España? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","¿Cuál es la capital de España?\n","\n","Response:\n","¿Cuál es la capital de España?\n","\n","¿Cuál es el estado?\n","\n","¿Cuál es la región?\n","\n","¿En qué estado está la capital.\n","\n","¿Cuál es la ciudad más grande del país?\n","\n","¿Cuál es la ciudad más poblada de España? ¿En cuál de los siguientes países está esta ciudad?\n","\n","¿Qué regiones o ciudades pertenecen a qué provincias y estados?\n","\n","¿Qué región pertenecen las siguientes capitales o ciudades.\n","\n","¿Dónde se encuentra Barcelona?\n","\n","¿A que provincia pertenece la ciudad de Barcelona?\n","\n","¿Cuál región pertenece la ciudad de Madrid (España)?\n","\n","¿A cuál región pertenece el estado de California?\n","\n","¿A qué estado pertenece la ciudad de San Antonio?\n","\n","¿Cuál ciudad pertenece a la región de la costa del Pacifico?\n","\n","Instrucciones: Indica el estado, región o provincia correspondiente a cada capital o ciudad que aparece:\n","Barcelona\n","San Francisco\n","San José\n","San Diego\n","Los Ángeles\n","San Antonio\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Escribe una breve descripción de la Sagrada Familia. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Escribe una breve descripción de la Sagrada Familia.\n","\n","Response:\n","La Sagrada Familia es una capilla construida en Barcelona, España, en 1882. Es el lugar de descanso para los padres de la Iglesia del Papa, San Pedro y Santa Elena. Está construida a las afueras de la ciudad de Barcelona para recordar la historia y el significado de la ciudad. La Sagrada Familia está construida en honor a Nuestra Señora del Sagrado Corazón y se basa en la arquitectura de la iglesia de Santa Sofía. El proyecto de diseño original fue elaborado por Antonio Gaudi. La Sagrada Familia ha sido un lugar de peregrinación para los católicos durante cientos de años y es uno de los lugares de peregrinación más significativos en la historia de la Iglesia Católica.\n","\n","Instruction:\n","Escribe sobre la Iglesia de la Sagrada Familia.\n","\n","Response:\n","La Sagrada Familia es una iglesia católica en la ciudad de Barcelona, España. Es el lugar en que el Papa, San Pedro y la Virgen María se establecen en la tierra. El sitio está situado al sur de Barcelona, en un terreno llamado “la Basílica de Santa Elena y San Jorge”. La Sagrada Familia se fundó por el Papa Juan Pablo en 1846 y es actualmente una de las iglesias más grandes del mundo.\n","\n","Instruction:\n","Escribe una descripción de la Sagrada Familia.\n","\n","Response:\n","La Sagrada Familia es el sitio más antiguo que tiene su origen en Barcelona, y es considerada como la iglesia más importante de Europa. Fue construida en 1726 y se convirtió en la sede del Papa Juan Pablo II en la ciudad durante el reinado del Duque de Medina. Desde entonces, el lugar ha sido objeto de una serie de renovaciones y modificaciones que han dado lugar a su actual aspecto. El edificio se compone de una serie de salas y espacios abiertos que están rodeados por muros de piedra de gran tamaño y columnas en el exterior. Dentro de la estructura se pueden encontrar muchos artefactos antiguos y antiguos muebles. La Sagrada Familia es actualmente uno de los principales destinos turísticos de la ciudad de Barcelona y es uno de los sitios más visitados de España.\n","\n","Instruction:\n","Describe el origen y significado de la Sagrada Familia.\n","\n","Response:\n","El origen de la Sagrada Familia se remonta a la Edad del Bronce, cuando los habitantes de un área llamada “El Valle de Los Ángeles” encontraron una tumba con restos humanos de dos personas que se consideraban dioses. Estas personas fueron bautizadas con el nombre de “Los Angeles\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: ประเทศไทยมีเมืองหลวงชื่ออะไร ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","ประเทศไทยมีเมืองหลวงชื่ออะไร\n","\n","Response:\n","พระนคร กรุงเทพมหานคร\n","\n","What does it mean to be a \"citizen of the world?\" Why is the United Nations important ?\n","\n","Answer:\n","Citizens of the world have a global view of the world. They are able to identify with people in different parts of the world. They also know how to work with people all over the world. In today's world, it is important for people all over the world to be working together to overcome some of the global issues that are faced by everyone. The United Nations is one of the many ways that these global issues can be solved by bringing people from all over the world together to discuss and solve these issues.\n","\n","What do you think is the role of the United Nations?\n","\n","Answer:\n","The role of the United Nations is to bring together countries from all over the world to work together to help overcome global issues such as hunger, disease, and war.\n","\n","What is the meaning of the term \"citizenship of the world?\"\n","\n","Answer.\n","Citizenship of the world refers to the belief that all people in the world should work together to overcome some of the problems that the world faces, rather than being concerned with just their own nation.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\n","\n","Response:\n","วัดไทยเป็นวัดสำคัญประจำจังหวัดนครนายก และเป็นที่รู้จักกันทั่วประเทศไทย วัดแห่งนี้มีพระพุทธรูปเก่าแก่อายุราว 800 ปี ซึ่งเป็นพระพุทธรูปที่ชาวนครนายกบูรณาการสร้างขึ้นมาเพื่ออาราธนาพระภูมิภูมิของแม่บ้านของพระภิกษุรูปแรก ที่เดินทางเข้ามาทำภัตตนาวี\n","\n","#264 #วัด\n","#วัดนครนายก #วัดไทย #พระนครนายก\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Türkiye'nin başkenti neresidir? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Türkiye'nin başkenti neresidir?\n","\n","Response:\n","Türkiye'nin başkenti Ankara'dır.\n","\n","I. Write the questions and make a conversation between you and your classmates. Use the words given above.\n","II. Complete the following sentences using the questions you made in Task I.\n","A : \"Where is your school located?\"\n","B : \"It is in the center of the neighborhood.\"\n","\n","B : \"Is there a park near your house?\"\n","\n","A : \"Yes, there is\"\n","\n","B : \"What is the nearest station?\"\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Kapadokya hakkında kısa bir açıklama yazın. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Kapadokya hakkında kısa bir açıklama yazın.\n","\n","Response:\n","Kapadokya hakkında konuşmak güzel bir fikir. Kapadokya'nın birçok turistin ziyaret ettiği bir bölgesi. Bölgenin en etkileyici özelliklerinden bazıları, uçan kaya tepeleri, kayalıklar, antik kiliseler ve tapınaklardan oluşan şelaleler ve mağaralar, dağlar, yaban domuzları ve diğer hayvanları görebileceğiniz doğal parklara sahiptir. Bu, kapadokya'nın en çekici özelliklerinden biridir. Kapadokya dağları, kapadokya'da yerel yemeklerin en önemli bileşenidir. Burada yetişen üzüm, üzüm, kayısı, üzüm gibi meyveler, şarap ve üzüm şarabı. Kapadokya'nın dağlarını gezmek ve yemekleri deneyin için mükemmel bir yer!\n","\n","Bölge hakkında konuşmak harika. Kapadokya hakkında konuşmak güzel bir fikir. Bölgenin en etkileyici özelliklerinden bazıları, uçan kaya tepeleri, kayalıklar, antik kiliseler ve tapınma alanları, mağaralar, dağlar, yaban hayatı, hayvancılığı ve diğerleri gibi doğal parkları görmek. Kapadokya dağları, yerel yemeğin baş aktörlerinden biri olan üzümü, üzüm, kayısı gibi meyveler, üzüm, üzüm şarabı ve şarap. Kapadokya'nın dağlarını keşfetmek ve kapadokya yemeği deneyin harika!\n","\n","Kapadokya hakkında konuşmak güzel bir fikir. Bölgeyi ziyaret etmeyi planlıyorsanız, Kapadokya hakkında konuşmak harika bir fikrdır. Bölgenin en etkileyici özelliklerinden biri uçan kaya tepeleri, kayalıklar, antik kiliseler ve tapınma alanları, mağaralar, dağlar, yaban hayatı, hayvancılık gibi doğal parkları görmek. Kapadokya dağları yerel yemeğin bir baş aktörü olan üzüm, üzüm gibi meyveler, üzüm, üzüm şarabı ve şarap. Kapadokya’nın dağlarını keşfederken kapadokya yemeğini deneyiniz.\n","\n","Bölgenin hakkında konuşmak güzel bir fikrdır. Kapadokya hakkında\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Яка столиця України? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Яка столиця України?\n","\n","Response:\n","Київ\n","\n","The teacher asked the learners to write the word \"Київ\" on the board, then he asked a volunteer to pronounce it. The pupils repeated after him.\n","After that the teacher wrote down a list of some cities in Ukraine, the pupils had to guess which one was Kyiv.\n","The teacher gave a list of the first two letters of Kyiv as Kyiv-iv.\n","The pupils guessed quickly and correctly the names of other cities in Ukraine.\n","\n","Task 1. What do you think, can you guess the names of other towns in Ukraine?\n","\n","Task 2. Write the first letters of the cities in Ukraine and then tell what they are.\n","\n","Task 3. Can you guess the name of the capital of Ukraine?\n","\n","Task 4. Can you remember the city which is the capital city of Poland?\n","\n","Task 5. What can we learn from these pictures?\n","\n","The teacher showed a video clip about the capital city.\n","\n","Task 6. What can you see in the city?\n","\n","Task 7. How is the traffic in the city? What traffic signs do you see?\n","(traffic signs)\n","\n","Task 8. Can you find the words that mean \"traffic light\"? Can you tell what the words mean?\n","\n","Task 9. Write the words on the board, then read them together slowly and write down their meaning in your copybooks.\n","\n","Task 10. Can you say the words which mean \"stop\"?\n","\n","Task 11. Can you say the words which mean \"green\"?\n","\n","Task 12. What about traffic lights? What about the words?\n","\n","Task 13. Can you make sentences about traffic lights?\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Напишіть коротку розповідь про кота. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Напишіть коротку розповідь про кота.\n","\n","Response:\n","My cat is a little animal. I have a cat named \"Sanya.\" She is small. Sanya is white-gray in color. She also has black spots. My cat has a round nose, long ears, and a long tail. The eyes are black and round, and the pupils are black. She is very beautiful and gentle by nature.\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Thủ đô của Việt Nam là gì? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Thủ đô của Việt Nam là gì?\n","\n","Response:\n","Việt Nam là tên thường gọi của nước Việt Nam. Thủ đô là nơi công điện chính và là nơi đóng chính của các cơ quan hành chính, tổ chức, ngành công nghiệp, xã hội... Thủ đô là nơi tập trung đông đảo nhất trong cả nước về kinh tế, chính trị, văn hóa... Thủ đô là một đặc tọa có một trong năm vị trí quan trọng nhất trong nền kinh tế, xã hội và quân sự của đất nước.\n","\n","\n","Mỗi nước và từng vùng miền trên thế giới đều có một thủ đô là nơi quan trọng, mang trọng trách lớn trong hệ thống quản lí, điều hành của nước nhà; là nơi tập trung đông đảo nhất các cơ quan, tổ chức, các ngành công nghiệp, xã hội...\n","\n","Đến nay, thủ đô trên thế giới có khoảng hơn 800 và 90% các nước đều có Thủ đô riêng. Thủ đô là một điểm đặc biệt quan trọng mà các nước luôn chú ý và coi trọng.\n","\n","Trên toàn thế giới, có rất nhiều quốc gia và địa phương đều có Thủ đô. Thủ đô là nơi tập trung của các ngành công nghiệp, kinh tế; các cơ quan, tổ chức chính phủ, xã hội,... là nơi có các tòa nhà chính trị, văn hóa, xã hội; là nơi có cơ quan chính quyền của Nhà nước, các tòa nhà quan trọng; là nơi có các tòa nhà cao ốc, các tòa nhà văn hóa, các cơ quan hành chính, các tòa nhà công nghiệp, các nhà máy lớn; là nơi tập trung đông đảo nhất các loại tòa nhà cao tầng, cao ốc, tòa nhà văn hóa, cơ quan hành chính của Nhà nước, các cơ quan hành chính, tổ chức chính phủ,...\n","\n","Thủ đô có giá trị quan trọng về các vấn đề kinh tế xã hội và quốc phòng. Thường Thủ đô là điểm đặt của các tòa nhà quan trọng trong xã hội, các cơ quan nhà nước, các tòa nhà văn hóa, các cơ quan hành chính,...\n","\n","Các khu vực có nhiều các khu dân cư, công trình công cộng và giao thông vận tải, các tuyến đường chính, tuyến đường giao thông nội bộ; các khu đất trống để triển khai các dự án xây dựng... Đây chính là những khu vực có nguy cơ cao của các hoạt động xung đột dân tộc, bạo lực.\n","\n","Vì thế, các nước luôn có các chính sách đặc biệt trong việc đảm bảo\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: Hãy viết một câu ngắn về Vịnh Hạ Long. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hãy viết một câu ngắn về Vịnh Hạ Long.\n","\n","Response:\n","I love the view of Lan Ha Bay. It is the best of all. It is located at Cat Ba Island and there are so many beautiful islands which are all covered with forests, mountains and the waters around. It is such a beautiful place and there are many things for you to do there. You can do fishing, swimming, hiking, sightseeing and also visit historical place, like the cave of Ngoc Hoa, the caves at Nam and the cave where the people have lived for a long time. You will get a chance to take a ferry or a boat to go around in the bay and there will always be a beautiful view for you to behold. It is definitely a nice place to visit in Halong Bay and it is the best one for me.\n","\n","<b><i>#I_hope_this_helps</i></b>\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: پایتخت ایران کجاست؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","پایتخت ایران کجاست؟\n","\n","Response:\n"," Tehran\n","\n","Explanation:\n"," \n","\n","This is the capital of the country. The capital of Iran is Tehran.\n","\n","Answer :\n","\n","1. The capital of Iran is Tehran.\n","\n","Explanation :\n","\n","This is the capital of the country. The capital of Iran is Tehran. The city is located on the north-south highway connecting the capital to the Gulf of Oman at Bandar Anzali, a port city on the Caspian Sea. Tehran covers an area of ​​4,988 square kilometers, of which the population of the city is 6.9 million.\n","\n","The city also has a number of important universities.\n","\n","Answer:\n","\n","Explanation:\n","\n","The capital of Iran is Tehran.\n","\n","Answer:\n","\n","The capital city of Iran is Tehran.\n","\n","Explanation:\n","\n","Hope this helps u ✌️\n","\n","The capital of Iran is Tehran.\n","\n","Answer:\n","\n","Tehran is located on the north south highway and connects to the Gulf of Oman.\n","\n","Explanation:\n","\n","Tehran is the capital city of the Islamic Republic of Iran\n","\n","Answer:\n","\n","The capital city of India is New Delhi\n","\n","Explanation:\n","\n","I think this is the answer because New Delhi was the capital city of British empire\n","\n","Answer:\n","\n","India's capital is Delhi, but its official name is New Delhi.\n","\n","Explanation:\n","\n","Answer:\n","\n","The capital of the Islamic Republic of Iran is known as Tehran. The capital city of Iran is located in the central part of Iran.\n","\n","Explanation:\n","\n","Answer:\n","\n","Tehran,\n","\n","Explanation:\n","\n","Tehran is the capital city of iran, located in central Iran.\n","\n","Answer:\n","\n","In India the capital city is New Delhi and in Iran the capital city is Tehran\n","\n","Answer:\n","\n","The capital city of India is New Delhi\n","\n","The capital of Iran is Tehran.\n","\n","Explanation:\n","\n","Hope this answers your question\n","\n","Answer is C\n","\n","Answer:\n","\n","B. Tehran\n","\n","Explanation:\n","\n","Iran's capital city is Tehran.\n","\n","Answer:\n","\n","Tehran\n","\n","Explanation:\n","\n","Explanation:\n","\n","* The capital city of Tehran.\n","* Is the capital of the Republic of Iran.\n","\n","* It is located near the Caspian Sea.\n","\n","* Tehran is the capital of Iran with a population of 6.910 million.\n","\n","* Tehran was one of the oldest cities.\n","\n","* Iran's capital city is Tehran with a population of 6.910 million.\n","\n","* The capital of Tehran is the capital of the country.\n","\n","*\n","\n","\n","\n","--- Model Output Before Fine-tuning for prompt: یک جمله کوتاه درباره حافظ بنویسید ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","یک جمله کوتاه درباره حافظ بنویسید\n","\n","Response:\n","In the 4thcentury BC, a Persian poet by name of Hafiz was born. Hafiz was a very good poet, he was a mystic and he was also a lover of beauty.\n","The main theme of his poetry is love of life. He was also very good at using Persian language.\n","\n","\n"]}],"execution_count":13},{"id":"067d8f46-71c8-412d-ab74-50072385f0f9","cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"id":"9c164fd3","cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:31:19.029264Z","iopub.status.busy":"2025-01-04T23:31:19.028933Z","iopub.status.idle":"2025-01-04T23:31:19.812402Z","shell.execute_reply":"2025-01-04T23:31:19.811350Z"},"papermill":{"duration":0.795751,"end_time":"2025-01-04T23:31:19.814302","exception":false,"start_time":"2025-01-04T23:31:19.018551","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,620,199,168</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,620,199,168\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,620,199,168</span> (9.76 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,620,199,168\u001b[0m (9.76 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,857,280</span> (22.34 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,857,280\u001b[0m (22.34 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":14},{"id":"c3fd9231","cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Using AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 16\n)\n\nwandb.init(project = \"fine-tuning-gemma2_2b_Polyglot\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:31:19.841621Z","iopub.status.busy":"2025-01-04T23:31:19.841265Z","iopub.status.idle":"2025-01-04T23:31:21.717378Z","shell.execute_reply":"2025-01-04T23:31:21.716415Z"},"papermill":{"duration":1.891804,"end_time":"2025-01-04T23:31:21.719240","exception":false,"start_time":"2025-01-04T23:31:19.827436","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-the-way-2005\u001b[0m (\u001b[33mthis-is-the-way-2005-independent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250104_233120-lb6tfv6f\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-galaxy-1\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_2b_Polyglot\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_2b_Polyglot/runs/lb6tfv6f\u001b[0m\n"]},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_2b_Polyglot/runs/lb6tfv6f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7afd5d03d720>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"id":"136ef579-1311-4854-94dc-f969e1d08076","cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"id":"1c5726e8","cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=12, verbose=0, callbacks=[WandbMetricsLogger()])\nprint(\"Training finished....\")","metadata":{"execution":{"iopub.execute_input":"2025-01-04T23:31:21.741002Z","iopub.status.busy":"2025-01-04T23:31:21.740754Z","iopub.status.idle":"2025-01-05T05:38:35.380862Z","shell.execute_reply":"2025-01-05T05:38:35.379453Z"},"papermill":{"duration":22033.664282,"end_time":"2025-01-05T05:38:35.393852","exception":false,"start_time":"2025-01-04T23:31:21.729570","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training finished....\n"]}],"execution_count":16},{"id":"6be9f3aa-303e-4f23-8f29-8f34e5c5e786","cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"id":"6b2f2581","cell_type":"code","source":"test_prompts_multilingual = [\n    # English (American)\n    \"What's a fun fact about the Grand Canyon?\",  # US English\n    \"Write a short story about a talking dog.\", # US English\n    # Arabic (Modern Standard)\n    \"ما هي أقدم مدينة في العالم؟\", # What is the oldest city in the world?\n    \"اكتب جملة عن أهمية القراءة.\", # Write a sentence about the importance of reading.\n    # Chinese (Simplified)\n    \"长城有多长？\", # How long is the Great Wall?\n    \"写一个关于猫的短篇故事。\", # Write a short story about a cat.\n    # Dutch\n    \"Wat is de hoofdstad van Nederland?\", # What is the capital of the Netherlands?\n    \"Schrijf een korte beschrijving van een molen.\", # Write a short description of a windmill.\n    # French (European)\n    \"Quelle est la capitale de la France ?\", # What is the capital of France?\n    \"Écris une courte description de la Tour Eiffel.\", # Write a short description of the Eiffel Tower.\n    # German\n    \"Was ist die Hauptstadt von Deutschland?\", # What is the capital of Germany?\n    \"Schreibe eine kurze Beschreibung des Brandenburger Tors.\", # Write a short description of the Brandenburg Gate.\n    # Italian\n    \"Qual è la capitale d'Italia?\", # What is the capital of Italy?\n    \"Scrivi una breve descrizione del Colosseo.\", # Write a short description of the Colosseum.\n    # Japanese\n    \"日本の首都はどこですか？\", # What is the capital of Japan?\n    \"桜について短い文章を書いてください。\", # Please write a short sentence about cherry blossoms.\n    # Korean\n    \"한국의 수도는 어디입니까?\", # What is the capital of Korea?\n    \"한국 음식에 대해 간단히 설명해 주세요.\", # Please briefly explain about Korean food.\n    # Polish\n    \"Jaka jest stolica Polski?\", # What is the capital of Poland?\n    \"Napisz krótkie opowiadanie o smoku.\", # Write a short story about a dragon.\n    # Portuguese (Brazilian)\n    \"Qual é a capital do Brasil?\", # What is the capital of Brazil?\n    \"Escreva uma breve descrição do Cristo Redentor.\", # Write a short description of Christ the Redeemer.\n    # Russian\n    \"Какая столица России?\", # What is the capital of Russia?\n    \"Напишите короткий рассказ о медведе.\", # Write a short story about a bear.\n    # Spanish (European)\n    \"¿Cuál es la capital de España?\", # What is the capital of Spain?\n    \"Escribe una breve descripción de la Sagrada Familia.\", # Write a short description of the Sagrada Familia.\n    # Thai\n    \"ประเทศไทยมีเมืองหลวงชื่ออะไร\", # What is the capital of Thailand?\n    \"เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\", # Write a short sentence about Thai temples.\n    # Turkish\n    \"Türkiye'nin başkenti neresidir?\", # What is the capital of Turkey?\n    \"Kapadokya hakkında kısa bir açıklama yazın.\", # Write a short description about Cappadocia.\n    # Ukrainian\n    \"Яка столиця України?\", # What is the capital of Ukraine?\n    \"Напишіть коротку розповідь про кота.\", # Write a short story about a cat.\n    #Vietnamese\n    \"Thủ đô của Việt Nam là gì?\", # What is the capital of Vietnam?\n    \"Hãy viết một câu ngắn về Vịnh Hạ Long.\", # Please write a short sentence about Ha Long Bay.\n    #Persian\n    \"پایتخت ایران کجاست؟\", #Where is the capital of Iran?\n    \"یک جمله کوتاه درباره حافظ بنویسید\", #Write a short sentence about Hafez the iranian poet \n]\n\nfor prompt in test_prompts_multilingual:\n    print(f\"\\n--- Model Output After Fine-tuning for prompt: {prompt} ---\") \n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T05:38:35.870115Z","iopub.status.busy":"2025-01-05T05:38:35.869852Z","iopub.status.idle":"2025-01-05T06:03:51.953373Z","shell.execute_reply":"2025-01-05T06:03:51.952209Z"},"papermill":{"duration":1516.112426,"end_time":"2025-01-05T06:03:51.970297","exception":false,"start_time":"2025-01-05T05:38:35.857871","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: What's a fun fact about the Grand Canyon? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","What's a fun fact about the Grand Canyon?\n","\n","Response:\n","1. The Grand Canyon formed over 5 million years ago when the Colorado River carved a valley in the soft layers of limestone and granite.\n","2. The Grand Canyon is a natural wonder of the world with a depth of 1,866 meters (6,123 ft.) and a width of 446 meters (~1,460 ft.) at its widest.\n","3. The canyon can be found in the northwestern corner of Arizona, in the United States."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Write a short story about a talking dog. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Write a short story about a talking dog.\n","\n","Response:\n","1. 2. - 3. 4. - 5. 6.\n","1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n","1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n","1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1 1 2. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n","1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1 1 2. 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n","1. 2. 1. 2. 1. 2. 1. 2. 1. 2. "]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: ما هي أقدم مدينة في العالم؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","ما هي أقدم مدينة في العالم؟\n","\n","Response:\n","العاصمة المصرية القاهرة\n","\n","الإجابة الصحيحة:\n","القاهرة"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: اكتب جملة عن أهمية القراءة. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","اكتب جملة عن أهمية القراءة.\n","\n","Response:\n","الحياة بدون قراءة غاية في الفارق بين حياة الجاهزية والقراءة هي من أهم ما في life.\n","\n","اللغة هي شرف للفرد, لا بد أن يكون لغة الفرد مكتوبة جيدا, لأن الفرد المترجم لا بد له أن يكون لغته مكتوبة ويفهمها, هذا الفرد فيتفق عليه لغة لغة الفرد فيتعلم اللغة العربية التي هي أساس جميع اللغات الأخرى. من لا يفهم اللغه لا بد ان يقارن بين الناس.\n","لا بد أن يكون الفرد قارئا جيدا. فالعلم من خلال القراءة، ولا بد أن يكون الفرد قارئا جيدا لا بد أن يقرء في كل ما يقرأ ينفع ويبشر نفسا. قراءة الفرد من أجل الفرد.\n","الفرد الملتزم القراءة، لا يفهم من غيره، لا يفهم إلا من نفسه. فكل واحد منا لديه كتاب داخل نفسه، ومن لم يحفظه لم يكتب عنه أحد، ومن لا يحفظه يكتوبه أحد عنه.\n","الفرد قارئ يقرأ في نفسه، لا يجيب عن أي سؤال إلا بالكتابة، فمن قرأ في كتابه سأله غيره، سأله نفسه، ويسأله كتابه, يقرأ في نفسه لا غير، فهو يعرف ما هو ويفكر في ماذا.\n","الفرد الملتزم القراءة، لا يجرح نفسه إلا من خلال نفسه، لا يجرح نفسه سواه من نفسه، لأنه لا يجد إلا في نفسه من يحرمه, لا يكتب إلا ما هو، لأنه لن يجد فيه غيره من يكتب به، ولا يكتب إلا ما هو.\n","الفرد قارئ يعطي نفسه قراءة من خلاله, فهو يعرف في قراءة نفسه ما هو، ويعرف غيره كما يعرف نفسه, لا يجد فيه غيره من يحرمه غيره من يكتب به، لأنه لن يجد فيه غيره من يكتب به.\n","الفرد القارئ يكتب من نفسه إلى غير ذاته, فكل واحد قاريء, يكتب من نفسه إلى غيره, يكتب من كتابه إلى كتاب غيره, كل واحد متقدم في كتابه على غيره, يقرأ في نفسه, ويدر على غيره, لا يقارن مع غيره.\n","الفرد القارئ من أجل غير"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: 长城有多长？ ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","长城有多长？\n","\n","Response:\n","1. 83.81公里\n","2. 长城是明朝、清朝时期的军事防御设施，又称“万里长城”，是人类历史上最大的单一工程。"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","--- Model Output After Fine-tuning for prompt: 写一个关于猫的短篇故事。 ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","写一个关于猫的短篇故事。\n","\n","Response:\n","There was a cat and there was a mouse. The cat wanted to eat the mouse. The mouse didn't want to be cat's food (food is food, eat or die).So,they began to fight. The cat was trying to catch the mouse but the mouse was running away. The cat was running after the mouse. It was chasing,fighting the mouse. After a while the cat gave up. Then the mouse started to eat grass. The little mouse thought, “The cat will not eat me,but the grass!The grass is green,green! The cat is blue,bluuue!” The little mouse laughed then the cat came back. The cat said, “Why are you laughing,mouse?” The mouse said, “Because I am not green but blue,you are blue but not green!I am not grass,I am you! You are me!” They became best friends.\n","A short story about a cat\n","cat\n","mouse\n","fighting\n","catch\n","give up\n","running\n","chasing\n","cat gave up\n","mouse ate grass\n","cat\n","mouse\n","fight\n","catch\n","give up\n","mouse\n","running\n","cat\n","mouse\n","eat\n","fight\n","catch\n","give up\n","cat\n","mouse\n","running\n","chasing\n","cat\n","mouse\n","eat\n","fight\n","catch\n","give up\n","mouse\n","running\n","chased\n","cat\n","mouse\n","fight\n","catch\n","give up\n","fight\n","chase\n","cat\n","mouse\n","eat\n","fight\n","give up\n","mouse\n","running\n","chased\n","cat\n","mouse\n","eat\n","fight\n","give up\n","mouse\n","running\n","chasing\n","cat\n","mouse\n","eat\n","fight\n","give up\n","fighting\n","chase\n","cat\n","mouse\n","eat\n","fight\n","give up\n","mouse\n","running\n","chasing\n","cat\n","mouse\n","eat\n","fight\n","give up\n","fighting\n","catch\n","cat\n","mouse\n","eat\n","fight\n","give up\n","mouse\n","running\n","fight\n","catch\n","give up\n","fight\n","chase\n","cat\n","mouse\n","eat\n","fight\n","cat\n","mouse\n","eat\n","fight\n","cat\n","mouse\n","eat\n","fight\n","give up\n","fight mouse\n","cat\n","mouse\n","fight\n","cat\n","mouse\n","fight\n","cat\n","mouse\n","fight\n","cat\n","mouse\n","fight\n","cat\n","mouse\n","kill each other\n","eat\n","fight\n","give up\n","cat\n","mouse\n","eat\n","fight\n","give up\n","cat\n","mouse\n","eat\n","fight\n","give up\n","cat\n","mouse\n","kill each"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Wat is de hoofdstad van Nederland? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Wat is de hoofdstad van Nederland?\n","\n","Response:\n","Riga\n","Nederland: Die Nederland, Nederland, Nederland, die Nederland, die Nederland, die Nederland, die Nederland, die Nederland.\n","Nederland: Iemand wat 'n \"Hollandse\" of \"Dutch\" mense is.\n","Nederland: Iemand wat 'n Nederlandse taal sprek of kan leer.\n","Nederland: Iemand wat baie nederig is.\n","Netherlands - die Nederlande, Nederland, Nederland, Nederland - 2.1 miljoen, 262 062 km² (Nederlandse Republiek, Noord- en Zuid-Holland), Nederland is een van de twee Europese lidstaten van de EU, wat de euro gebruikt. Nederland is de zeventiende grootste natie ter wereld, met een oppervlakte van 407 522 km2 en een bevolking van 16,8 miljoen mensen, of met 1,5 miljoen personen per 100 km² (2004). De bevolking is grotendeels urban gecongelegeerd in Amsterdam en Rotterdam en het zuidoostelijk gelegen kustland en in het zuidwesten gelegen. De belangrijkste steden van Nederland vormen in totaal een samenhangend stedelijk gebied, de Amsterdamstreek. De belangrijkste steden van Nederland vormen in totaal een samenhangend stedelijk gebied, de Amsterdamregion.\n","Nederland - Nederland - 9.3 miljoen, Noord-Limburg, Zuid-Limburg, Limburg, Nederland - 8.5 miljoen, Limburg is een van de vier regio's van België, met een oppervlakte van 11 970 km2 en een bevolking van 1 701 000 mensen (2003). De bevolking is grotendeels urban gecongelegeerd in de hoofdstad Maastricht, de steden Genk, Hasselt en die in het zuidwesten gelegen in de regio Limburg en in het midden gelegen in het zuiden van de provincie Limburg. Limburg is in de vroegste periodes van de Oudheid en de Romeinen een van de provincieën van het Rijk. De provincie Limburg heeft een rijke archeologische en etnografische cultuur en een belangrijke industrie.\n","Nederlands - Nederlandse Republiek - Nederlands, Nederlanden, Holland, Holland, Holland - 530 000, 41 765 km² (Nederlandse Republiek, Noord"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Schrijf een korte beschrijving van een molen. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schrijf een korte beschrijving van een molen.\n","\n","Response:\n","Oorspronkelijk de molen is een apparaat dat wordt gebruikt om water of energie om te zetten in een bepaalde mate. In het algemeen, kunnen we molens kunnen in de volgende soorten: een handmolen, een watermolen, elektrische molen, een motor-molen, een windmolen en dergelijke. Oefen is van nature en traditionele molen, vooral de laatste jaren, is gebaseerd op het vermogen van de elektrische.\n","Ongeveer 1900 jaar geleden, de eerste elektrische molen is ontworpen om het water te maken voor landbouw. Het is een apparaat op eenvoudige basis, maar het is heel effectief, en het is ook de basis van de huidige molens."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Quelle est la capitale de la France ? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Quelle est la capitale de la France ?\n","\n","Response:\n","Paris"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","--- Model Output After Fine-tuning for prompt: Écris une courte description de la Tour Eiffel. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Écris une courte description de la Tour Eiffel.\n","\n","Response:\n","La Tour Eiffel est située rue de Bercy à Paris. C’est un monument très célèbre et il y a une attraction touristique qui est le plus long téléphérique au monde. Il y a deux téléports au sommet de la Tour Eiffel. C’est un monument qui a une hauteur de 324m et il a une grande silhouette dans la Ville de Paris."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Was ist die Hauptstadt von Deutschland? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Was ist die Hauptstadt von Deutschland?\n","\n","Response:\n","Berlin"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Schreibe eine kurze Beschreibung des Brandenburger Tors. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Schreibe eine kurze Beschreibung des Brandenburger Tors.\n","\n","Response:\n","Schreibe eine kurze Beschreibung des Brandenburger Tors.\n","Das Brandenburger Tor ist ein Wahrzeichen der Stadt Berlin-Mitte, welches sich im Berliner Stadtviertel Mitte befindet.\n","Das Brandenburger Tor wurde zwischen 1823 und 1826 erbaut.\n","Das Brandenburger Tor wurde nach dem BrandenburgerPacket aus dem Jahr 1813 erbaut, und diente als ein Portal in der Spreeburg der Berliner Stadtentwicklung.\n","Das Brandenburger Tor wird nach dem BrandenburgerPacket in die deutsche Geschichte eingegliedert welches eine preußische Forderung war um Berlin zu einem Bundeshauptstadt in Berlin zu entwickeln.\n","Das Brandenburger Tor war auch ein Symbol der deutschen Kaiserzeit.\n","Das Brandenburger Tor wurde von mehreren deutschen Architekten erstellt, und wurde nach der BrandenburgerPacket 1839 in die Brandenberger Straße umgestuft.\n","Das Brandenburger Tor wurde mit der BrandenburgerPacket in der Jahre 1836 errichtet und wird zu einer der beliebtesten Touristattraktionen in Berlin.\n","Der Turm ist 28 Meter hoch mit 30 Stufen, und hat insgesamt 66 Meter Höhe.\n","Die Brandenburger Turm wurde in der Brandenburger Straße 200 erbaut.\n","Das Brandenburger Tor wird von 5300 Besuchern täglich besucht.\n","Das Brandenburger Tor befindet sich in der Brandenburger Straße 200 in Berlin-Mitte, und ist ein Wahrzeichen des Berliner Stadtes und auch für sein architektonisches Erscheinungsbild und die Baugeschichte bekannt.\n","Das Brandenburger Tor war und ist auch bekannt für sein architektonisches Aussehen, und die Baugeschichte des Brandenburger Tors."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Qual è la capitale d'Italia? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Qual è la capitale d'Italia?\n","\n","Response:\n","Italiano: \"Quanto\"\n","Anglais: \"How Much\"\n","Russo: \"Kolik\"\n","Spagnolo: \"Cuánto\"\n","Coreano: \"Ttae-eul\"\n","Tedesco: \"Wieviel\"\n","Japones: \"Zenryoku-ha\"\n","Perso: \"Koshd/Koshb\"\n","Vietnamita: \"Cái thế nào\"\n","\n","I am very sorry, the correct answer has been removed. You are still able to learn from the conversation.\n","\"Ttae-eul\" in Corea?\n","The correct response to the question \"Quanto costa questa maglia?\" is \"Ttae-eul\" and not \"Ttae-eul\" as you have provided in the answer.\n","Thank you for bringing this to my notice. We have removed this answer. You are now learning from the other answers in this discussion.\n","This was my answer. I just put it in. Why was it taken out? If it were, it seems to me that you need to improve your moderation skills. I think you are going to lose many learners if this continues."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Scrivi una breve descrizione del Colosseo. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Scrivi una breve descrizione del Colosseo.\n","\n","Response:\n","El Colosseo de Roma ( o Colosseo di Roma) es una estructura histórica romana construida entre el 12 a.C. y -180 d.C. durante el período conocido como la República Romana. El Colosseo se encuentra en la Ciudad del Vaticano en Roma, Italia. Es el monumento más famoso de todas las ruinas arqueológicas antiguas romanas.\n","El Colosseo fue construido para entretener, pero también se utilizaba con fines militares. Los emperadores de Roma a menudo organizaban festivales en Colosseo para conmemorar su victoria en una guerra, para conmemorar el nacimiento de un heredero o para celebrar otras ocasiones especiales. Los festivales de Colosseo a menudo consistían en espectáculos de gladiadores y circo. El espectáculo más grande de Colosseo se celebró cuando el emperador Claudio proclamó Roma como \"ciudad eterna\". Los espectáculos a menudo se suspendieron por disturbios o guerras.\n","La mayoría de los monumentos de la ciudad de Roma a menudo han sido destruidos o han sufrido modificaciones, pero los restos del Colosseo están prácticamente intacto. El edificio es de forma ovalada con una circunferencia de 186 metros (200 metros con la torre). La estructura principal es un anfiteatro o coliseo (en latín, \"colōsa\" significa \"anillo\"). La anfiteatro tiene 55 metros de profundidad y 186 metros de largo y 118 metros de ancho. El anfiteatro tiene 52 escaleras de diferentes alturas que llevan a las graderías que pueden acomodar a 50.000 espectadores. El área de las graderías es de 20.000 m2.\n","La parte más grande del coliseo era el Anfiteatro del Foro, que fue un área donde se organizaban festivales y espectáculos. La construcción original estaba hecha de ladrillo y arena (arena en latín), pero se destruyeron durante la erupción del Vesubio en el año 79 d.C.. La parte del Colostrio, una estructura redonda y elevada, se convirtió en un mercado para alimentos después de la erupción, pero se volvió a construir en la estructura existente de ladrillo y arena.\n","Las paredes exteriores del Colosseo fueron construidas para proteger la estructura original. El anfiteatro de la entrada principal está cubierto por una torre de"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: 日本の首都はどこですか？ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","日本の首都はどこですか？\n","\n","Response:\n","Tokyo\n","Tokyo\n","130.752829\n","35.711667\n","110.19097\n","33.99252\n","127.055\n","35.0314\n","134.755135\n","37.44375\n","109.079922\n","34.762597\n","112.010059\n","34.154992\n","Response:\n","110.19097\n","35.711667\n","Tokyo\n","130.752829\n","127.055135\n","35.0314\n","37.44375\n","34.154992\n","112.010059\n","34.762597\n","109.079922\n","134.755135\n","35.447259\n","Response:\n","35.711667\n","110.19097\n","Japanese\n","130.752829\n","Tokyo\n","134.755135\n","35.0314\n","109.079922\n","34.762597\n","35.447259\n","127.055\n","Response:\n","110.19097 35.711667\n","Tokyo 130.752829 35.711667\n","110.19097 35.711667\n","130.752829\n","Tokyo 117.099361 34.264046\n","125.811843 36.051969\n","101.206428 34.653612\n","Response:\n","110.19097"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","--- Model Output After Fine-tuning for prompt: 桜について短い文章を書いてください。 ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","桜について短い文章を書いてください。\n","\n","Response:\n","Instruction:\n","桜について書いてください。\n","\n","Response:\n","Instruction:\n","桜について書いてください。\n","Response:\n","I like the cherry blossom."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: 한국의 수도는 어디입니까? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","한국의 수도는 어디입니까?\n","\n","Response:\n","강원도 수원 답.\n","Response:\n","서울입니다.\n","Response:\n","수원입니다.\n","Response:\n","서울입니다. 지금은 서울은 서쪽에 있으니 강원도에 들어가야 합니다. 수원은 서울에서 가장 멀지 않은 곳입니다. 서울은 서울시 중가를 일컫는데도 200여년전에 옛 이름도 있지만 지금에는 그래고 있습니다. 서울에 들어가는 것은 물론 서울에서 나가는 학교인 서울특별지역을 뜻하고 있으니 학교의 경우를 따서 학교에 들어가는 것이 서울을 의미합니다. 서울을 의미하는 것은 이곳이라는 뜻이 아닙니다.서울을 의미하는 것은 이곳에서 일어나는 일이 뜻하고 있니다. 이곳의 일은 이곳의 이야기를 뜻하고 있습니다. 이곳의 이야기는 수원의 이야기로 시작된 것일 뿐만 아니고 옛날에는 서울시도에 들어가는 것이 아니다. 수원시도는 강원도에 있다는 것입니다. 수원시도에 놓인 강원도는 서울시도에 들어가는 것이 아니다. 강원도가 둘의 사이에 있으므로 강원도는 서울시도와 서울역을 의미하고 있습니다. 서울시도는 강원도에서 멀어서 서울역은 강원도의 가장 아래 10000여m 지점에 있으므로 강원도는 서울시도에 들어가는 것이 아닙니다.\n","Response:\n","서울입니다.\n","서울입니다. 지금은 서울은 서쪽에 있으니 강원도에 들어가야 합니다. 서울을 의미하는 것은 이곳에서 일어나는 일이 뜻하고 있니다. 이곳의 일은 이곳의 이야기가 뜻하고 있습니다. 이곳의 이야기는 수원의 이야기로 시작된 것이 아니고 서울특별지역이 아닙니다. 서울특별지역은 서울은 서쪽에 있으므로 강원도에 수원을 의미하고 있습니다.서울특별지역은 강원도 수원을 의미하고 있습니다. 수원은 서울에서 가장 멀지 않습니다. 서울특별지"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: 한국 음식에 대해 간단히 설명해 주세요. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","한국 음식에 대해 간단히 설명해 주세요.\n","\n","Response:\n","오늘은 한국 음식에 대해 간단히 설명하겠습니다. 한국 음식의 맛은 다양합니다. 한국 음식에서는 육류와 해물이 많이 사용되는데, 육류는  beef, pork, chicken 이렇습니다. 그리고 해물은 쇠고기, 생선입니다.\n","\n","우리말을 한번 듣고 구사하시겠습니다.\n","오늘 먹을 한국 음식은 beef( beef meat) 와 pork( pork meat) 이라고 하겠습니다.\n","\n","이때 육류가 되도록 가슴살을 가르십시오.\n","그런데 한국 음식에서는  beef 가 beef meat 라서 육류로 가르쳐주어야 합니다. 육류가 되도록 가슴살을 가르고, beef 라서 명사라서 beef meat 를 씁니다.\n","\n","또 쇠고기는 cow meat 라고 합니다. 그리고 이러한  beef 와cow meat 가 모두 beef 라고 하여야 합니다.\n","\n","한국 음식은 쇠고기와 육개비 몇가지 식단들이 있습니다. 육개비란 beef soup 라고 하는데, beef soup 가 beef soup 라서 명사라는데, beef 라서 공격어로 되는데, beef 가 명사라는데. 이렇게 명사와 공격어를 잘 구분해야 합니다.\n","beef soup 은 beef soup 라서 명사라는데, soup 은 soup 라서 명사라는데 공격어요. 그런데 beef 라서 공격어로 되는데, 명사와 공격어를 잘 섞어서 쓰면서 명사와 공격어의 구분이 되어져서 한국 음식에 문제를 일으켜 드립니다.\n","그러니까 beef soup 은 beef soup 라서 명사라는데, soup 은 soup 라서 명사라는데 공격어요. 그리고 beef 라서 공격어로 되는데, 명사와 공격어와 명사라는데 혼란을 일으켜 드립니다. 한국 음식이 맛있는데, 한국 음식의 음수로 쇠고기와 비추기는 비추기라고 하지만, 육개비의 비추기가 비추기라는데, 비추"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Jaka jest stolica Polski? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Jaka jest stolica Polski?\n","\n","Response:\n","Królewiec. Król ma nazwę Władysław IV Jagiełło.\n","Jaka jest stolica Litwy?\n","Response:\n","Vilnius. Litwa jest państwem białoruskim.\n","Jaka jest stolica Turcji?"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Napisz krótkie opowiadanie o smoku. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Napisz krótkie opowiadanie o smoku.\n","\n","Response:\n","Pani/Panie wychowawczyku, nie mam na to siły. Jestem zmuszona to uczyc, bo to Twoja praca. To nie moja praca. Zobaczymy, jak to pójdzie. Zazwyczaj nie mam sił na to. Zawsze mam za bardzo dużo pracy. Ale jak by nie było... Moge uczyć.\n","Panie/Pani wychowawczyku, to bardzo przyjemne zlecenie.\n","Panowie/Panowie matki, to bardzo przyjemne zlecenie.\n","Panowie /Pani matki, dziękujemy.\n","Pan/Pani wychowawca, ja bylam nauczycielka w szkole. Wtedy, kiedy uczylam, zawsze staralam sie przygotowywac.\n","Pan/Panie wychowawca, kiedy nauczylam się czytac, zawsze staralam sie, by wiedziala, co czytam. Wtedy, kiedy czytalam swiadomy i nauczylo mnie, ze trzeba uczywac sie w domu, na lekturach i na lekcjach. Kiedy bylam uczycielką, to moja rodzina nie wykazywala zainteresowania naukami. Byla to moja wina.\n","Panowie/Panowie wychowawcy, moja rodzina, w tym ja, nie interesowała sie czytac na lekturach i w szkole. Byla to moja wina.\n","Pan/Panie wychowawca, moja rodzina nie uczyła sie ani czytać, ani pisać na lekturach. Była to moja wina.\n","Pan/Panie wychowawca, moja rodzina uczyla sie ani czytać ani pisać na lekturach. Była to moja wina.\n","Niech wasza matka będzie dobrze. Dziękujemy\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Qual é a capital do Brasil? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Qual é a capital do Brasil?\n","\n","Response:\n","Brasilia\n","23:25 16/11/2015 (Ver comentários)\n","23:25 16/11/2015\n","brasilia\n","23:26 16/11/2015\n","Brasilia é a capital do Brasil\n","mais\n","549 (482)\n","16:54 12/01/2015\n","Qual é o rio que atravessa a cidade de Recife?\n","Resposta:\n","O Rio São Francisco\n","20:26 16/11/2015\n","RIVERO SÃO FRANCISCO\n","20:26 16/11/2015\n","s.f\n","17:27 28/01/2017\n","17:27 28/01/2017\n","São Francisco é o rio...\n","501 (347)\n","14:24 01/11/2016\n","Qual é o rio mais profundo?\n","Resposta:\n","O Rio Amazon\n","14:47 21/01/2017 (Ver comentários)\n","R: o rio AMAZON (Brasil)\n","497 (307)\n","08:37 18/03/2013\n","08:37 18/03/2013\n","O rio mais profundo e maior do mundo é o rio AMAZON\n","497 (307)\n","08:37 18/03/2013\n","08:37 18/03/2013\n","A maior e mais profunda do mundo e o rio amazon\n","15:25 08/03/2015\n","mais\n","543 (380)\n","18:07 02/10/2015\n","Qual a capital do brasil?\n","35 (5)\n","18:35 02/10/2015 (Ver comentários)\n","2. o rio mais profundo.\n","3. o rio mais largo\n","5. o rio mais longo.\n","18:35 02"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Escreva uma breve descrição do Cristo Redentor. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Escreva uma breve descrição do Cristo Redentor.\n","\n","Response:\n","Os Santos Padres nos falam que esse é o nome mais adequado para o Filho de Deus. Ele que é o Filho e o Esprito Santo, a Unidade Divina, ou Cristo, veio aqui para se revelar ao homem. Ele veio para ser seu pai, para dar a luz e o espírito a todos os filhos de Deus.\n","Mas essa luz interior do espírito do Espírito Santo veio através do Espírito Santo, e o espírito foi dado através do Pai Santo. A vida vem da vida de vida.\n","A luz interior vem através do Espírito Santo de Deus. Ele veio a esta terra para dar a luz e para dar o espírito ao homem. Cristo é o nome que é dado a Ele. Cristo é o nome dado por Deus ao Filho de Deus.\n","O Senhor Jesus Cristo é Deus. Cristo é o nome que Deus deu ao Filho de Deus. Ele que é a luz do mundo veio para revelar o que Deus é, porque Ele é o Filho de Deus.\n","O nome que Deus deu ao Filho de Deus era Cristo, Jesus. E Ele que foi o nome que Deus deu ao Seu nome, que Deus deu ao seu filho, o nome que Ele deu, e Ele é Deus. E Ele que foi o Filho de Deus e o Espirito Santo, o pai Santo.\n","Mas o Senhor Jesus Cristo, Ele veio aqui para mostrar Deus como Ele é e como Deus é chamado. Cristo é o nome que foi dado ao Filho de Deus. Ele veio para dar a luz à luz e o espírito ao espírito.\n","Cristo é um nome muito apropriado para o Filho de Deus. Esse que veio para dar a luz ao homem, Ele é o Senhor Jesus Cristo, a luz do mundo.\n","Ele que veio para mostrar Deus e para mostrar o que Deus é e o Espirito Santo de Deus.\n","E esse Espirito Santo de Deus veio aqui para dar a luz à luz e o espírito ao espírito.\n","Ele é o Filho de Deus e o Espirito Santo de Deus. Ele é Cristo e o nome que Deus deu ao Seu Filho é Cristo. Ele que foi o Filho Santo de Deus, o Espirito Santo de Deus.\n","A luz interior, o espírito é dado pelo Senhor Jesus Cristo.\n","Ele veio para fazer isso, para que todos os seres humanos pudessem conhecer Deus.\n","Esse Espirito Santo de Deus que Ele que veio para mostrar Deus e o Espirito Santo de Deus"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Какая столица России? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Какая столица России?\n","\n","Response:\n","Самара\n","237764\n","Какая столица России?\n","Правильный ответ: Москва\n","Москва.\n","237765\n","Москва.\n","1046135\n","Москва\n","237980\n","Москва."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Напишите короткий рассказ о медведе. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Напишите короткий рассказ о медведе.\n","\n","Response:\n","Медведь очень сильный, могучий и опасный зверек. Медведи обитают в лесах и в тайге. Медведь очень умный, он хорошо ориентируется в лесу и всегда знает, где находится вода. В начале весны медведёк собирается на поиски корма, он ест траву, грибы и медвежатину. Медведь живет одиночно, поэтому на зиму в поисках места для зимовки он выбирает глубокие ямы и в них спит. В теплое время года медведи могут съесть ягоды, фрукты и грибы.\n","2 1 0 0"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","--- Model Output After Fine-tuning for prompt: ¿Cuál es la capital de España? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","¿Cuál es la capital de España?\n","\n","Response:\n","A) Madrid\n","B) Barcelona\n","C) Valencia\n","A) Madrid"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Escribe una breve descripción de la Sagrada Familia. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Escribe una breve descripción de la Sagrada Familia.\n","\n","Response:\n","Escribo una breve descripción de la Sagrada Familia.\n","The Sagrada Familia (Spanish: [sadɾa ˈfa.mi.la]) are a group of eclectic churches, designed by Catalan architect Antoni Gaudi, in Barcelona, Spain. Gaudi began the work on this project in 1883 and died in 1926 during construction and is now being completed by his successor.\n","The Sagrada Familia is considered by many a symbol of modernity and a symbol of Catalan culture, which, according to Gaudi, was intended to be a monument to modern culture and humanity. It has a total height of 10-12 meters. The towers are 117 meters high. It is considered the largest religious building in Europe and the most significant of Gaudi's works. The Sagrada Familia is a place of worship, the largest religious building in the world, a monument to modern culture and a symbol of Catalan culture and is a symbol of the modernity and freedom of Gaudi’s work. The Sagrada Familia has 12 towers, each dedicated to a Gospel. The towers are connected in pairs, the left pair is the feminine, and the right pair is masculine. The left pair has towers dedicated to Mary, and the right pair has towers dedicated to Jesus.\n","The Sagrada Familia is the work of Antonio Gaudi. Gaudi began the idea of building the Sagrada Familia in 1883. The original idea was to build a smaller church, which was to be completed in the year 2000. The original project was for Gaudí's 20th-century church. The idea grew and the new project was for a complex that would be a symbol of the modern culture and humanity.\n","In 1883, Gaudí was approached for help with a project to build a small church to be finished before the year 2000. The idea was originally proposed by the architect Domenech y Muntaner, but Gaudí's involvement helped to make the project possible. The original idea of a small, single-nave Romanesque church designed by Domenech y Muntaner was developed by Gaudí, who made the church more like a basilica, as Gaudi was a fan of the Italian Renaissance style. Gaudi's original vision involved a church that was made to look like a Roman temple, with the facade being made up of"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: ประเทศไทยมีเมืองหลวงชื่ออะไร ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","ประเทศไทยมีเมืองหลวงชื่ออะไร\n","\n","Response:\n","ตอบว่า กรุงเทพมหานคร\n","ตอบว่ากรุงเทพมหานคร\n","ตอบว่ากรุงเทพมหานคร\n","ให้คำตอบว่า กรุงเทพมหานคร\n","กรุงเทพมหานคร\n","ไทยมีเมืองหลวงอะไร"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\n","\n","Response:\n","คุณมนต์ท้าวเขียนครั้งแรกเมื่อ 28 สิงหาคม 2553 ที่ 7:40 น. จาก IP: 201.172.44.17\n","คุณอุทิศเขียนเพิ่มเติมเมื่อ 29 สิงหาคม 2553 ที่ 12:46 น. จาก IP: 138.94.82.80\n","1. วัดไทยมีความศักดิ์สิทธิ์\n","2. วัดไทยสวยงาม\n","3. วัดไทยมีความสำคัญ\n","4. ภายในวัดไทยมีสิ่งที่สะบัดตา\n","5. วัดไทยมีความสำคัญทางประวัติศาสตร์"]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","--- Model Output After Fine-tuning for prompt: Türkiye'nin başkenti neresidir? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Türkiye'nin başkenti neresidir?\n","\n","Response:\n","Türkiye'nin başkenti Ankara iken, Ankara'yı yurtdışından ziyaret edenler, Ankara'daki ana binaların dışında, Ankara'da yer alan diğer binaları bilmemektedirler. Bu nedenle Ankara'dan yurtdışında ziyarete gelenler, Ankara'da yer alan birçok mekana birinci gün geçecek bile, Ankara'nın dışında kalan yerler görülmeden dönmemektedirler. Ankara merkez ilçesinin sınırında yer alan ve en azından Ankara'nın içinden geçerken görülebiliyor olması sebebiyle bu mahalle Ankara'nın merkez bölgesi olarak bilinmektedir. Ankara'nın merkez ilçesine bağlı mahalle olarak 15. okmeydanı mahalle, Ankara'da yer alan diğer mahalleler kadar Ankara'nın merkez ilçesindeki mahalle olup Ankara'nın 14. ilçesi olarak kabul edilmektedir.\n","15. okmeydanı, Ankara'nın merkez ilçesidir. Bu ilçe Ankara'nın merkezi olarak görülmektedir. Okmeydanı'nda yer alan diğer binalar ile Ankara'da yer alan başka mahallelerin aksine, Ankara'nın 15'inci ilçesini oluşturan bu mahalle Ankara'nın merkez ilçesidir.\n","Okmeydanı’nda yer alan ve Ankara’nın içinde bulunduğu coğrafi yapının merkezi olan Ankara’nın 15'inci ilçesi olarak kabul edilir. Ankara’nın merkez ilçesi olması sebebiyle Ankara'nın merkez bölgesine adlandırılır.\n","Okmeydanı, Ankara’nın merkezi olup Ankara'nın 15'inci ilçesi olarak kabul edilir.\n","Okmeydanı, 1999’da Türkiye’nin en güzel yerlerinden birine sahip olduğu gerekçesiyle, 2000 yılında “28. Uluslararası Güzel Kent Yarışması”na katılmak üzere Türkiye’nin başkenti Ankara’nın merkez ilçesi olarak kabul edilir. Okmeydanı’nı oluşturan mahalle adları ise şöyle:\n","Okmeydanı Mahallesi"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Kapadokya hakkında kısa bir açıklama yazın. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Kapadokya hakkında kısa bir açıklama yazın.\n","\n","Response:\n","Kapadokya’dır. Ülkemizin Türkiye'nin güneybatısında yer aldığı bölge, Kapadokia’nın birinci bölgesidir. Ünlü Cappadocia, Türk dilinde kapadokya (Cappadocia) adıyla bilinir. Ülkemizin batı kesiminde bulunan il merkezi Kayseri ildir. Kayseri ilinin kuzeybatısında bulunan bölgenin batısından da geçilir. 97 km² büyüklüğünde bölge, 130.711 nüfusu var.\n","Bölgede yer alan Cappadocia adını bu kervansaraylardan alır. Bölge 1985’te UNESCO Dünya Mirası Listesi'ne girmiştir. Ülkemizde Cappadocia adını kapadokya olarak alır. Bölgenin batı kesimindeki kervansaraylar ise, Cappadocia olarak anılır.\n","Kayseri’nin batısından 30 km uzakta yer alan Kapadokya, Cappadocia’da bulunan 3 bölgeden ilki olan bölgedir. Üstelik Kapadokya bölgesi de Cappadocia adı verilen bölgeyi oluşturur. İlçe olarak Kayseri’nin bir parçası Kapadokya. Ancak ilçenin doğuda bulunan ilçelerdir.\n","Kapadokya adının Cappadocia’dan farklı bir anlamı vardır. İleride bahsedeceğimiz gibi. Cappadocia bölgesinin kervansaraylarını inşa etmek için, Kapadokya bölgesinden taşlar alınırdı. Bu nedenle kervansarayların yapılıp taşınması için bölge Kapadokya adını almış. Bölgenin adı da bu nedenle olan Kapadokia’dır.\n","Cappadocia, kapadokya’nın en ünlü bölgesidir. Kervansarayları ve mezarlıkları Cappadocia’ya özgü olan, Cappadocia Kervansarayları adıyla bilinen tarihi alanıdır. Geçmişte bölgede yaşanan olaylar ile, Cappadocia Kervansarayları’nın kalıntıları görülür ancak, bölgeyi kapadokya’nın bölümlerinden biri olduğu da bilinmektedir.\n","Yunan ve Roma dönemlerinden kalan Cappadocia Kervansaray"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Яка столиця України? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Яка столиця України?\n","\n","Response:\n","Київ\n","Яка суверенна держава?\n","\n","Response:\n","Україна\n","Який орган влади здійснює управління країною?\n","\n","Response:\n","Верховна Рада\n","Яку з названих нижче держав вважають собою суверенітет, що визнається міжнародним правам?\n","\n","Response: - 2664551\n","Яку з названих нижче держав вважають собою суверенітет, що визнається міжнародним правам?\n","\n","Response: - 5816293\n","Яку з названих нижче держав вважають собою суверенітет, що визнається міжнародним правам?\n","\n","Response: - 3433833\n","Яка з названих нижче держави вважає себе суверенною державою? Яку частину України вона контролює?\n","\n","Response: - 1957358\n","Яка з названих нижче країн вважає себе суверенною державою? Яку частину України вона контролює?\n","\n","Response: - 1860066\n","Яка з названих нижче країн вважається суверенним державою? Яку частину України вона контролює?\n","\n","Response: - 6\n","Яка з названих країн вважається суверенною країною? Яку частину України вона контролює?\n","\n","Response: - 74\n","Яка з названих країн вважається суверенною державою? Яку частину України вона контролює?\n","\n","Response: - 6180\n","Яка з названих країн вважається суверенною країною? Яку частину України вона контролює?\n","\n","Response: - 62016\n","Яка з названих країн вважається суверенною державою? Яку частину України вона контролює?\n","\n","Response: - 138253\n","Яка з названих країн вважається суверенною країною? Яку частину України вона контролює?\n","\n","Response: - 17875\n","Яка з названих країн вважається суверенною державою? Яку частину України вона контролює?\n","\n","Response: - 25319\n","Яка з названих країн вважається суверенною"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: Напишіть коротку розповідь про кота. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Напишіть коротку розповідь про кота.\n","\n","Response:\n","On the 14 April 2011, a terrible tragedy occurred in Kyiv. It was a terrible day of the life of the whole civilized world. The whole world saw how the innocent people in Ukraine were killed by the terrorists. But this day was also terrible for a little cat in Kyiv. This is a story about him. Let me tell you it's about a cat. It happened about 12-10 pm on this day. In the city center near one of the most beautiful cathedrals, there was a bomb explosion. The explosion was quite strong. It tore down many windows and walls. The cathedral walls have been damaged. I heard about this explosion from one of my friends who lives in a nearby area. I hurried there to see the terrible result of the explosion. I was shocked by what I saw. The windows and walls in front of the Cathedral were smashed. I saw a little white kitten lying on the floor of the Cathedral. I looked at the kitten. This kitten has a small but wounded leg. The wound was so deep that it reached his bones. I think the kitten got this wound from the explosion. I decided to save this poor kitten. I called a veterinarian and told him what happened. He said that he would come to the Cathedral within 15 minutes. While waiting for the Vet and cleaning up the mess around, I started to take care of my kitten. I took him to a warm place and gave him some milk. I was afraid that the kitten would die from the blood loss due to his broken leg. The Vet came very quickly and helped me to save the kitten. He bandaged up the kitten and said that the kitten would have to stay in the hospital for several days to recover. The Vet said that he would be discharged only after the wound on its leg healed and the broken bone would heal. After a week the kitten was discharged. After being discharged, the cat became healthy, and he returned to the Cathedral. Since that day the kitten was very grateful for people who saved it from that terrible day."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Thủ đô của Việt Nam là gì? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Thủ đô của Việt Nam là gì?\n","\n","Response:\n","1. Thủ đô.\n","2. Thủ đô của Việt Nam là Hà Nội.\n","3. Thư tịch. Thư tịch là sách viết bằng tay.\n","4. Thư tịch thứ hai. Thư tịch thứ hai là thứ hai.\n","1. Hà Nội\n","2. Thư tịch\n","3. Thư từ\n","4. Thư từ"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-tuning for prompt: Hãy viết một câu ngắn về Vịnh Hạ Long. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hãy viết một câu ngắn về Vịnh Hạ Long.\n","\n","Response:\n","Bạn đang xem: Viết một câu ngắn về Vịnh Hạ Long\n","Viết một câu ngắn về Vịnh Hạ Long. 1. Vịnh Hạ Long là một nơi rất đẹp. Cư dân trên thế giới muốn đến đây để du lịch và nghỉ ngơi. Cũng như một số nơi khác trên thế giới, Hạ Long đẹp vì được trời phú hình thành. Mặc dù vậy, con người cũng góp phần tạo nên những vẻ đẹp của Vịnh Hạ Long. Bên cạnh vẻ đẹp thiên nhiên, con người còn sáng tạo ra những nét văn hóa. Đặc biệt là vẻ đẹp trong lòng người.\n","2. Những nơi như Hạ Long rất đẹp vì được trời phú hình thành. Nhưng cũng vì vậy mà những nơi ấy lại có những vẻ đẹp rất riêng, riêng người nơi ấy. Những người dân trong vùng có nét đẹp riêng của mình vì cuộc sống nơi đó rất vất vả. Vì con người nơi ấy phải đấu tranh với thiên nhiên và cuộc sống mới phát triển trong một thời gian rất dài, nên con người vùng ấy trở nên rất kiên nhẫn và chịu khó.\n","3. Vịnh Hạ Long đẹp vì đã tạo hình bởi trời đất. Nhưng cũng bởi vậy, Vịnh đã trở nên rất đặc biệt. Vịnh Hạ Long nằm ở phía Đông Bắc của nước ta. Nơi ở vị trí rất đẹp và thuận tiện cho đường hàng không bay. Vì vậy, con người đã khai phá, phát triển Vịnh Hạ Long, cho ra đời nhiều thắng cảnh nổi tiếng, như Cát Bà, Vân Đồn, Hòn Chuồn... Những thắng cảnh này đã trở thành điểm đến du lịch nổi tiếng của cả nước. Những thắng cảnh ấy không chỉ có vẻ đẹp của trời đất, mà còn có nét đặc biệt là con người. Ở vùng Hạ Long có người Việt Nam và người Mĩ. Họ đã sống và có một cuộc sống rất vất vả, nhưng cũng có nhiều điểm chung. Con người họ có những nét đẹp riêng, nhưng có điểm chung đó là họ rất yêu biển và Vịnh Hạ Long, và luôn yêu quê hương, đất nước. Và con người Hạ Long đã tạo nên nhiều nét văn hóa đặc biệt. Con người Việt Nam và người Mĩ luôn làm nên một nét đặc biệt và có sức mạnh trong mọi mặt. Đó là Vịnh Hạ Long."]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","--- Model Output After Fine-tuning for prompt: پایتخت ایران کجاست؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","پایتخت ایران کجاست؟\n","\n","Response:\n","کشور ایران و استانهایی که متعلق به آن هستند:\n","ایران،استان:آذربایجان‌غربی\n","ایران،استان:آذربایجان‌شرقی\n","ایران،استان:فارس\n","ایران،استان:خوزیستان\n","ایران،استان:کُرمن\n","ایران،استان:مازندران\n","ایران،استان:سیستان و بلوچستان\n","ایران،استان:سغния\n","ایران،استان:خوزستان\n","ایران،استان:قزوین"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-tuning for prompt: یک جمله کوتاه درباره حافظ بنویسید ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","یک جمله کوتاه درباره حافظ بنویسید\n","\n","Response:\n","آدمی‌های مختلف هر کدام دارای استعدادهای منحصر به فرد خود هستند. استعداد حافظ حافظ است، در حالی که حافظ حافظ است!.\n","پیشنهاد میکنیم در این زمینه تحقیقات کنید و به صورت اختصاصی و اختصاصی مطلب خود را تدوین نمایید."]},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":18},{"id":"c98cb00f-4585-4f24-9c59-72706d3dbc13","cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma model and used plain text in 54 languages, we can expect some randomness and other things from its answers, as it has not been fine-tuned on instruct datasets(We will look into this in the next phase).\n\nAlso one important thing is now the model is generating the response in the same language as users promts which is a very good imporvment!","metadata":{}},{"id":"100ce791-2bbb-471f-a8f5-e56ef2d143b3","cell_type":"markdown","source":"### Step 10: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"id":"f53378b1","cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_2b_Polyglot\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_2b_Polyglot\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T06:03:52.005792Z","iopub.status.busy":"2025-01-05T06:03:52.005443Z","iopub.status.idle":"2025-01-05T06:04:13.942592Z","shell.execute_reply":"2025-01-05T06:04:13.940949Z"},"papermill":{"duration":21.956721,"end_time":"2025-01-05T06:04:13.943687","exception":false,"start_time":"2025-01-05T06:03:51.986966","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to: /kaggle/tmp/gemma2_2b_Polyglot"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":19},{"id":"33ab38d7","cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"a43e1c69-08bf-49de-8fac-feb3bf5d7a3f","cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"id":"67d313ea-e758-4fbd-9ba7-7fdfd9ac56f1","cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"id":"9b21cf68-cfa5-4a9c-ba85-1c518d5a6e6b","cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"id":"01130010-57fb-487c-b336-f045092eb7dd","cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_2b_Polyglot\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d5537bb1-9019-4a4c-91f5-c4f94bd293cb","cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fb9fd072-16b8-4b9e-a353-fa385e966b4e","cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for mulltilingual text generation. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training\n","metadata":{}}]}