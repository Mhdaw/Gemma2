{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":205088,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":56633,"modelId":78150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":14824.909689,"end_time":"2024-12-28T14:00:19.824082","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-28T09:53:14.914393","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c25c80ad-d35e-4229-a3f5-9e1fd732bc19","cell_type":"markdown","source":"# Fine-Tuning Gemma for Swedish Language\nThis notebook demonstrates the fine-tuning of the Gemma model on Swedish datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the Swedish dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"id":"96bbc93c-09a1-41b3-853b-f7d5ed3fe934","cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_sv)","metadata":{}},{"id":"43b54365-29cd-45b1-a58c-c69a287e840f","cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used gemma2_9b_en","metadata":{}},{"id":"bbdf1ccc-fb29-4061-897d-9ef907a11bb2","cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"id":"5e13f400","cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{},"outputs":[],"execution_count":null},{"id":"a259e231","cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:55:14.968076Z","iopub.status.busy":"2024-12-28T09:55:14.967806Z","iopub.status.idle":"2024-12-28T09:55:24.414405Z","shell.execute_reply":"2024-12-28T09:55:24.412949Z"},"papermill":{"duration":9.452629,"end_time":"2024-12-28T09:55:24.415676","exception":false,"start_time":"2024-12-28T09:55:14.963047","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1735379720.729710      74 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: === \n","learning/45eac/tfrc/runtime/common_lib.cc:479\n","E1228 09:55:20.773216752      74 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-12-28T09:55:20.773200663+00:00\", grpc_status:2}\n"]},{"data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"149e8545-641a-4ccd-a2b1-cf50e2c903d4","cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"id":"b6f54bd9","cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient\n# from google.colab import userdata\n#import getpass\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:55:24.425441Z","iopub.status.busy":"2024-12-28T09:55:24.425116Z","iopub.status.idle":"2024-12-28T09:55:24.429520Z","shell.execute_reply":"2024-12-28T09:55:24.428565Z"},"papermill":{"duration":0.011123,"end_time":"2024-12-28T09:55:24.431038","exception":false,"start_time":"2024-12-28T09:55:24.419915","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"cfa240e9","cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:55:24.440035Z","iopub.status.busy":"2024-12-28T09:55:24.439733Z","iopub.status.idle":"2024-12-28T09:55:40.561400Z","shell.execute_reply":"2024-12-28T09:55:40.560481Z"},"papermill":{"duration":16.128758,"end_time":"2024-12-28T09:55:40.563365","exception":false,"start_time":"2024-12-28T09:55:24.434607","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"execution_count":4},{"id":"8bf48f0e-068f-41a7-a3ac-a988ab9582e5","cell_type":"markdown","source":"## Step 2: Load and Explore Swedish Dataset\nWe are using the `allenai/c4` dataset with Swedish (`sv`) data. The dataset is loaded in streaming mode for efficient handling of large datasets.\n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"id":"6d539f84-a633-4ac1-892e-69632745f849","cell_type":"markdown","source":"Since we want to fine-tune the Gemma 2 9b model for adapting to the Swedish language, we need a good amount of high-quality Swedish text corpus. For that, we use the 'C4' dataset, which is a multilingual text dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/allenai/c4)  \n\n**Dataset Summary (from the original dataset page):**  \nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on the Common Crawl dataset: [https://commoncrawl.org](https://commoncrawl.org).\n\nThis is the processed version of Google's C4 dataset.","metadata":{}},{"id":"5890e5b6","cell_type":"code","source":"data = load_dataset(\"allenai/c4\", \"sv\", streaming=True)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:55:40.572445Z","iopub.status.busy":"2024-12-28T09:55:40.572216Z","iopub.status.idle":"2024-12-28T09:56:28.450963Z","shell.execute_reply":"2024-12-28T09:56:28.449875Z"},"papermill":{"duration":47.885976,"end_time":"2024-12-28T09:56:28.453138","exception":false,"start_time":"2024-12-28T09:55:40.567162","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"371feacc","cell_type":"code","source":"sample_data = []\nfor i, example in enumerate(iter(data[\"train\"])):\n    if i >= 2:  # Change this number to get more examples\n        break\n    sample_data.append(example[\"text\"])\n\nprint(\"Sample Swedish Data:\")\nfor i, text in enumerate(sample_data):\n    print(f\"Example {i + 1}:\", text[:500])  # Print the first 500 characters to get a preview","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:56:28.463604Z","iopub.status.busy":"2024-12-28T09:56:28.463336Z","iopub.status.idle":"2024-12-28T09:56:35.801975Z","shell.execute_reply":"2024-12-28T09:56:35.801202Z"},"papermill":{"duration":7.345736,"end_time":"2024-12-28T09:56:35.803935","exception":false,"start_time":"2024-12-28T09:56:28.458199","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample Swedish Data:\n","Example 1: Zara's Custom Tailor (Pattaya, Thailand) - omdömen\n","Restauranger i närheten av Zara's Custom Tailor\n","Saker att göra i närheten av Zara's Custom Tailor\n","Zara's Custom Tailor, Pattaya - omdömen\n","Nr 16 av 53 Shopping i Pattaya\n","Öppet nu: 10:30 - 23:00\n","216/4 Moo 9, Second Road | Between Soi 8 & 7, Pattaya 20150, Thailand\n","Öppet i dag: 10:30 - 23:00\n","Öppettider i dag: 10:30 - 23:00\n","+66 81 940 4602\n","från 47,30 US$\n","suad c\n","Passar till bröllop\n","Bra ställe att köpa en kostym. . . gick för att köpa en bröllopsdräkt\n","Example 2: Hej Aidan!\n","Du placerade dig som nummer ett i PP1U, men blev dock inte svensk mästare då du rider för Great Britain.\n","-Det kändes ganska bra. Jag hade absolut inte förväntat mig det. Men han var på gång innan loppet och under loppet var väldigt säker och stadig. Så jag hade en bra känsla.\n","Jag vet att din häst kallas för ”Jimmy”. Kan du berätta mer om honom?\n","-Det är familjens uppfödning och min pappa och jag har hjälpts åt att rida in och träna honom. Det sista året har jag haft honom som min egen.\n"]}],"execution_count":6},{"id":"2fe4a0a6","cell_type":"code","source":"# Define the maximum number of examples for training and validation\nmax_train_examples = 5000\nmax_val_examples = 100\n\n# Create a plain-text list from a subset of the dataset\n# Load data subsets\ntrain_text_data = [example[\"text\"] for example in itertools.islice(data[\"train\"], max_train_examples)]\nval_text_data = [example[\"text\"] for example in itertools.islice(data[\"validation\"], max_val_examples)]\n\n# Check the first example to ensure loading is correct\n#print(\"First training example:\", train_text_data[0])\n#print(\"First validation example:\", val_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:56:35.813256Z","iopub.status.busy":"2024-12-28T09:56:35.813018Z","iopub.status.idle":"2024-12-28T09:56:36.867199Z","shell.execute_reply":"2024-12-28T09:56:36.865630Z"},"papermill":{"duration":1.060917,"end_time":"2024-12-28T09:56:36.869001","exception":false,"start_time":"2024-12-28T09:56:35.808084","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","training length:5000\n"]}],"execution_count":7},{"id":"028237a1-aea9-4ea9-a48d-1c957a5dcb76","cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"id":"ebba8973","cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(val_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:56:36.879833Z","iopub.status.busy":"2024-12-28T09:56:36.879529Z","iopub.status.idle":"2024-12-28T09:56:37.340975Z","shell.execute_reply":"2024-12-28T09:56:37.339898Z"},"papermill":{"duration":0.469551,"end_time":"2024-12-28T09:56:37.343168","exception":false,"start_time":"2024-12-28T09:56:36.873617","status":"completed"},"tags":[]},"outputs":[],"execution_count":8},{"id":"2b9a76dd-2952-4379-add7-ff758f416dcd","cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n","metadata":{}},{"id":"947e6655","cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_en/3\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:56:37.353153Z","iopub.status.busy":"2024-12-28T09:56:37.352877Z","iopub.status.idle":"2024-12-28T09:59:33.560911Z","shell.execute_reply":"2024-12-28T09:59:33.559739Z"},"papermill":{"duration":176.215338,"end_time":"2024-12-28T09:59:33.562815","exception":false,"start_time":"2024-12-28T09:56:37.347477","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":9},{"id":"582c664f","cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<48}  {str(variable.shape):<14}  {str(variable.value.sharding.spec)}')","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:59:33.574073Z","iopub.status.busy":"2024-12-28T09:59:33.573769Z","iopub.status.idle":"2024-12-28T09:59:33.578686Z","shell.execute_reply":"2024-12-28T09:59:33.577669Z"},"papermill":{"duration":0.012615,"end_time":"2024-12-28T09:59:33.580298","exception":false,"start_time":"2024-12-28T09:59:33.567683","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'keras_hub.src.models.gemma.gemma_decoder_block.GemmaDecoderBlock'>\n","decoder_block_1/pre_attention_norm/scale          (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_attention_norm/scale         (3584,)         PartitionSpec(None,)\n","decoder_block_1/attention/query/kernel            (16, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/key/kernel              (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/value/kernel            (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/attention_output/kernel  (16, 256, 3584)  PartitionSpec('model', None, None)\n","decoder_block_1/pre_ffw_norm/scale                (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_ffw_norm/scale               (3584,)         PartitionSpec(None,)\n","decoder_block_1/ffw_gating/kernel                 (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_gating_2/kernel               (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_linear/kernel                 (14336, 3584)   PartitionSpec('model', None)\n"]}],"execution_count":10},{"id":"29f9f392-cfd6-4093-90b3-c4ced74ac166","cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"id":"d7400eb4","cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:59:33.591167Z","iopub.status.busy":"2024-12-28T09:59:33.590947Z","iopub.status.idle":"2024-12-28T09:59:33.595546Z","shell.execute_reply":"2024-12-28T09:59:33.594817Z"},"papermill":{"duration":0.012053,"end_time":"2024-12-28T09:59:33.597258","exception":false,"start_time":"2024-12-28T09:59:33.585205","status":"completed"},"tags":[]},"outputs":[],"execution_count":11},{"id":"0fc952b7-c03b-4326-9796-7ab94caa5fa2","cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"id":"9204ef4c","cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\", # Greeting and request for recent information\n    \"Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap?\", # Request for historical knowledge and cultural impact\n    \"Skriv en kort dikt på svenska om ett höstlandskap.\", # Request for poetic creativity\n    \"Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige.\", # Request for technical explanation and geographical context\n    \"Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T09:59:33.608063Z","iopub.status.busy":"2024-12-28T09:59:33.607849Z","iopub.status.idle":"2024-12-28T10:03:51.369349Z","shell.execute_reply":"2024-12-28T10:03:51.367921Z"},"papermill":{"duration":257.773374,"end_time":"2024-12-28T10:03:51.375236","exception":false,"start_time":"2024-12-28T09:59:33.601862","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output Before Fine-Tuning for prompt: Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\n","\n","Response:\n","I’m great thanks, how about you? I’ve recently discovered how to make a great pasta sauce. It’s very tasty and easy!\n","(I’m great thanks, how about you? Recently, I’ve learned how to make a great pasta sauce. It’s very tasty and it’s easy!)\n","\n","Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\n","\n","Hej! How are you doing today? Tell me something interesting you’ve recently learned.\n","\n","Hej, hur mår du idag? Berätta något intressant du lärt dig nyligen.\n","\n","Hej, how are you doing today? Tell me something interesting you’ve learned lately.\n","\n","Hej! Hur mår du idag? Berätta nåt intressant du har lärt dig nyligen.\n","\n","Hej, how are you today? Tell me something interesting you’ve learnt recently.\n","\n","Hej. Hur mår du idag? Berätta nåt intressant du lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you’ve learned recently.\n","\n","Hej. Hur mår du idag? Berätta nåt intressant du har lärt dig nyligen.\n","\n","Hej, how are you doing today? Tell me something interesting you’ve recently learned.\n","\n","Hej. Hur mår du idag? Berätta någonting intressant du har lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you’ve learnt lately.\n","\n","Hej. Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you’ve learnt lately.\n","\n","Hej. Hur mår du idag? Berätta någonting intressant du har lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you have learnt recently.\n","\n","Hej. Hur mår du idag? Berätta nåt intressant du has lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you’ve learned lately.\n","\n","Hej. Hur mår du idag? Beratta nåt intressant du has lärt dig nyligen.\n","\n","Hej. How are you today? Tell me something interesting you’ve learned lately.\n","\n","Hej. Hur\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap?\n","\n","Response:\n","Under den italienska renässansen utvecklades en mängd konst, musik och arkitektur. Renässansens idéer om humanism och vetande ledde till en stor uppblåsning av kunskap inom många områden, inklusive matematik, vetenskap, astronomi, arkitektur, litteratur och musik. Den italienska renässansen var en tid som innebar en återupplivning av humanistiskt tänkande och ett stort steg framåt i vetenskap och konst. Renässansen började i Italien under 1400-talet och fortsatte fram till 1600-talet. Den var en tid då människor började intressera sig för vetenskap, konst, musik och litteratur på ett helt nytt sätt. Människor började studera vetenskap och konst på ett sätt som aldrig tidigare gjorts. Renässanstiden innebar en förändring av människans sätt att se på världen och dess relation till naturen. Den italienska renässansen ledde till en uppblåsning av vetenskap och konst, som fortfarande kan ses i dag. Den ledde till en stor utveckling inom matematik, vetenskap och konst som fortfarande är synligt i dagens samhälle.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Skriv en kort dikt på svenska om ett höstlandskap. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Skriv en kort dikt på svenska om ett höstlandskap.\n","\n","Response:\n","Hösten, när blad faller,\n","När träden blir kala\n","När luften blir svalare,\n","Och vinden blir kallare.\n","\n","Skuggigt och mörkt blir marken,\n","När dagarna blir kortare\n","Och färgerna blir mörkare\n","Och färgerna blir mer gråa.\n","\n","Höst är en tid för vila,\n","När träd och buskar vila.\n","Förbered dig på en ny vår\n","Med nya blad och nya blommor.\n","\n","Hösten är en tid för reflektion,\n","När vi reflekterar över livet\n","Och vi uppskattar naturen,\n","Och vi uppskattar naturen.\n","\n","Hösten är en tid för förberedelser\n","För vintern och dess kyla,\n","För att vi kan njuta av\n","För att vi kan njuta av den vackra hösten\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige.\n","\n","Response:\n","I enlighet med din fråga kan man säga att artificiell intelligens fungerar på det sättet att den använder sig av algoritmer och maskinlärning för att simulera och förstärka mänskligt tänkande och beteende. Vanligaste användningsområdena inkluderar automatiserad datainsamling och analys, maskininlärning för att förbättra beslutsprocesser och för att automatisera vissa typer av arbetsuppgifter.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket?\n","\n","Response:\n","'Att gapa efter mycket' means to be very hungry.\n","For example if you have not eaten anything for a long time, then you are hungry, so you would say 'ja har gapat efter mycket', or 'jag gapar efter mycket', meaning I am very hungry.\n","\n","Instruction:\n","Om du sa 'jag gapar efter mycket', vad skulle någon annan kunna svara?\n","\n","Response:\n","If somebody else heard you and you were very hungry, they might say 'ta det lugnt' which means take it easy.\n","\n","Om du sa 'jag gapar efter mycket', vad skulle någon annan kunna svara?\n","\n","Response:\n","If someone heard you were very hungry they might ask if you wanted some coffee or a snack.\n","\n","\n"]}],"execution_count":12},{"id":"ab7508c0-687f-4836-b109-71e034ef8c8f","cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"id":"6c029c04","cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-12-28T10:03:51.387939Z","iopub.status.busy":"2024-12-28T10:03:51.387594Z","iopub.status.idle":"2024-12-28T10:03:52.300622Z","shell.execute_reply":"2024-12-28T10:03:52.299595Z"},"papermill":{"duration":0.921809,"end_time":"2024-12-28T10:03:52.302678","exception":false,"start_time":"2024-12-28T10:03:51.380869","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,270,779,392\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> (34.54 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,270,779,392\u001b[0m (34.54 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,073,408</span> (110.91 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,073,408\u001b[0m (110.91 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":13},{"id":"9ff3cc0a","cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_9b_sv\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-28T10:03:52.317049Z","iopub.status.busy":"2024-12-28T10:03:52.316801Z","iopub.status.idle":"2024-12-28T10:03:54.052798Z","shell.execute_reply":"2024-12-28T10:03:54.051848Z"},"papermill":{"duration":1.745302,"end_time":"2024-12-28T10:03:54.054494","exception":false,"start_time":"2024-12-28T10:03:52.309192","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-the-way-2005\u001b[0m (\u001b[33mthis-is-the-way-2005-independent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241228_100353-mq6zd5ur\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomic-armadillo-1\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_sv\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_sv/runs/mq6zd5ur\u001b[0m\n"]},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_9b_sv/runs/mq6zd5ur?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e930054cee0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":14},{"id":"b52fe76b-57ad-4f3a-ac5c-dd1fa16eb9c6","cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"id":"1eb25453","cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])#","metadata":{},"outputs":[],"execution_count":null},{"id":"d4e6cef0-4a9b-4196-bd68-0a8f54c67b34","cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"id":"6a4cdce5","cell_type":"code","source":"test_prompts = [\n    \"Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\", # Greeting and request for recent information\n    \"Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap?\", # Request for historical knowledge and cultural impact\n    \"Skriv en kort dikt på svenska om ett höstlandskap.\", # Request for poetic creativity\n    \"Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige.\", # Request for technical explanation and geographical context\n    \"Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T13:45:40.612341Z","iopub.status.busy":"2024-12-28T13:45:40.611995Z","iopub.status.idle":"2024-12-28T13:50:15.307132Z","shell.execute_reply":"2024-12-28T13:50:15.305573Z"},"papermill":{"duration":277.876756,"end_time":"2024-12-28T13:50:16.877100","exception":false,"start_time":"2024-12-28T13:45:39.000344","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Hej! Hur mår du idag? Berätta något intressant du har lärt dig nyligen.\n","\n","Response:\n","Hey! How are you doing today? Tell me something interesting you have learned lately."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap? ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Vad vet du om renässansens historia i Italien? Kan du förklara dess inverkan på konst och vetenskap?\n","\n","Response:\n","Hur kan historien om en epok förstås utan att ta hänsyn till den tid dess historia äger rum? I detta avseende, renässansen är ett resultat av en lång och utmanande process av historiska händelser som de medeltida Italien gick.\n","Medeltida Italien var en mycket rik plats. Han hade många städer-stater som var rika med handel och industri, och var mycket förmögen. Dessa städers stat hade också mycket välutbildade borgare som kunde läsa och skriva, och det var därför de var mycket intresserade av grekisk och romersk litteratur, och de uppmuntrade till översättning och kopiering av grekiska och romerska skrifter.\n","På detta sätt, i medeltida Italien, särskilt i Florens, upptäcktes den grekisk-romerska litteraturen och det ledde till en förändring i människors tänkande.\n","Det finns något annat att beakta i historien om renässansen, och det är upptäckten av det nya och det okända, det vill säga upptäckten av Amerika.\n","Denna händelse, som ägde rum i början av renässansen, hade ett enormt inflytande på utvecklingen av den humanistiska perioden.\n","Denna upptäckt gjorde det möjligt för många européer att komma i kontakt med den amerikanska kontinenten. Nya land, nya resurser och nya möjligheter att tjäna och tjänstgör i nya städer och städer. Men det fanns också människor som förblev i Europa och inte gick till Amerika. Dessa människor hade möjligheten att tjäna på import och export av material från Amerika. På detta sätt, Europa blev mycket rikare.\n","Den andra gruppen bestod av dem som förblev i Europa, och de hade en stor inverkan på utvecklingen av renässansen. Dessa människor gick till Amerika och fick mycket rika material som guld och silver.\n","Med all denna rikdom, som staden-staterna hade, kunde adeln inte längre styra städerna och statliga stater, eftersom borgarna hade mycket pengar och kunde köpa mycket inflytelserik politik.\n","Det var därför som i renässansen, med införandet av den borgliga samhällsordningen,"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Skriv en kort dikt på svenska om ett höstlandskap. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Skriv en kort dikt på svenska om ett höstlandskap.\n","\n","Response:\n","Jag ser solen\n","Det är höst. Jag ser solen gå ner på himmelen. Jag ser röda, gula och gröna träd. Jag ser ett skugg. Jag ser ett långa träd.\n","Höst är min favorit säsong. Det är mycket vackert.\n","It's autumn. I can see the sun go down on the sky. I can see red, yellow and green trees. I see a shadow. I see long tree.\n","Autumn is my favorite season. It is very beautiful."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Förklara, med enkla ord, hur artificiell intelligens fungerar och vilka dess vanligaste användningsområden är i Sverige.\n","\n","Response:\n","Artificiell intelligens (AI) är en disciplin inom datavetenskap som syftar till att skapa maskiner som kan utföra uppgifter som normalt sett kräver mänsklig intelligens, såsom problem-lösning, planering, uppfattning, mål-inriktning, strategiformulering, och självkorrigering. Dessa maskiner kallas för AI-system.\n","I Sverige används AI främst inom sektorerna sjukvård, försäkring, bank och finans, men det finns även många projekt inom andra branscher. AI-system används till exempel för att hjälpa läkare att diagnostisera sjukdomar snabbare och mer exakt, för att hjälpa försäkringsbolag att bedöma risker snabbare och mer rättvist, samt för att hjälpa banker att upptäcka bedrägerier. AI-system används också för att hjälpa företag inom olika branscher att optimera sina processer och förbättra effektiviteten.\n","Artificiell intelligens fungerar genom att maskiner tränas på stora mängder data, vilket gör att de kan lära sig mönster och tag team och uppgifterna som de tränats på. Detta kallas för maskininlärning och är en av de mest använda teknologierna inom AI. Maskinerna tränas ofta med så kallade neurala nätverk, som är inspirerade av"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","Om någon sa: 'Att gapa efter mycket', vad skulle det betyda? I vilken situation skulle man kunna använda det uttrycket?\n","\n","Response:\n","Hur kan man tolka den här frasen? Det beror på vilken dialekt man talar och vad man menar med \"mycket\". Om jag generaliserar kan jag säga så här:"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"execution_count":17},{"id":"3c9204f0-5b79-49d8-8c71-8609462e71cc","cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma model and used plain text in target language, we can expect some randomness and other things from its answers, as it has not been fine-tuned on instruct datasets(We will look into this in the next phase).","metadata":{}},{"id":"29ea0644-7ec2-44d8-9c53-1f957b39f4b3","cell_type":"markdown","source":"### Step 10: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"id":"f1b4f8d0","cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_9b_sv\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_9b_sv\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2024-12-28T13:50:20.019324Z","iopub.status.busy":"2024-12-28T13:50:20.019000Z","iopub.status.idle":"2024-12-28T13:51:34.216271Z","shell.execute_reply":"2024-12-28T13:51:34.215070Z"},"papermill":{"duration":77.280353,"end_time":"2024-12-28T13:51:35.753973","exception":false,"start_time":"2024-12-28T13:50:18.473620","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to: /kaggle/tmp/gemma2_9b_sv"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":18},{"id":"2e6c9196","cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"353bd0bd-aadb-4a35-996a-e2bb922d6115","cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"id":"5c421589-afdb-4ed1-a84f-8cf23d386e64","cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"id":"8ba2a560-0bb1-4829-8703-602e8ca71cbc","cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"id":"5b9fadb2-6b82-4785-8367-d12c26f000aa","cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_9b_sv\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a37fc490-a1d9-4460-addf-63fa1a27d316","cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"51fde7d6-d7c3-41c6-9697-c8fb69f2b363","cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Swedish text generation. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training\n","metadata":{}}]}