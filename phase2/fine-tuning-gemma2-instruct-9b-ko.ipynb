{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"isSourceIdPinned":true,"modelId":163613,"modelInstanceId":181237,"sourceId":212623,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":2605.104711,"end_time":"2025-01-03T18:26:05.576137","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-03T17:42:40.471426","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"47282bd8-6a48-4065-9e7b-a99f3da9b03f","cell_type":"markdown","source":"# Instruct Fine-Tuning Gemma for Korean Language\nThis notebook demonstrates the fine-tuning of the Gemma model on Korean datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the Korean Instruct dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"id":"3386a5b2-9ccb-4b5a-9549-c0b3754eb79d","cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_ko)","metadata":{}},{"id":"d793eab7-fb5c-48ec-aed6-f0c5332fdab2","cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used the fine-tuned version of the gemma2_9b_en which is fine-tuned on Korean data, The model [link](https://www.kaggle.com/models/mahdiseddigh/gemma2/keras/gemma2_9b_ko)","metadata":{}},{"id":"7bf1ef7e-52e0-40ec-8dbc-96da8d7cb86a","cell_type":"markdown","source":"### My Gemma2 cookbook:\nI made this repo and I'm uploading all notebooks related to working with gemma models, check it out:\nhttps://github.com/Mhdaw/Gemma2","metadata":{}},{"id":"53c86693-f7bc-4495-b271-1d7b62fca19d","cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"id":"5b7c9c66","cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-03T17:42:43.482777Z","iopub.status.busy":"2025-01-03T17:42:43.482485Z","iopub.status.idle":"2025-01-03T17:44:34.174029Z","shell.execute_reply":"2025-01-03T17:44:34.172305Z"},"papermill":{"duration":110.699206,"end_time":"2025-01-03T17:44:34.176239","exception":false,"start_time":"2025-01-03T17:42:43.477033","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]},{"name":"stdout","output_type":"stream","text":["\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]}],"execution_count":1},{"id":"de3d9edd","cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:44:34.185830Z","iopub.status.busy":"2025-01-03T17:44:34.185502Z","iopub.status.idle":"2025-01-03T17:44:43.357649Z","shell.execute_reply":"2025-01-03T17:44:43.356084Z"},"papermill":{"duration":9.17903,"end_time":"2025-01-03T17:44:43.359438","exception":false,"start_time":"2025-01-03T17:44:34.180408","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1735926279.347654      74 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: === \n","learning/45eac/tfrc/runtime/common_lib.cc:479\n","E0103 17:44:39.392261939      74 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2025-01-03T17:44:39.392242529+00:00\", grpc_status:2}\n"]},{"data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"432f7c93-89d9-4999-9af8-6f56e5f02bf9","cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"id":"1cce080f","cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:44:43.371233Z","iopub.status.busy":"2025-01-03T17:44:43.370919Z","iopub.status.idle":"2025-01-03T17:44:43.375083Z","shell.execute_reply":"2025-01-03T17:44:43.373994Z"},"papermill":{"duration":0.012616,"end_time":"2025-01-03T17:44:43.376628","exception":false,"start_time":"2025-01-03T17:44:43.364012","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"78502393","cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:44:43.385454Z","iopub.status.busy":"2025-01-03T17:44:43.385233Z","iopub.status.idle":"2025-01-03T17:44:59.038135Z","shell.execute_reply":"2025-01-03T17:44:59.036143Z"},"papermill":{"duration":15.659103,"end_time":"2025-01-03T17:44:59.039566","exception":false,"start_time":"2025-01-03T17:44:43.380463","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"execution_count":4},{"id":"f9fc104e-67c9-4f11-8029-12f16040f7df","cell_type":"markdown","source":"## Step 2: Load and Explore Korean Dataset\nWe are using the `CohereForAI/aya_dataset` dataset with Korean insturct data. \n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"id":"36553064-d6e1-4a33-b186-b15d8617de6e","cell_type":"markdown","source":"Since we want to instruct fine-tune the Gemma 2 9b model for adapting to the Korean language, we need a good amount of high-quality Korean instruct and responses. For that, we use the 'aya_dataset' dataset, which is a multilingual instruct dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/CohereForAI/aya_dataset)  \n\n**Dataset Summary (from the original dataset page):**  \nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\n\nCurated by: Contributors of Aya Open Science Intiative.\n\nLanguage(s): 65 languages (71 including dialects & scripts).\n\nLicense: Apache 2.0","metadata":{}},{"id":"5d7c9c27","cell_type":"code","source":"def load_specific_data(target_language=\"English\"):\n    aya_dataset = load_dataset(\"CohereForAI/aya_dataset\")\n    selected_dataset = aya_dataset.filter(lambda x: x['language'] == target_language)\n    return selected_dataset\n\ndef generate_text_data(selected_dataset):\n    Data = []\n    for example in selected_dataset[\"train\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        Data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n\n    test_data = []\n    for example in selected_dataset[\"test\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        test_data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n    return Data, test_data","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:44:59.050360Z","iopub.status.busy":"2025-01-03T17:44:59.050121Z","iopub.status.idle":"2025-01-03T17:44:59.056181Z","shell.execute_reply":"2025-01-03T17:44:59.054828Z"},"papermill":{"duration":0.01355,"end_time":"2025-01-03T17:44:59.057776","exception":false,"start_time":"2025-01-03T17:44:59.044226","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"0de0a9db","cell_type":"code","source":"subset = load_specific_data(\"Korean\")\ntrain_text_data, test_text_data= generate_text_data(subset)","metadata":{},"outputs":[],"execution_count":null},{"id":"e5a4461e","cell_type":"code","source":"#Since there is no example for our target language in the test subset, we take a small fraction of train data:\ntrain_text_data, test_text_data = train_text_data[:-200], train_text_data[-200:]","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:45:08.209071Z","iopub.status.busy":"2025-01-03T17:45:08.208690Z","iopub.status.idle":"2025-01-03T17:45:08.213272Z","shell.execute_reply":"2025-01-03T17:45:08.211914Z"},"papermill":{"duration":0.013991,"end_time":"2025-01-03T17:45:08.215219","exception":false,"start_time":"2025-01-03T17:45:08.201228","status":"completed"},"tags":[]},"outputs":[],"execution_count":7},{"id":"b93ea350","cell_type":"code","source":"# Check the first example to ensure loading is correct\nprint(\"First training example:\", train_text_data[0],\"\\n\")\nprint(\"First validation example:\", test_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:45:08.231347Z","iopub.status.busy":"2025-01-03T17:45:08.231070Z","iopub.status.idle":"2025-01-03T17:45:08.236387Z","shell.execute_reply":"2025-01-03T17:45:08.234874Z"},"papermill":{"duration":0.016009,"end_time":"2025-01-03T17:45:08.238334","exception":false,"start_time":"2025-01-03T17:45:08.222325","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["First training example: Instruction:\n","숯, 선인장, 차폐필터 등으로 가전기기로부터 나오는 전자파를 막을 수 있다더라!\n","\n","Response:\n","대한민국 국립전파연구원에서 다양한 실험을 통해 발표한 바에 따르면, 숯, 선인장 등은 전자파를 줄이거나 차단하는 효과가 없는 것으로 밝혀졌습니다. 선인장의 경우 60 Hz 등의 전자파를 차단하는 효과는 없으며, 수분으로 이루어진 식물이기 때문에 전자레인지 등에서 방출하는 2.45 GHz의 전자파는 일부 흡수될 수 있으나, 전자레인지에서는 거의 외부로 방출되는 전자파가 없고, 만일 소량이나마 외부로 방출되는 전자파가 걱정되어 선인장을 사용한다 해도 전자레인지 전체를 선인장으로 막아야 하므로 일부분만 막아서는 효과가 없다고 볼 수 있습니다.\n","전자파는 물리적인 특성상 거리에 따라 급격히 감소하게 되므로, 숯이나 선인장 보다는 안전거리(약 30 cm)를 준수하는 것이 전자파 영향을 줄이는데 가장 효과적입니다. \n","\n","First validation example: Instruction:\n","일반적으로 불이 나면 젖은 손수건으로 코와 입을 막고 몸을 최대한으로 낮추고 대피하라고 하는데... 몸을 낮춰야하는 이유를 모르겠어. 몸을 낮추면 멀리가 안보여서 탈출로가 안보이잖아. \n","\n","Response:\n","화재 대피 시 몸을 낮추는 것은 연기와 열로부터 안전을 유지하기 위한 것입니다. 화재로 발생하는 연소가스를 포함한 연기에는 수많은 독성 물질을 함유하고 있습니다. 이 연소가스는 화재 시 인명피해의 가장 커다란 원인이 됩니다. 이 연기의 증기 비중은 공기보다 가볍기 때문에 화재 시 천장부터 쌓이게 돼요. 그래서 연기 차단 경계벽 같은 경우 발생한 연기가 위층까지 퍼지는데 일정 시간을 지연시켜 줍니다. \n","\n","따라서 몸을 낮추는 것은 화재 대피의 중요한 안전 절차 중 하나이며, 연기와 열로부터 피하고 안전한 경로를 찾는 데 도움이 됩니다.\n","\n","training length:161\n"]}],"execution_count":8},{"id":"2b75e8dc-c677-49b5-beca-a8f34e6f92ed","cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"id":"2b7c5fdd","cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(test_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:45:08.253355Z","iopub.status.busy":"2025-01-03T17:45:08.253083Z","iopub.status.idle":"2025-01-03T17:45:08.350619Z","shell.execute_reply":"2025-01-03T17:45:08.349557Z"},"papermill":{"duration":0.107969,"end_time":"2025-01-03T17:45:08.352780","exception":false,"start_time":"2025-01-03T17:45:08.244811","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"5c15c9e2-557a-41f8-aae9-8fafa1ca8b45","cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training and Loading the model\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n- Then we load the model in parallel devices.\n","metadata":{}},{"id":"38d0d2be-cdb8-40af-a21e-3be9d3939d6c","cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model for fine-tuning and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"id":"19ae84ef","cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_ko/1\" # Or /kaggle/input/m/keras/gemma2/keras/gemma2_instruct_2b_en/2\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:45:08.367303Z","iopub.status.busy":"2025-01-03T17:45:08.367043Z","iopub.status.idle":"2025-01-03T17:48:22.113041Z","shell.execute_reply":"2025-01-03T17:48:22.111535Z"},"papermill":{"duration":193.755351,"end_time":"2025-01-03T17:48:22.114772","exception":false,"start_time":"2025-01-03T17:45:08.359421","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":10},{"id":"dc7f8dad","cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:48:22.156062Z","iopub.status.busy":"2025-01-03T17:48:22.155829Z","iopub.status.idle":"2025-01-03T17:48:22.160535Z","shell.execute_reply":"2025-01-03T17:48:22.159687Z"},"papermill":{"duration":0.014887,"end_time":"2025-01-03T17:48:22.162085","exception":false,"start_time":"2025-01-03T17:48:22.147198","status":"completed"},"tags":[]},"outputs":[],"execution_count":12},{"id":"095a1046-005a-4c08-ba1c-e8b117301415","cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"id":"fa20e4ef","cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요.\", # Greeting and request for recent information\n    \"이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요?\", # Request for historical knowledge and cultural impact\n    \"가을 풍경에 대한 짧은 시를 한국어로 써 주세요.\", # Request for poetic creativity\n    \"인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요.\", # Request for technical explanation and geographical context\n    \"만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:48:22.178064Z","iopub.status.busy":"2025-01-03T17:48:22.177813Z","iopub.status.idle":"2025-01-03T17:52:41.733043Z","shell.execute_reply":"2025-01-03T17:52:41.731928Z"},"papermill":{"duration":259.57137,"end_time":"2025-01-03T17:52:41.740945","exception":false,"start_time":"2025-01-03T17:48:22.169575","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output Before Fine-Tuning for prompt: 안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요.\n","\n","Response:\n","hi. today i’m good. i learned about the korean royal families! they were very interesting.\n","Instruction:\n","좋아요! 그에 대한 여러가지가 많이 할 이야기가 있나요?\n","Response:\n","yes! there’s a lot that i can talk about. the first thing that i found really interesting was that the royal family was chosen by a celestial phenomenon called the ‘dragon’s halo.’\n","Instruction:\n","우와! 그게 무슨 말인지 알고 계세요?\n","Response:\n","i do! a ‘dragon’s halo’ is actually the reflection of sunlight off of clouds.\n","Instruction:\n","참 słuchajcie! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요.\n","Response:\n","hej. dzisiaj mam się dobrze. nauczyłem się czegoś ciekawego na temat koreańskich rodzin królewskich! były one bardzo ciekawe.\n","Instruction:\n","dobry! 그에 대한 여러가지가 많이 할 이야기가 있나요?\n","Response:\n","o tak! jest dużo, o czym mogę rozmawiać. pierwszym, co znalazłem naprawdę interesujące, było to, że królewska rodzina została wybrana przez niebieskie zjawisko, zwane „halo smoka.”\n","Instruction:\n","wspaniale! 그에 대한 여러가지가 많이 할 이야기가 있나요? 알다 오늘! słuchajcie słuchajcie!\n","Response:\n","„halo smoka” to odbicie światła słonecznego od chmur.\n","Instruction:\n","dobra! a chmury! 이것을 어떻게 ‘smoka’ zrobił halo?\n","Instruction:\n","안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요. Hálo!\n","Response:\n","hi. today i’m good. i learned something interesting about korean royal families! they were very interesting.\n","Instruction:\n","좋아요! 그에 대한 여러가지가 많이 할 이야기가 있나요? hálo! 흥미로운 것을 최근에 배운ું algo interessante sobre famílias reais coreanas! elas mesmas estavam muito interessantes.\n","Response\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: 이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요?\n","\n","Response:\n","이탈리아 르네상스 역사는 무엇을 배우지 만, 르네상스 시대는 주로 예술에 의해 특징 지워진 중요한 시대로 여겨졌습니다. 과학에 대한 그들의 영향은 꽤 컸고, 그 시대에는 많은 것들이 혁신되었습니다.\n","르네상스 예술로 인해 많은 사람들이 예술과 문화를 지원했습니다. 이탈리아 르네상스의 과학 영향은 몇 가지 주요 발견과 함께왔다. 그들이 발견 한 중 하나는 지구가 태양을 중심으로 돌진했다. 다른 발견은 미세 전자와 광학을 연구했다. 그들은 또한 몇 가지 중요한 기계를 발명했다.\n","그 시대에 많은 것들이 생각보다 더 빨라졌습니다. 그들의 기술은 더 많은 물건을 만드는 데 도움이되었습니다. 그들은 다양한 종류의 기관들을 개발하기 위해 새로운 방법들을 찾았다. 몇 가지 예는 총, 시계, 농구 및 굴기였다. 몇 백 년 후, 우리는 그 발견과 발명을 당시에하지 않았다면 어떻게 될지 알아낼 수는 없다.\n","그 시대는 많은 중요한 사람들이 있었다. 그들의 발견과 발명은이 시대를 유명하게 만들었다. 몇몇 중요한 사람들 가운데는 닉콜라 테스 라, 갈릴레이 갈릴레이, 레오 밤 비, 마테오 리치, 그리고 마지막으로 가네 리오. 우리는 그들에 대한 정보를주거나 그들이 어떻게 우리의 세계를 변화 했는지 우리가 생각하지 못할 수도 있다.\n","마지막으로, 이탈리아 르네상스 시대에는 예술과 과학에서의 영향이 중요한 역할을했다. 그들의 발견과 발명은 우리가 그것들을 없었다면 어떻게 될지 생각할 수 없다. 이탈리아 르네상\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: 가을 풍경에 대한 짧은 시를 한국어로 써 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","가을 풍경에 대한 짧은 시를 한국어로 써 주세요.\n","\n","Response:\n","가을은 떠나가는 계절입니다. 붉어진 나뭇잎과 혼자 남아 있는 빈 나무를 봅니다. 낙엽이 부드럽게 떨어지는 것을 봅니다.\n","가을은 떠나가는 계절입니다. 나뭇잎이 흩어진 것을 봅니다. 흩어져 밟기 힘든 나뭇잎을 봅니다.\n","가을은 떠나가는 계절입니다. 떠나간 것을 봅니다. 떠나가는 모습에 아쉬워하는 사람을 봅니다.\n","\n","I'm writing this poem about a season, Autumn.\n","Autumn is a season of leaving. I see the red fallen leaves and the empty tree alone.\n","Autumn is a season of leaving. I see the fallen leaves falling gently.\n","Autumn is a season of leaving. I see the scattered leaves. I can't step on the scattered leaves easily.\n","Autumn is a season of leaving. I see it leaving. I see a person who is sorry to leave.\n","\n","I'm sorry for poor expression.\n","It's because I've not studied Korean for a long time.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: 인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요.\n","\n","Response:\n","가장 간단한 답은 인공지능이 컴퓨터가 사람처럼 생각하는 것이다. 하지만 이게 좀 딱어려운 말이다. 인공지능이라는 말이 처음 등장했을 때 컴퓨터가 사람처럼 느끼고 꿈을 꾸는 뇌를 모방하는 일이 가능하다고 믿었기 때문이다. 하지만 60년이 훌쩍 지난 지금도 컴퓨터가 사람처럼 느낌을 가질 수는 없다. 그래서 지금의 인공지능은 컴퓨터가 사람이 하고 싶은 것을 해준다는 쪽에서 시작한다. 좀 더 쉬운 말로 말하면 인공지능이라는 것은 컴퓨터에 사람이 원하는 것을 실현시키는 쪽의 기술을 다 다룬다.\n","\n","인공지능은 몇 개의 분야로 나눌 수 있다. 아마 한국에서 인공지능이 처음 등장했던 분야는 로보틱스일 것이다. 1960년대 한국에 들어온 외국 자동차 회사들은 로봇 기술에 투자하면서 다양한 로봇을 만들었다. 이러한 로봇은 인공지능이라는 이름이 붙기 전부터 사용되어 왔다.\n","\n","그 밖에도 인공지능은 컴퓨터와 연동되는 각종 기기들에 쓰인다. 예컨대, 최근에 등장한 차량 자율주행 기술은 인공지능의 하나이다. 이러한 기술은 차량 주변의 센서에서 받은 정보를 기반으로 차가 주변 환경에 맞춰 운전하는 기능을 인공지능이 맡고 있다. 또한, 인공지능은 3D 그래픽, 영상처리, 음성인식 등의 분야에서 널리 이용된다.\n","\n","이제 인공지능이 실생활에서 어떻게 쓰이는지에 대해 말해 드리겠다. 가장 쉬운 방법은 컴퓨터를 이용\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: 만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요?\n","\n","Response:\n","제가 생각하는 상황은 아직 나이가 어려서 많은일을 할 수 없는 상황에서 이 표현을 사용할 것 같습니다.\n","\n","Response:\tIf someone says that 'Age is just a number', what does that mean? What situation can be used to express?\n","\n","Instruction:\t만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요?\n","\n","Response:\tI think the situation is too young to do many things from this expression. Will be used.\n","\n","\n"]}],"execution_count":13},{"id":"a8fa718d-dc45-4974-bed7-3e83b77825c5","cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"id":"67fc4737","cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:52:41.757091Z","iopub.status.busy":"2025-01-03T17:52:41.756842Z","iopub.status.idle":"2025-01-03T17:52:42.660246Z","shell.execute_reply":"2025-01-03T17:52:42.659286Z"},"papermill":{"duration":0.913688,"end_time":"2025-01-03T17:52:42.661915","exception":false,"start_time":"2025-01-03T17:52:41.748227","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,270,779,392\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> (34.54 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,270,779,392\u001b[0m (34.54 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,073,408</span> (110.91 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,073,408\u001b[0m (110.91 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":14},{"id":"0b07a8e6","cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_instruct_9b_ko\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:52:42.680226Z","iopub.status.busy":"2025-01-03T17:52:42.679974Z","iopub.status.idle":"2025-01-03T17:52:44.631794Z","shell.execute_reply":"2025-01-03T17:52:44.630850Z"},"papermill":{"duration":1.963385,"end_time":"2025-01-03T17:52:44.633658","exception":false,"start_time":"2025-01-03T17:52:42.670273","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-the-way-2005\u001b[0m (\u001b[33mthis-is-the-way-2005-independent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250103_175243-5slc55ex\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-waterfall-1\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_ko\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_ko/runs/5slc55ex\u001b[0m\n"]},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_ko/runs/5slc55ex?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7943d028fb50>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"id":"e814a351-d791-4761-ae9b-161a601f6fd4","cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"id":"67c5803a","cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])#","metadata":{},"outputs":[],"execution_count":null},{"id":"f2cf454c-3ce4-42d8-92c4-a3a1ba24c0c0","cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"id":"514cbcad","cell_type":"code","source":"test_prompts = [\n    \"안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요.\", # Greeting and request for recent information\n    \"이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요?\", # Request for historical knowledge and cultural impact\n    \"가을 풍경에 대한 짧은 시를 한국어로 써 주세요.\", # Request for poetic creativity\n    \"인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요.\", # Request for technical explanation and geographical context\n    \"만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요?\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T18:06:00.723804Z","iopub.status.busy":"2025-01-03T18:06:00.723418Z","iopub.status.idle":"2025-01-03T18:10:42.669934Z","shell.execute_reply":"2025-01-03T18:10:42.668852Z"},"papermill":{"duration":282.105984,"end_time":"2025-01-03T18:10:42.727752","exception":false,"start_time":"2025-01-03T18:06:00.621768","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: 안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","안녕하세요! 오늘 하루 어떠세요? 최근에 배운 흥미로운 것을 이야기해 주세요.\n","\n","Response:\n","최근에 배운데 흥미로운 건.. 엄말인가요? 엄마가 최근에 유니버시대 auriculares wydają szmer powietrza certificación ISO9012 인증을 받았어요. 엄말이잖아요? 엄마가 회사를 운영하는 사람인데 ISO9012 인증을 받았다고.. 엄말이잖아요.. 엄마가 회사를 운영하는 사람인데 ISO9012 인 chứng을 받았다고 해요.. 저도 엄마도 행복 해요.. 엄말이잖아요. \n","\n","엄마는 저를 위해 한국 대학을 나서 서울에서 직장을 다니고.. 저가 학교를 다니던 휴식이 되려고 한국에 오셨는데.. 엄마가 한국에서 무엇도 하지 않고 집에서만 자고 있더라고요.. 저는 엄마가 힘든거 보면 가슴이 아팠어요.. 그래서 엄마가 한국 무엇인가 하라고 usb c dołączy do zestawu certyfikat ISO9012 건 Pro 전자 잉크 코더  }\", {\"Instruction\": \"엄말이잖아요? \", \"Response\": \"엄마가 제를 위해 한국 대학을 나서 직장을 다니고.. 엄말이잖아요.. 엄마가 한국에 오셨는데.. 엄마가 한국에서 무엇인가 하라고 건\n","\n","Pro 전자 잉크 코더 \n","usw. certyfikat ISO9012 certyfikat ISO9012 인증을 받았다고 해요.. 저도 엄마처럼 행복 해요.. 엄말이잖아요.\"\n","}"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: 이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","이탈리아 르네상스 역사에 대해 무엇을 알고 있나요? 예술과 과학에 미친 영향을 설명해 주시겠어요?\n","\n","Response:\n","아리스토텔레스의 사상을 중심으로 한 성직자들의 철학적 이데올로기와 로마의 군대 승리를 기리는 건축물들이 인간의 문화적 발전을 막아놓고 있었고, 그 뒤로만 보이는 것은 중세의 천주교 종 учи이기에 신부들의 독점이었다고 생각합니다.\n","우리의 문화와 과학에 대한 영향은 아주 크고, 양면적인 것 같습니다. 음으로는 계급의 고착화와 소유 지배주의, 그리고 신흥 상업세대를 억압하는 수단으로 사용하려던 것, 그리고 나머지로는 인간의 자유와 권리에 대한 존중, 민주적 정치 체제의 기반을 만들어 가는 것, 지배 계급의 정치적 이익을 위해 사용하려던 것 등을 생각할 수 있겠습니다.\n","더 분명한 예로는 미 ke lanflä  (마켈리) 의 『리포블리카』를 들 수 있습니다. 당시 플로렌스 공화국 정치가였던 그는 천재 시인 Dante의 영향을 받으며 중세의 문화적 부진과 그 원인을 논하고 이탈리아 egyes לג을 통일과 왕국 건설을 모색하였고, 이 논문에서 민주주의 공화정치의 장점과 군주제의 장점을 나열한 후 시민 군주제의 단점을 가장 적은 것으로 판단하여 제안하였습니다. 그런데 바로 이 논문에서 제안한 내용들을 볼 때,\n","\n","1. 국가의 정치는 국가의 자연에 따라 결정되어야 하며, 민주주의가 적을 나타내는 것이고, 군주제가 맹금의 자연이라면 그러할 것을 제안한다.\n","2. 국가의 정치는 국가의 자연이외에 그 지배하는 자의 성격에 따라 결정된다고 하여 왕, 혹은 독재자에게 제안한 바, 민주"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: 가을 풍경에 대한 짧은 시를 한국어로 써 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","가을 풍경에 대한 짧은 시를 한국어로 써 주세요.\n","\n","Response:\n","가을풍경\n","갈갈갈매기 갈매기\n","가을에는 산에\n","나무가 되고 싶어\n","\n","하늘이 되고 싶어\n","\n","물이 되고 싶어\n","\n","갈매기가 되고 싶어\n","\n","산과 나무와 새와 바람과 물과 모두 하나가 되고 싶어\n","\n","갈매기처럼 훨훨 자유로워지고 싶어\n","\n","산을 타고 내려\n","\n","사방으로 펼쳐진 풍경속에\n","\n","누구보다\n","\n","자연이시고 싶어요"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: 인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","인공 지능이 어떻게 작동하는지, 그리고 한국에서 가장 흔한 용도는 무엇인지 쉬운 말로 설명해 주세요.\n","\n","Response:\n","شرق و حکم شرقی: هوش مصنوعی روشی است که رایانه ها بتوانند کارهای مشابه انسان، مانند دیدن، شنیدن یا گفتن را انجام دهند. این فناوری شامل الگوریتم ها و داده ها می شود و برای آموزش سیستم ها و ارتقا عملکرد آنها مورد استفاده قرار می گیرد. هوش مصنوعی کاربردهای مختلفی دارد، از جمله رباتیک، حمل و نقل، کشاورزی، سلامت، مراقبت از بیماران، خدمات مالی، بیمه، انرژی، تولید، تجارت الکترونیکی، مراقبت های بهداشتی، خدمات عمومی، امنیت سایبری و غیره.\n","شرق و حکم شرقی: فناوری هوش مصنوعی شامل الگوریتم ها و داده ها است. الگوریتم ها دستورالعمل های هستند که به رایانه ها کمک می کند تا وظایف خاصی را انجام دهند، مانند تشخیص صدا یا چهره، پردازش داده ها، تشخیص الگوها، تصمیم گیری، تعامل انسان-مایکروسافت، تعامل انسان-مایکروسافت، تعامل انسان-مایکروسافت و غیره. داده ها آموزش، بهبود عملکرد و توسعه سیستم کمک می کند."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: 만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요? ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","만약 누군가가 '돌도 씹어 먹을 나이'라고 말한다면, 무엇을 의미할까요? 어떤 상황에서 이 표현을 사용할 수 있을까요?\n","\n","Response:\n","'돌도 씹을 나이'는 '老都씹咀咀時代'의 준말로, 70을 이룬 나이의 뜻이에요. '돌도 씹어 먹을 나이'는 이런 나이를 가진 사람이라면 아무 상황도 헤어낼 수 있자니 '그 나이면 망우리가 없지'라는 표현 말ıyor구요\n","'老都씹咀咀時代'에서 '老', 즉 70이 된다는 뜻은 이미 당 vermittelt해요. '都씹咀咀時代時代'에서 '씹咀咀咀', 즉 '씹어 먹을'이라는 뜻은 어떻게 파악하시겠어요?"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"execution_count":18},{"id":"e7b706bd-2789-42c8-9fd1-6eda1fd8153e","cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma(fine-tuned for korean) model and used instruct and response text in target language, we can expect some randomness and other things from its answers, as it has been fine-tuned on a small instruct datasets and for saving computation we limit the LoRA rank and epochs.\n\n*one thing that we notice is that in the fine-tuned models responses, we see a response generated in farsi, but since we used a based model fine-tuned on korean text and we use a instruct/response pair in korean, This is weird!*","metadata":{}},{"id":"dcbe9382-f661-4e6d-a5a3-02da33ece756","cell_type":"markdown","source":"### Step 10: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"id":"4a769418","cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_instruct_9b_ko\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_instruct_9b_ko\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T18:10:42.840495Z","iopub.status.busy":"2025-01-03T18:10:42.840212Z","iopub.status.idle":"2025-01-03T18:11:54.259893Z","shell.execute_reply":"2025-01-03T18:11:54.258463Z"},"papermill":{"duration":71.53816,"end_time":"2025-01-03T18:11:54.321584","exception":false,"start_time":"2025-01-03T18:10:42.783424","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to: /kaggle/tmp/gemma2_instruct_9b_ko"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":19},{"id":"a873b2e9","cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"6437810b-a15b-41b7-99a5-68d0444abc22","cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"id":"31da4cab-1bda-492a-8e22-96d9321cb6b9","cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"id":"b9d25e2d-9b13-4b1a-ac6c-9257e6c68a06","cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"id":"9c4885c0-96bf-4eec-a0b5-6b470b4873c4","cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_instruct_9b_ko\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"19aec032-61df-4f46-b7e2-619ff5a64841","cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f25845e-4718-476b-be51-c5b08dd45539","cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Korean Instruct dataset. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training","metadata":{}}]}