{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"isSourceIdPinned":true,"modelId":163613,"modelInstanceId":181437,"sourceId":212866,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":4881.287775,"end_time":"2025-01-01T17:31:21.386142","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-01T16:10:00.098367","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e1773dd8-c557-455c-9a40-d1800c24c604","cell_type":"markdown","source":"# Instruct Fine-Tuning Gemma for Persian Language\nThis notebook demonstrates the fine-tuning of the Gemma model on Persian datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the Persian Instruct dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"id":"3d079306-46a2-43de-b7bd-419eb71127b5","cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_fa)","metadata":{}},{"id":"51ef7e97-0ad5-444e-a571-c8c4f0530960","cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used the fine-tuned version of the gemma2_9b_en which is fine-tuned on Persian(farsi) data, The model [link](https://www.kaggle.com/models/mahdiseddigh/gemma2/keras/gemma2_9b_fa)","metadata":{}},{"id":"b7c4b6c8-23d5-4b90-912c-cbe11a0070e2","cell_type":"markdown","source":"# Note:\n#### I mistakenly used French data for training a model that I wanted to work with persian, I noticed this after the training was over.\n##### How ever the model turned out to be working better on farsi even that it is fine-tuned on french language, I have two possible reasones for this:\n- **1- using the fine-tuned base model(on farsi)**\n- **2- since the base model was also trained on large amount of multi lingual data, I think the model adapts realtions between the french instruct/response pairs and uses that knowledge in generating for farsi which is a different language**","metadata":{}},{"id":"5fe87014-4ef5-4d5b-864b-a45f250ff97f","cell_type":"markdown","source":"### My Gemma2 cookbook:\nI made this repo and I'm uploading all notebooks related to working with gemma models, check it out:\nhttps://github.com/Mhdaw/Gemma2","metadata":{}},{"id":"f6811334-e7c6-475e-a49d-96543294173f","cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"id":"20a17589","cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-01T16:10:03.087400Z","iopub.status.busy":"2025-01-01T16:10:03.087031Z","iopub.status.idle":"2025-01-01T16:11:53.852709Z","shell.execute_reply":"2025-01-01T16:11:53.851445Z"},"papermill":{"duration":110.77349,"end_time":"2025-01-01T16:11:53.854842","exception":false,"start_time":"2025-01-01T16:10:03.081352","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]},{"name":"stdout","output_type":"stream","text":["\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]}],"execution_count":1},{"id":"68a01b8e","cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:11:53.864470Z","iopub.status.busy":"2025-01-01T16:11:53.864216Z","iopub.status.idle":"2025-01-01T16:12:03.299857Z","shell.execute_reply":"2025-01-01T16:12:03.299011Z"},"papermill":{"duration":9.443083,"end_time":"2025-01-01T16:12:03.301916","exception":false,"start_time":"2025-01-01T16:11:53.858833","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1735747919.131944      74 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: === \n","learning/45eac/tfrc/runtime/common_lib.cc:479\n","E0101 16:11:59.171488397     185 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2025-01-01T16:11:59.171471319+00:00\"}\n"]},{"data":{"text/plain":["[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n"," TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n"," TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n"," TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n"," TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n"," TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n"," TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n"," TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"id":"32f6031f-dae3-4df5-9e3a-73e42a74f41b","cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"id":"10be414e","cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:03.314352Z","iopub.status.busy":"2025-01-01T16:12:03.314002Z","iopub.status.idle":"2025-01-01T16:12:03.318183Z","shell.execute_reply":"2025-01-01T16:12:03.317301Z"},"papermill":{"duration":0.013209,"end_time":"2025-01-01T16:12:03.319797","exception":false,"start_time":"2025-01-01T16:12:03.306588","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"302cd9aa","cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:03.328868Z","iopub.status.busy":"2025-01-01T16:12:03.328646Z","iopub.status.idle":"2025-01-01T16:12:18.443891Z","shell.execute_reply":"2025-01-01T16:12:18.443148Z"},"papermill":{"duration":15.122792,"end_time":"2025-01-01T16:12:18.446513","exception":false,"start_time":"2025-01-01T16:12:03.323721","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"execution_count":4},{"id":"5713b714-3d6c-42ca-91e7-5e7248f51805","cell_type":"markdown","source":"## Step 2: Load and Explore Persian Dataset\nWe are using the `CohereForAI/aya_dataset` dataset with Persian insturct data. \n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"id":"a1082628-aa8d-4577-ae32-92f5aa425743","cell_type":"markdown","source":"Since we want to instruct fine-tune the Gemma 2 9b model for adapting to the Persian language, we need a good amount of high-quality Persian instruct and responses. For that, we use the 'Persian' dataset, which is a multilingual instruct dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/CohereForAI/aya_dataset)  \n\n**Dataset Summary (from the original dataset page):**  \nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\n\nCurated by: Contributors of Aya Open Science Intiative.\n\nLanguage(s): 65 languages (71 including dialects & scripts).\n\nLicense: Apache 2.0","metadata":{}},{"id":"2147d5b8","cell_type":"code","source":"def load_specific_data(target_language=\"English\"):\n    aya_dataset = load_dataset(\"CohereForAI/aya_dataset\")\n    selected_dataset = aya_dataset.filter(lambda x: x['language'] == target_language)\n    return selected_dataset\n\ndef generate_text_data(selected_dataset):\n    Data = []\n    for example in selected_dataset[\"train\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        Data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n\n    test_data = []\n    for example in selected_dataset[\"test\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        test_data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n    return Data, test_data","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:18.456436Z","iopub.status.busy":"2025-01-01T16:12:18.456197Z","iopub.status.idle":"2025-01-01T16:12:18.461536Z","shell.execute_reply":"2025-01-01T16:12:18.460782Z"},"papermill":{"duration":0.012829,"end_time":"2025-01-01T16:12:18.463423","exception":false,"start_time":"2025-01-01T16:12:18.450594","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"7ae2eeda","cell_type":"code","source":"subset = load_specific_data(\"French\")#My mistake!\ntrain_text_data, test_text_data= generate_text_data(subset)","metadata":{},"outputs":[],"execution_count":null},{"id":"d4b5d5b7","cell_type":"code","source":"#Since there is no example for our target language in the test subset, we take a small fraction of train data:\ntrain_text_data, test_text_data = train_text_data[:-200], train_text_data[-200:]","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:26.729867Z","iopub.status.busy":"2025-01-01T16:12:26.729544Z","iopub.status.idle":"2025-01-01T16:12:26.733661Z","shell.execute_reply":"2025-01-01T16:12:26.732794Z"},"papermill":{"duration":0.012857,"end_time":"2025-01-01T16:12:26.735413","exception":false,"start_time":"2025-01-01T16:12:26.722556","status":"completed"},"tags":[]},"outputs":[],"execution_count":7},{"id":"9bc6363d","cell_type":"code","source":"# Check the first example to ensure loading is correct\nprint(\"First training example:\", train_text_data[0],\"\\n\")\nprint(\"First validation example:\", test_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:26.747930Z","iopub.status.busy":"2025-01-01T16:12:26.747691Z","iopub.status.idle":"2025-01-01T16:12:26.752653Z","shell.execute_reply":"2025-01-01T16:12:26.750953Z"},"papermill":{"duration":0.012723,"end_time":"2025-01-01T16:12:26.753602","exception":false,"start_time":"2025-01-01T16:12:26.740879","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["First training example: Instruction:\n","Quels président des États-Unis ne s’est jamais marié ?\n","\n","Response:\n","James Buchanan est le seul président qui ne s'est jamais marié. \n","\n","First validation example: Instruction:\n","Quels sont les 10 termes clés que je devrais connaître sur le changement climatique ? Définissez-les brièvement.\n","\n","Response:\n","Bien sûr! Voici 10 termes clés liés au changement climatique accompagnés de brèves définitions :\n","\n","Changement climatique : altération à long terme des conditions météorologiques moyennes de la Terre, souvent causée par des activités humaines qui libèrent des gaz à effet de serre dans l'atmosphère.\n","\n","Effet de serre : Processus naturel par lequel certains gaz présents dans l'atmosphère retiennent la chaleur du soleil, réchauffant ainsi la surface de la Terre.\n","\n","Réchauffement climatique : Augmentation de la température moyenne de la Terre due à l'augmentation de l'effet de serre, principalement causée par les activités humaines.\n","\n","Dioxyde de carbone (CO2) : Un gaz à effet de serre libéré par la combustion de combustibles fossiles et la déforestation, contribuant au changement climatique.\n","\n","Atténuation : mesures prises pour réduire ou prévenir les émissions de gaz à effet de serre, visant à atténuer la gravité du changement climatique.\n","\n","Adaptation : stratégies et changements mis en œuvre pour s'adapter aux impacts du changement climatique, comme la construction de défenses contre les inondations ou la modification des pratiques agricoles.\n","\n","Énergie renouvelable : sources d'énergie comme l'énergie solaire, éolienne et hydroélectrique qui ne produisent pas d'émissions de gaz à effet de serre et sont durables.\n","\n","Déforestation : défrichement des forêts, souvent à des fins agricoles ou de développement, qui réduit la capacité de la Terre à absorber le dioxyde de carbone.\n","\n","Élévation du niveau de la mer : augmentation du niveau moyen des océans de la planète due à la fonte des glaciers et à la dilatation thermique de l'eau de mer à mesure qu'elle se réchauffe.\n","\n","Accord de Paris : traité international adopté en 2015, par lequel les pays se sont engagés à réduire les émissions de gaz à effet de serre et à limiter le réchauffement climatique bien en dessous de 2 degrés Celsius au-dessus des niveaux préindustriels.\n","\n","training length:1222\n"]}],"execution_count":8},{"id":"879d5143-76e9-40f5-89f3-0e99a0a2b3fe","cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"id":"cdbf8453","cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(test_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:26.766594Z","iopub.status.busy":"2025-01-01T16:12:26.766385Z","iopub.status.idle":"2025-01-01T16:12:26.869955Z","shell.execute_reply":"2025-01-01T16:12:26.868642Z"},"papermill":{"duration":0.112518,"end_time":"2025-01-01T16:12:26.871831","exception":false,"start_time":"2025-01-01T16:12:26.759313","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"1520ee0a-4e52-4df1-b6b4-8a827436debe","cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training and Loading the model\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n- Then we load the model in parallel devices.\n","metadata":{}},{"id":"3c7d1e47-a524-481d-9097-0118dd003824","cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model for fine-tuning and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"id":"acd0ae1f","cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_fa/1\" # Or /kaggle/input/m/keras/gemma2/keras/gemma2_instruct_2b_en/2\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:12:26.886712Z","iopub.status.busy":"2025-01-01T16:12:26.886388Z","iopub.status.idle":"2025-01-01T16:15:43.268281Z","shell.execute_reply":"2025-01-01T16:15:43.266771Z"},"papermill":{"duration":196.3917,"end_time":"2025-01-01T16:15:43.270175","exception":false,"start_time":"2025-01-01T16:12:26.878475","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":10},{"id":"3b4e0812","cell_type":"code","source":"decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\nprint(type(decoder_block_1))\nfor variable in decoder_block_1.weights:\n  print(f'{variable.path:<48}  {str(variable.shape):<14}  {str(variable.value.sharding.spec)}')","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:15:43.286439Z","iopub.status.busy":"2025-01-01T16:15:43.286158Z","iopub.status.idle":"2025-01-01T16:15:43.291563Z","shell.execute_reply":"2025-01-01T16:15:43.290334Z"},"papermill":{"duration":0.015195,"end_time":"2025-01-01T16:15:43.292799","exception":false,"start_time":"2025-01-01T16:15:43.277604","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'keras_hub.src.models.gemma.gemma_decoder_block.GemmaDecoderBlock'>\n","decoder_block_1/pre_attention_norm/scale          (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_attention_norm/scale         (3584,)         PartitionSpec(None,)\n","decoder_block_1/attention/query/kernel            (16, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/key/kernel              (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/value/kernel            (8, 3584, 256)  PartitionSpec('model', None, None)\n","decoder_block_1/attention/attention_output/kernel  (16, 256, 3584)  PartitionSpec('model', None, None)\n","decoder_block_1/pre_ffw_norm/scale                (3584,)         PartitionSpec(None,)\n","decoder_block_1/post_ffw_norm/scale               (3584,)         PartitionSpec(None,)\n","decoder_block_1/ffw_gating/kernel                 (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_gating_2/kernel               (3584, 14336)   PartitionSpec(None, 'model')\n","decoder_block_1/ffw_linear/kernel                 (14336, 3584)   PartitionSpec('model', None)\n"]}],"execution_count":11},{"id":"5eea9606","cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:15:43.307864Z","iopub.status.busy":"2025-01-01T16:15:43.307586Z","iopub.status.idle":"2025-01-01T16:15:43.312445Z","shell.execute_reply":"2025-01-01T16:15:43.311318Z"},"papermill":{"duration":0.014225,"end_time":"2025-01-01T16:15:43.314045","exception":false,"start_time":"2025-01-01T16:15:43.299820","status":"completed"},"tags":[]},"outputs":[],"execution_count":12},{"id":"ad5f2739-11bc-4a2d-b6e1-597f2900ddbb","cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"id":"1dd64945","cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن.\", # Greeting and request for recent information\n    \"راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟\", # Request for historical knowledge and cultural impact\n    \"یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس.\", # Request for poetic creativity\n    \"به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه.\", # Request for technical explanation and geographical context\n    \"اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:15:43.329583Z","iopub.status.busy":"2025-01-01T16:15:43.329311Z","iopub.status.idle":"2025-01-01T16:20:00.583693Z","shell.execute_reply":"2025-01-01T16:20:00.582267Z"},"papermill":{"duration":257.270185,"end_time":"2025-01-01T16:20:00.591196","exception":false,"start_time":"2025-01-01T16:15:43.321011","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output Before Fine-Tuning for prompt: سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن.\n","\n","Response:\n","I have learned that you have to say \"khodahafez\" when you end a phone call.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟\n","\n","Response:\n","رنانسانس به دوران پس از تاریکی میانه ها و قبل از انقلاب صنعتی اطلاق می گردد که در اروپای مرکزی جریان داشت. در طی سالهای 1450 تا 1550 رنسانس در ایتالیا به عنوان یک پدیده فرهنگی شکل گرفت. در طی قرن ها پیش از آنکه رنسانس آغاز گردد، فرهنگ اروپا در دام تاریکی میانه ها گرفتار بود. در طی تاریکی میانه ها، کلیسای کاتولیک زورگویانه عمل می نمود و آزادی اندیشه را سرکوب می نمود. این امر به فروگرفتن علوم و فرهنگ در طول قرون وسطی منجر گردید. با روی کار آمدن ایده رنسانس، به دلیل تلاشهای دانشمندان و فیلسوفان رنسانسی، اندیشه ای مبنی بر شکاکیت در بین مردم رواج یافت. این امر منجر به روی کار آمدن دانشمندانی همچون کپلر و گاالیله گردید که نظریه های علمی کلیسای کاتولیک را زیر سئوال می بردند. رنسانس همچنین منجر به خلق آثار شاعرانه و زیبای هنری در طی این دوران گردید. رنسانس با انقلاب صنعتی که در طی قرن 19 در اروپا آغاز گردید، با مکانیزمهای جدید صنعتی، برداشتهای جدید مردم از جهان را محدود نمود.\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس.\n","\n","Response:\n","که بر چمنگاه سبز گیسوانم\n","بر سر باغ پر چین و شونه‌ی سر\n","بر دامن جان کهن و خرم من\n","بر سنگ دل بی سر و ته من\n","که برگردانم برگ های پاییزی\n","برگ های زرد و سرخ و نارنجی\n","که زرد و سرخ و نارنجی اند\n","بر شاخه های درخت تنومند\n","تنومند و خشک و شکسته شده\n","شکسته شده از دست زمان\n","از دست زمان بزرگ و سفید\n","برگ های زرد و سرخ و نارنجی\n","برگ های تو ای عاشق من\n","عاشق من دیوانه و شیدا\n","شیدا و بیابان و پر بلا\n","بیابان و بیواسته شده\n","و نگاه های پاییزی من\n","پاییزی ام از نگاه های تو\n","تو و عشقی که سر به فلک کشیده\n","سربه فلک و خشک و بی آبی\n","بی آبی از روی دامن آتش\n","از دامن آتش تامی\n","تامیمی در دل زمستانی\n","در دل زمستانی من امشب\n","امشب پاییز مرا ببوس\n","بوس پاییزی و عاشقانه\n","عاشقانه و بی جان و سرگسسته\n","سرگسسته از عشق بی جواب\n","بی جواب و نگاه های پاییزی\n","نگاه های پاییزی من امشب\n","پاییز من امشب بگو مرا\n","بگو زمستان عاشقانه ای\n","عاشقانه ای در راه است\n","در راه و برگ های پاییزی\n","برگ های پاییزی من امشب\n","پاییز من بگو مرا ببوس\n","بوس مرا با نگاه های پاییزی\n","نگاه های پاییزی من امشب\n","یه آهنگ ترکیبی از موسیقی های مختلف بنویسم؟\n","آهنگ ترکیبی بنویسم؟\n","شاید بله\n","به شرط آنکه خواب من\n","خواب مرا ببیند امشب\n","خواب امشب من عید است\n","عید و نگاه های پاییزی\n","نگاه های پاییزی من امشب\n","یه نگاه ترکیبی بنویسم\n","نگاه ترکیبی بنویسم\n","بوسه های پاییزی امشب\n","پاییز من بگو مرا ببوس\n","بوسه های ترکیبی پاییزی\n","نگاه های ترکیبی پاییزی\n","پاییز من بگو مرا ببوس\n","بوسه های ترکیبی پاییزی\n","نگاه های ترکیبی پاییزی\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه.\n","\n","Response:\n","درک و برنامه‌ریزی برای انجام کارهایی که معمولاً به درک پیچیده‌ای از جهان و نحوه عمل آن نیاز دارند. کاربردهای آن در بسیاری از زمینه‌ها مانند تشخیص پزشکی، مدیریت دارایی، تجزیه و تحلیل حقوقی و بسیاری موارد دیگر دیده می‌شود. این فناوری همچنین می‌تواند به کسب‌وکارها در بهبود خدمات مشتری، اتوماسیون فرآیندها، پیش‌بینی نتایج و تصمیم‌گیری بهتر و نوآوری و توسعه محصول جدید کمک کند.\n","Artificial intelligence (AI) is the science of making machines or computer systems work like humans. AI systems can perceive their environment, learn from experience, reason logically, communicate naturally using language, and exercise good judgment. Common applications of artificial intelligence include speech recognition, image recognition, decision-making, natural language processing, and machine learning. AI can also help businesses improve customer service, automate processes, make better predictions and decisions, as well as innovate and develop new products.\n","\n","برچسب ها: هوش ، مصنوعی ، تعریف ، هوش ، مصنوعی ، یعنی ، چیستان ، چیستانها ، احمقانه ، جالب ، جالب ترین\n","\n","\n","\n","--- Model Output Before Fine-Tuning for prompt: اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟\n","\n","Response:\n","وقتی کلاً همه چی به دست یه نفره و اونم خیلی ساده و بی‌خود، اصلاً به چشم نمیاد، اون موقع میگیم پا رو از گلیمش درازتر کرده.\n","برای مثال: آقا جون، خواهری دارم که مجرده، میشه با یکی از آشنایان و خوبانتون آشنا بشی؟ - چشماležit، تو کل تهران یه دختر خوب نیافتی؟ - آفرین داداشی، امشب برا کانال منم ویدیو میاری؟\n","اگه کسی پا رو از گلیمش درازتر کرده، یعنی خیلی کارهای اونو انجام داده، در حالی که اون خیلی کارهای ساده‌ایو انجام داده. گلیمش هم به اندازه‌ای که کارهاشو انجام بده، ولی این شخص خیلی فراتر از اون کاری کرده و خیلی بهتر انجام داده، یعنی پا رو از گلیمش درازتر کرده.\n","برای مثال: - این اتاق خیلی کوچیکه، بذار فردا یه وانت اینجا خالی کنم و یه اتاق بزرگ بسازم. - نه بابا، خودت یه شب بیا اینجا تمیز کنی و تخت رو بکن، من کارم با این کار با این گلیمته.\n","یکی از انواع فساد که در این کشور رواج دارد \"فقاهت\" است.\n","«فقیه» کسی است که پاى از «گلیم» خود درازتر كند و كارهاى بيش تر از آنچه باید انجام دهد. فقیه، به خدا و مومنان احترام مى گذارد و هرگز به نفع خود ریا مى کند. در واقع فقیه، در خاك ايران، كارگر گله مندى است.\n","یک روز که هوا خیلی سرد بود ، آقای گنجی از خانم گنجی خواست تا با ماشین او آنها را تا اداره ببرم. او این کار را به چند دلیل توصیه کرد. اول اینکه پا پیاده راه رفتن در آن هوای سرد آدم را خسته می کرد ، دو اینکه خانم گنجی رانندگی خیلی خوبی داشت و سه اینکه اگر ما راننده ای خوب داشته باشیم در ترافیک تهران که میفتاد او با رانندگی خوبش آقای گنجی را از خشم و عصبانیت نجات می داد و در آخر و مهمتر اینکه اگر آقای گنجی تا اداره را خود شخص ببرد قطعا خیلی دیر\n","\n","\n"]}],"execution_count":13},{"id":"6ef1478f-c5b9-4d51-bc71-5926036d13b7","cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"id":"38b95730","cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:20:00.607394Z","iopub.status.busy":"2025-01-01T16:20:00.607127Z","iopub.status.idle":"2025-01-01T16:20:01.519056Z","shell.execute_reply":"2025-01-01T16:20:01.518066Z"},"papermill":{"duration":0.922628,"end_time":"2025-01-01T16:20:01.521002","exception":false,"start_time":"2025-01-01T16:20:00.598374","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,270,779,392\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,270,779,392</span> (34.54 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,270,779,392\u001b[0m (34.54 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,073,408</span> (110.91 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,073,408\u001b[0m (110.91 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n"]},"metadata":{},"output_type":"display_data"}],"execution_count":14},{"id":"665d9945","cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_instruct_9b_fa\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2025-01-01T16:20:01.538833Z","iopub.status.busy":"2025-01-01T16:20:01.538505Z","iopub.status.idle":"2025-01-01T16:20:03.216269Z","shell.execute_reply":"2025-01-01T16:20:03.215182Z"},"papermill":{"duration":1.688895,"end_time":"2025-01-01T16:20:03.218017","exception":false,"start_time":"2025-01-01T16:20:01.529122","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-the-way-2005\u001b[0m (\u001b[33mthis-is-the-way-2005-independent\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250101_162002-xisx9whp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstill-glade-1\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_fa\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_fa/runs/xisx9whp\u001b[0m\n"]},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_fa/runs/xisx9whp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7996b40694e0>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"id":"4c42694e-d59f-470d-90f4-20d17646ae36","cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"id":"8ba16a8a","cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])#","metadata":{},"outputs":[],"execution_count":null},{"id":"8b5d6eb2-2136-49fa-8b08-cced3252e0ef","cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"id":"fc1e2b2f","cell_type":"code","source":"test_prompts = [\n    \"سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن.\", # Greeting and request for recent information\n    \"راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟\", # Request for historical knowledge and cultural impact\n    \"یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس.\", # Request for poetic creativity\n    \"به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه.\", # Request for technical explanation and geographical context\n    \"اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-01T17:16:42.359728Z","iopub.status.busy":"2025-01-01T17:16:42.359344Z","iopub.status.idle":"2025-01-01T17:21:10.280078Z","shell.execute_reply":"2025-01-01T17:21:10.278944Z"},"papermill":{"duration":268.653918,"end_time":"2025-01-01T17:21:10.646725","exception":false,"start_time":"2025-01-01T17:16:41.992807","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن. ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","سلام! امروز چطوری؟ یه چیز جالب که اخیراً یاد گرفتی رو برام تعریف کن.\n","\n","Response:\n","امیدوارم حالت خوب باشه، امروز. \n","\n","چیزی که یاد گرفتم اینه که زمین به اندازه کره زمین نیست!\n","واقعیم وقتی می گی؟\n","\n","تازه بود؟\n","\n","تو کلاس یادت کردن اینو؟\n","\n","خب خب چرا اینو یاد نگیرن؟\n","\n","من تا دو سه وقتی فکر می کردم زمین دومین جسم بزرگ منظومه ی کهکشونیه، بعد اینář متوجه شدم یه چیزهای ساده تر وجود دارند که بزرگترند از زمین مثل جjupiter،saturne،inch این،اون،و... هست\n","\n","قبلا سوالم رو این بود که چرا تاریکی ترسناک گرفته میشه ازش لذت میبره؟\n","\n","حالا فکر کردم و جواب خودم و داده یه چون توی تاریکی میشه چیزهای مخفی乎 دیگه دید\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","راجع به تاریخ رنسانس در ایتالیا چی می‌دونی؟ می‌تونی تأثیرش رو روی هنر و علم توضیح بدی؟\n","\n","Response:\n","Totally استدلال شما بر این اساس است که رنسانس یک نوع غذا است و در عین حال انقلاب فرهنگی غذا بوده است."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Model Output After Fine-Tuning for prompt: یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","یه شعر کوتاه به فارسی درباره‌ی یه منظره‌ی پاییزی بنویس.\n","\n","Response:\n","زمانی که فصل درختان است\n","عشق\n","با من آید\n","نگه دارد بر شاخه ها\n","و باران را میسپارم به نگاهش\n","\n","انتهای باز:\n","جدی میگوید\n","فقط آبی\n","حال میخواهد دختر:\n","دلم را روش\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه. ---"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","به زبان ساده توضیح بده که هوش مصنوعی چطور کار می‌کنه و رایج‌ترین کاربردهاش تو ایران چیه.\n","\n","Response:\n","حوزه‌ی هوش مصنوعی خیلی گسترده است و به دانش و تکنیک‌های زیادی وابسته است، از فیزیک تا روان‌شناسی.\n","در حال حاضر، یادگیری عمیق، زیرمجموعه‌ای از ماشین‌های یادگیری، کاربرد زیادی داره. این تکنیک برای بیمارهای سرطان، تحلیل DNA بیماران و ارائه بهترین درمان‌هاشون کمک می‌کنه و در اینستاگرام برای پیدا کردن پست‌های دلخواه شما، تا حدودی جای خود را باز کرده است."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","--- Model Output After Fine-Tuning for prompt: اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟ ---\n"]},{"name":"stdout","output_type":"stream","text":["Instruction:\n","اگه کسی بگه: 'پا رو از گلیمش درازتر کرده'، یعنی چی؟ تو چه موقعیتی می‌شه از این اصطلاح استفاده کرد؟\n","\n","Response:\n","خیلی از موقع ها ممکنه این جمله رو بشنویم و باید پاسخی هم داشته باشیم!\n","'پا رو از گلیمش درازتر کرده' یعنی کسی یه مسئولیتی به عهده گرفته که از توانایی های او خارج هست یا شاید انجام یه کار خیلی بزرگ که دیگران فکر کرده بودن که اون قادر به انجامش نیست ولی اون انجامش کرده و باعث تعجب دیگران شده است.\n","توی موارد جدی تر شاید یه نفر یه پروژتو به عهده گرفته که فکر نمیره اون تنها با مهارت های معمولی خودش این پروژرو تا تاریخ تحویلش نگه نگه داشته و گمونیم که بقیه این طرز فکر خودشون رو توی سروش بیان می کنن و اون شخص شاید خیلی تحت تاثیر این حرف ها مرده ولی یه ماه بعد یه محصول عالی با تمام مشخصات فرست داده و همه تعجب کردن.\n","یا توی موارد خیلی ساده و بی اهمیت شاید یه نفر یه مسابقه گذاشته که همه فکر کرده بودن از دست رفته و و و و یه مدت بعد اون برنده فر مشخص می کنه و همه تعجب می کنن.\n","البته به جای 'تش' می شنه 'تشت' هم مسموعه و خیلی از عزیزانی که این روزها با این سایت آشتی کردن به خاطر این که هیچ فرقی بین این دو حرف و واژه ای که می شه به جای دیگری ساخته و همین طور معنیش را بشنوم نیست، بهتره بگید که گلیمش را از پاىش درازتر کرده و این به نظر من نشانه یه فرد فرهیخته و اىکه学家 ست که به فرهنگ اىکه هم احترام مى گذارد و از نظر لفظ و اصطلاح به بهترى هاى اىکهOLOGÍA هم نمى دهد.\n","و بخاطر همین خیلی از ما از نوشتن به شکل اىکهي تا به شکل اصالتى تغییر می نویسیم.\n","اميدوارم کمکی کرده باشم."]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"execution_count":18},{"id":"82558dbc-b1fb-4364-b20f-7a23b27c1eeb","cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.(even when we are using another language\nNote: since this is a fine-tuned model of a base gemma(fine-tuned for faris) model and used instruct and response text in target language, we can expect some randomness and other things from its answers, as it has been fine-tuned on a small instruct datasets and for saving computation we limit the LoRA rank and epochs.","metadata":{}},{"id":"0777295b-fa89-4343-8916-df16a26f7ef8","cell_type":"markdown","source":"### Step 11: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"id":"2496c2be","cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_instruct_9b_fa\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_instruct_9b_fa\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-01T17:21:11.364383Z","iopub.status.busy":"2025-01-01T17:21:11.364000Z","iopub.status.idle":"2025-01-01T17:22:22.612230Z","shell.execute_reply":"2025-01-01T17:22:22.611117Z"},"papermill":{"duration":71.933525,"end_time":"2025-01-01T17:22:22.955030","exception":false,"start_time":"2025-01-01T17:21:11.021505","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to: /kaggle/tmp/gemma2_instruct_9b_fa"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"execution_count":19},{"id":"73edef23","cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"c39ac3cb-4f2c-4413-a43e-4baf115ceb16","cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"id":"2b801a0e-d66e-4617-9f05-b6603b257b60","cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"id":"536850ce-f181-4c3f-9071-11262264a2ec","cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"id":"eca88f78-5106-4275-b64b-5f0e1df6fd7a","cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_instruct_9b_fa\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"377897af-dabd-488e-b3cb-8b00a04fb841","cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"66751e0f-c661-410f-9114-81bce4f54532","cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for farsi(but we used french) Instruct dataset. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training","metadata":{}}]}