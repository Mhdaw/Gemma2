{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":213805,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":182255,"modelId":163613}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":17690.149081,"end_time":"2025-01-03T22:27:48.352793","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-03T17:32:58.203712","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Instruct Fine-Tuning Gemma for Japanese Language\nThis notebook demonstrates the fine-tuning of the Gemma model on Japanese datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the Spanish dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n","metadata":{}},{"cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_instruct_9b_ja)","metadata":{}},{"cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used the fine-tuned version of the gemma2_9b_en which is fine-tuned on japanese data, The model [link](https://www.kaggle.com/models/mahdiseddigh/gemma2/keras/gemma2_9b_ja)","metadata":{}},{"cell_type":"markdown","source":"### My Gemma2 cookbook:\nI made this repo and I'm uploading all notebooks related to working with gemma models, check it out:\nhttps://github.com/Mhdaw/Gemma2","metadata":{}},{"cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-03T17:33:01.210991Z","iopub.status.busy":"2025-01-03T17:33:01.210788Z","iopub.status.idle":"2025-01-03T17:35:09.481902Z","shell.execute_reply":"2025-01-03T17:35:09.480717Z"},"papermill":{"duration":128.279179,"end_time":"2025-01-03T17:35:09.484083","exception":false,"start_time":"2025-01-03T17:33:01.204904","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:09.494449Z","iopub.status.busy":"2025-01-03T17:35:09.494198Z","iopub.status.idle":"2025-01-03T17:35:18.477290Z","shell.execute_reply":"2025-01-03T17:35:18.476050Z"},"papermill":{"duration":8.990109,"end_time":"2025-01-03T17:35:18.478613","exception":false,"start_time":"2025-01-03T17:35:09.488504","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"cell_type":"code","source":"import os\n# Set the environment variables for Kaggle and Weights & Biases.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"WANDB_API_KEY\"] = \"wand-api-key\" # or UserSecretsClient().get_secret(WANDB_API_KEY) or userdata.get(WANDB_API_KEY) or getpass.getpass(\"Enter your WANDB_API_KEY: \")\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:18.490808Z","iopub.status.busy":"2025-01-03T17:35:18.490453Z","iopub.status.idle":"2025-01-03T17:35:18.494850Z","shell.execute_reply":"2025-01-03T17:35:18.493821Z"},"papermill":{"duration":0.01275,"end_time":"2025-01-03T17:35:18.496176","exception":false,"start_time":"2025-01-03T17:35:18.483426","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:18.505531Z","iopub.status.busy":"2025-01-03T17:35:18.505307Z","iopub.status.idle":"2025-01-03T17:35:33.746009Z","shell.execute_reply":"2025-01-03T17:35:33.744905Z"},"papermill":{"duration":15.24789,"end_time":"2025-01-03T17:35:33.748133","exception":false,"start_time":"2025-01-03T17:35:18.500243","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Load and Explore Spanish Dataset\nWe are using the `CohereForAI/aya_dataset` dataset with japanese insturct data. \n\n**Subtasks:**\n- Load training and validation datasets.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"cell_type":"markdown","source":"Since we want to instruct fine-tune the Gemma 2 9b model for adapting to the Japanese language, we need a good amount of high-quality Japanese instruct and responses. For that, we use the 'Japanese' dataset, which is a multilingual instruct dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/CohereForAI/aya_dataset)  \n\n**Dataset Summary (from the original dataset page):**  \nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\n\nCurated by: Contributors of Aya Open Science Intiative.\n\nLanguage(s): 65 languages (71 including dialects & scripts).\n\nLicense: Apache 2.0","metadata":{}},{"cell_type":"code","source":"def load_specific_data(target_language=\"English\"):\n    aya_dataset = load_dataset(\"CohereForAI/aya_dataset\")\n    selected_dataset = aya_dataset.filter(lambda x: x['language'] == target_language)\n    return selected_dataset\n\ndef generate_text_data(selected_dataset):\n    Data = []\n    for example in selected_dataset[\"train\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        Data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n\n    test_data = []\n    for example in selected_dataset[\"test\"]:\n        instruction = example[\"inputs\"]\n        response = example[\"targets\"]\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        test_data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n    return Data, test_data","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:33.758413Z","iopub.status.busy":"2025-01-03T17:35:33.758174Z","iopub.status.idle":"2025-01-03T17:35:33.763815Z","shell.execute_reply":"2025-01-03T17:35:33.762894Z"},"papermill":{"duration":0.012285,"end_time":"2025-01-03T17:35:33.765128","exception":false,"start_time":"2025-01-03T17:35:33.752843","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset = load_specific_data(\"Japanese\")\ntrain_text_data, test_text_data= generate_text_data(subset)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Since there is no example for our target language in the test subset, we take a small fraction of train data:\ntrain_text_data, test_text_data = train_text_data[:-200], train_text_data[-20:]","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:42.874421Z","iopub.status.busy":"2025-01-03T17:35:42.874183Z","iopub.status.idle":"2025-01-03T17:35:42.877523Z","shell.execute_reply":"2025-01-03T17:35:42.876753Z"},"papermill":{"duration":0.011915,"end_time":"2025-01-03T17:35:42.878936","exception":false,"start_time":"2025-01-03T17:35:42.867021","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the first example to ensure loading is correct\nprint(\"First training example:\", train_text_data[0],\"\\n\")\nprint(\"First validation example:\", test_text_data[0])\nprint(f'\\ntraining length:{len(train_text_data)}')","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:42.891838Z","iopub.status.busy":"2025-01-03T17:35:42.891601Z","iopub.status.idle":"2025-01-03T17:35:42.895834Z","shell.execute_reply":"2025-01-03T17:35:42.894795Z"},"papermill":{"duration":0.012335,"end_time":"2025-01-03T17:35:42.897157","exception":false,"start_time":"2025-01-03T17:35:42.884822","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(train_text_data)\nval_data = tf.data.Dataset.from_tensor_slices(test_text_data)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:42.910031Z","iopub.status.busy":"2025-01-03T17:35:42.909812Z","iopub.status.idle":"2025-01-03T17:35:43.029747Z","shell.execute_reply":"2025-01-03T17:35:43.028816Z"},"papermill":{"duration":0.128622,"end_time":"2025-01-03T17:35:43.031642","exception":false,"start_time":"2025-01-03T17:35:42.903020","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training and Loading the model\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n- Then we load the model in parallel devices.\n","metadata":{}},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_9b_ja/1\" # Or /kaggle/input/m/keras/gemma2/keras/gemma2_instruct_2b_en/2\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:35:43.045512Z","iopub.status.busy":"2025-01-03T17:35:43.045292Z","iopub.status.idle":"2025-01-03T17:39:10.896079Z","shell.execute_reply":"2025-01-03T17:39:10.895273Z"},"papermill":{"duration":207.860065,"end_time":"2025-01-03T17:39:10.898077","exception":false,"start_time":"2025-01-03T17:35:43.038012","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model for fine-tuning and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:39:10.933648Z","iopub.status.busy":"2025-01-03T17:39:10.933395Z","iopub.status.idle":"2025-01-03T17:39:10.938517Z","shell.execute_reply":"2025-01-03T17:39:10.937137Z"},"papermill":{"duration":0.013767,"end_time":"2025-01-03T17:39:10.939434","exception":false,"start_time":"2025-01-03T17:39:10.925667","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample prompt to check performance before and after fine-tuning\ntest_prompts = [\n    \"こんにちは！今日は कैसाですか？最近学んだ面白いことを教えてください。\", # Greeting and request for recent information\n    \"イタリアのルネサンスの歴史について何を知っていますか？芸術と科学への影響を説明してもらえますか？\", # Request for historical knowledge and cultural impact\n    \"秋の風景についての短い詩を日本語で書いてください。\", # Request for poetic creativity\n    \"人工知能がどのように機能するのか、そして日本で最も一般的な用途は何なのかを簡単な言葉で説明してください。\", # Request for technical explanation and geographical context\n    \"もし誰かが「手に余る」と言ったら、それは何を意味するでしょうか？どのような状況でこの表現を使うことができるでしょうか？\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output Before Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:39:10.955154Z","iopub.status.busy":"2025-01-03T17:39:10.954884Z","iopub.status.idle":"2025-01-03T17:43:25.914202Z","shell.execute_reply":"2025-01-03T17:43:25.912741Z"},"papermill":{"duration":254.975501,"end_time":"2025-01-03T17:43:25.921949","exception":false,"start_time":"2025-01-03T17:39:10.946448","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:43:25.937974Z","iopub.status.busy":"2025-01-03T17:43:25.937632Z","iopub.status.idle":"2025-01-03T17:43:26.886186Z","shell.execute_reply":"2025-01-03T17:43:26.885240Z"},"papermill":{"duration":0.959048,"end_time":"2025-01-03T17:43:26.888071","exception":false,"start_time":"2025-01-03T17:43:25.929023","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 20\n)\n\nwandb.init(project = \"fine-tuning-gemma2_instruct_9b_ja\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2025-01-03T17:43:26.906087Z","iopub.status.busy":"2025-01-03T17:43:26.905835Z","iopub.status.idle":"2025-01-03T17:43:28.452066Z","shell.execute_reply":"2025-01-03T17:43:28.450919Z"},"papermill":{"duration":1.557246,"end_time":"2025-01-03T17:43:28.453641","exception":false,"start_time":"2025-01-03T17:43:26.896395","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=20, callbacks=[WandbMetricsLogger()])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"cell_type":"code","source":"test_prompts = [\n    \"こんにちは！今日は कैसाですか？最近学んだ面白いことを教えてください。\", # Greeting and request for recent information\n    \"イタリアのルネサンスの歴史について何を知っていますか？芸術と科学への影響を説明してもらえますか？\", # Request for historical knowledge and cultural impact\n    \"秋の風景についての短い詩を日本語で書いてください。\", # Request for poetic creativity\n    \"人工知能がどのように機能するのか、そして日本で最も一般的な用途は何なのかを簡単な言葉で説明してください。\", # Request for technical explanation and geographical context\n    \"もし誰かが「手に余る」と言ったら、それは何を意味するでしょうか？どのような状況でこの表現を使うことができるでしょうか？\", # Request for interpretation of an idiomatic expression\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n--- Model Output After Fine-Tuning for prompt: {prompt} ---\")\n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T22:11:54.572011Z","iopub.status.busy":"2025-01-03T22:11:54.571664Z","iopub.status.idle":"2025-01-03T22:16:42.256763Z","shell.execute_reply":"2025-01-03T22:16:42.255673Z"},"papermill":{"duration":291.445598,"end_time":"2025-01-03T22:16:44.077078","exception":false,"start_time":"2025-01-03T22:11:52.631480","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our target language.\nNote: since this is a fine-tuned model of a base gemma(fine-tuned for Japanese) model and used instruct and response text in target language, we can expect some randomness and other things from its answers, as it has been fine-tuned on a small instruct datasets and for saving computation we limit the LoRA rank and epochs.","metadata":{}},{"cell_type":"markdown","source":"### Step 11: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_instruct_9b_ja\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_instruct_9b_ja\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T22:16:47.803012Z","iopub.status.busy":"2025-01-03T22:16:47.802660Z","iopub.status.idle":"2025-01-03T22:18:01.191449Z","shell.execute_reply":"2025-01-03T22:18:01.190176Z"},"papermill":{"duration":77.166399,"end_time":"2025-01-03T22:18:03.039163","exception":false,"start_time":"2025-01-03T22:16:45.872764","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_instruct_9b_ja\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the Gemma model for Japanese Instruct dataset. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training","metadata":{}}]}