{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":219360,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":187069,"modelId":163613}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":13595.8954,"end_time":"2025-01-05T22:37:19.085643","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-05T18:50:43.190243","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Instruct Fine-Tuning Gemma for 14 Languages\nThis notebook demonstrates the fine-tuning of the Gemma model on 14 datasets. We will explore the workflow from data loading and preprocessing to model fine-tuning and evaluation.\n\n**Key Steps:**\n1. Setup environment variables for Kaggle and Weights & Biases (wandb).\n2. Load and preprocess the 14 Instruct dataset.\n3. Set up model parallelism for TPU utilization.\n4. Fine-tune the Gemma model using LoRA (Low-Rank Adaptation).\n5. Evaluate model performance before and after fine-tuning.\n\nthe used languages:\n\n`Spanish`,`Iranian Persian`,`Japanese`,`Korean`,`Russian`,`German`,`Swedish`,\n                     `Simplified Chinese`, `Danish`, `English(american and british)`, `Finnish`,`Italian`, `Dutch`,\n                     `Turkish`","metadata":{}},{"cell_type":"markdown","source":"##### you can look into the fine-tuning process logs in here: [link](https://wandb.ai/this-is-the-way-2005-independent/fine-tuning-gemma2_2b_instruct_Polyglot)","metadata":{}},{"cell_type":"markdown","source":"#### Device:\nwe used the TPU VM v3-8 from kaggle.\n#### Base model:\nwe used the fine-tuned version of the gemma2_2b_en which is fine-tuned on 54 datasets(multilingual).                                                \nThe model [link](https://www.kaggle.com/models/mahdiseddigh/gemma2/keras/gemma2_2b_polyglot)\n\nThe fine-tuning notebook: [link](https://www.kaggle.com/code/mahdiseddigh/fine-tuning-gemma2-2b-polyglot)","metadata":{}},{"cell_type":"markdown","source":"### My Gemma2 cookbook:\nI made this repo and I'm uploading all notebooks related to working with gemma models, check it out:\nhttps://github.com/Mhdaw/Gemma2","metadata":{}},{"cell_type":"markdown","source":"### Step 0: Installing the Required Libraries and Frameworks\nTo ensure that all necessary libraries and frameworks are installed, run the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp keras datasets kagglehub keras_hub \n!pip install -q -U tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu\n!pip install -q -U wandb","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:50:46.244108Z","iopub.status.busy":"2025-01-05T18:50:46.243857Z","iopub.status.idle":"2025-01-05T18:52:38.678728Z","shell.execute_reply":"2025-01-05T18:52:38.677558Z"},"papermill":{"duration":112.442744,"end_time":"2025-01-05T18:52:38.680592","exception":false,"start_time":"2025-01-05T18:50:46.237848","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:52:38.690382Z","iopub.status.busy":"2025-01-05T18:52:38.690108Z","iopub.status.idle":"2025-01-05T18:52:47.932434Z","shell.execute_reply":"2025-01-05T18:52:47.931242Z"},"papermill":{"duration":9.249262,"end_time":"2025-01-05T18:52:47.934087","exception":false,"start_time":"2025-01-05T18:52:38.684825","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Setup Environment Variables\nWe will configure the environment variables required for:\n- Kaggle API access\n- Weights & Biases for tracking experiments\n- TensorFlow backend optimization.\n","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KAGGLE_USERNAME\"] = \"your-kaggle-username\"\nos.environ[\"KAGGLE_KEY\"] = \"your-kaggle-key\"\nos.environ[\"WANDB_API_KEY\"] = \"your-wandb-key\"\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:52:47.946838Z","iopub.status.busy":"2025-01-05T18:52:47.946512Z","iopub.status.idle":"2025-01-05T18:52:47.950882Z","shell.execute_reply":"2025-01-05T18:52:47.949822Z"},"papermill":{"duration":0.013296,"end_time":"2025-01-05T18:52:47.952410","exception":false,"start_time":"2025-01-05T18:52:47.939114","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(f\"num cpus:{os.cpu_count()}\") ","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:52:47.962154Z","iopub.status.busy":"2025-01-05T18:52:47.961914Z","iopub.status.idle":"2025-01-05T18:52:47.965035Z","shell.execute_reply":"2025-01-05T18:52:47.964211Z"},"papermill":{"duration":0.009586,"end_time":"2025-01-05T18:52:47.966295","exception":false,"start_time":"2025-01-05T18:52:47.956709","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport keras_nlp\nfrom datasets import load_dataset\nimport itertools\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:52:47.975761Z","iopub.status.busy":"2025-01-05T18:52:47.975540Z","iopub.status.idle":"2025-01-05T18:53:03.471762Z","shell.execute_reply":"2025-01-05T18:53:03.470778Z"},"papermill":{"duration":15.50364,"end_time":"2025-01-05T18:53:03.473898","exception":false,"start_time":"2025-01-05T18:52:47.970258","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Load and Explore Korean Dataset\nWe are using the `CohereForAI/aya_dataset` dataset. \n\n**Subtasks:**\n- Load training and validation datasets for each language and then concatenante them into a general dataset.\n- Extract sample data for exploration.\n- Limit dataset size for efficient experimentation.\n","metadata":{}},{"cell_type":"markdown","source":"Since we want to instruct fine-tune the Gemma 2 2b model for adapting to the 14 languages, we need a good amount of high-quality multilingual instruct and responses. For that, we use the 'aya_dataset' dataset, which is a multilingual instruct dataset.\n\nYou can look into it on Hugging Face: [Link](https://huggingface.co/datasets/CohereForAI/aya_dataset)  \n\n**Dataset Summary (from the original dataset page):**  \nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\n\nCurated by: Contributors of Aya Open Science Intiative.\n\nLanguage(s): 65 languages (71 including dialects & scripts).\n\nLicense: Apache 2.0","metadata":{}},{"cell_type":"code","source":"def load_and_process_aya(languages, max_examples=None):\n    \"\"\"Loads and processes the AYA dataset for multiple languages.\n\n    Args:\n        languages: A list of target languages.\n        max_examples: Maximum number of examples to load per language. If None, loads all.\n\n    Returns:\n        A dictionary where keys are languages and values are dictionaries\n        containing 'train' and 'test' lists of processed text data.\n        Returns an empty dictionary if there are errors.\n    \"\"\"\n\n    try:\n        aya_dataset = load_dataset(\"CohereForAI/aya_dataset\")\n    except Exception as e:\n        print(f\"Error loading AYA dataset: {e}\")\n        return {}\n\n    all_lang_data = {}\n\n    for lang in languages:\n        try:\n            print(f\"Processing {lang} data...\")\n            selected_dataset = aya_dataset.filter(lambda x: x['language'] == lang)\n\n            Data = []\n            for example in selected_dataset[\"train\"]:\n                instruction = example[\"inputs\"]\n                response = example[\"targets\"]\n                template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n                Data.append(template.format(**{\"instruction\": instruction, \"response\": response}))\n\n            if max_examples is not None:\n                Data = Data[:max_examples]\n\n            test_data = Data[-min(75, len(Data)):]  # Handle cases with fewer than 75 examples\n            train_data = Data[:-min(75, len(Data))]\n            if len(train_data)>0:\n                all_lang_data[lang] = {\"train\": train_data, \"test\": test_data}\n                print(f\"Loaded {len(train_data)} training examples for {lang}\")\n            else:\n                print(f\"Not enough data to create train/test split for {lang}\")\n\n        except Exception as e:\n            print(f\"Error processing {lang} data: {e}\")\n            continue # Skip to the next language\n\n    return all_lang_data","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:53:03.484528Z","iopub.status.busy":"2025-01-05T18:53:03.484241Z","iopub.status.idle":"2025-01-05T18:53:03.492854Z","shell.execute_reply":"2025-01-05T18:53:03.491945Z"},"papermill":{"duration":0.016014,"end_time":"2025-01-05T18:53:03.494290","exception":false,"start_time":"2025-01-05T18:53:03.478276","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"languages_to_load = [\"Spanish\",\"Iranian Persian\",\"Japanese\",\"Korean\",\"Russian\",\"German\",\"Swedish\",\n                     \"Simplified Chinese\", \"Danish\", \"English\", \"Finnish\",\"Italian\", \"Dutch\",\n                     \"Turkish\"]\n# we use 14 languages and 1000 train example for each, Note some languages have less than 1000 exmaples.\nloaded_data = load_and_process_aya(languages_to_load, max_examples=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_data_train = []\nfull_data_val = []\nfor lang, data in loaded_data.items():\n    full_data_train.extend(data[\"train\"])\n    full_data_val.extend(data[\"test\"])\n  \nprint(f\"Total train examples: {len(full_data_train)}\")\nprint(f\"Total test examples: {len(full_data_val)}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:53:45.340497Z","iopub.status.busy":"2025-01-05T18:53:45.340249Z","iopub.status.idle":"2025-01-05T18:53:45.344972Z","shell.execute_reply":"2025-01-05T18:53:45.344025Z"},"papermill":{"duration":0.030721,"end_time":"2025-01-05T18:53:45.346572","exception":false,"start_time":"2025-01-05T18:53:45.315851","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Data Preprocessing\nThe text data will be converted into TensorFlow datasets for training and validation. Key preprocessing steps include:\n- Creating TensorFlow datasets from plain-text lists.\n- Shuffling and batching training data for optimized input.\n- Optional text cleaning (if needed).\n","metadata":{}},{"cell_type":"code","source":"batch_size = 4\n\n# Convert the lists of text data to TensorFlow datasets\ntrain_data = tf.data.Dataset.from_tensor_slices(full_data_train)\nval_data = tf.data.Dataset.from_tensor_slices(full_data_val)\n\n# Preprocess each text sample\ndef preprocess_text(text):\n    return tf.convert_to_tensor(text, dtype=tf.string)\n\n# Apply preprocessing (optional if text is already clean)\ntrain_data = train_data.map(preprocess_text)\nval_data = val_data.map(preprocess_text)\n\n# Shuffle and batch the training data\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size)\nval_data = val_data.batch(batch_size)","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:53:45.393312Z","iopub.status.busy":"2025-01-05T18:53:45.393089Z","iopub.status.idle":"2025-01-05T18:53:45.571470Z","shell.execute_reply":"2025-01-05T18:53:45.570604Z"},"papermill":{"duration":0.203914,"end_time":"2025-01-05T18:53:45.573269","exception":false,"start_time":"2025-01-05T18:53:45.369355","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Model Parallelism for Efficient Training and Loading the model\nWe configure model parallelism using TPUs to handle the large-scale Gemma model. Key components:\n- **Device Mesh:** A mapping of TPU devices.\n- **Layout Map:** Specifies the sharding strategy for different layers.\n- Then we load the model in parallel devices.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Model Overview\nWe initialize the Gemma model for fine-tuning and explore its architecture.\n\n### Key Model Parameters:\n- **Model ID:** Pretrained Gemma version for transfer learning.\n- **LoRA:** Enable Low-Rank Adaptation for fine-tuning.\n- **Sequence Length:** Adjusted for task requirements.\n","metadata":{}},{"cell_type":"markdown","source":"**Note: the device mesh for 9b and 2b model is different! use accordingly.**","metadata":{}},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)\n\nmodel_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Assuming the second dimension (2304) of the attention kernels is divisible by 8,\n# we shard along that dimension:\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)  # Shard embeddings along model dimension\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (None, model_dim, None) # Shard attention kernels along second dimension\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (None, model_dim, None)  # Shard attention output kernels along second dimension\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)  # Shard FFN gating kernels along second dimension\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None) # Shard FFN linear kernels along first dimension\n\nmodel_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\nmodel_id = \"/kaggle/input/gemma2/keras/gemma2_2b_polyglot/1\" # change this if you want\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:53:45.620696Z","iopub.status.busy":"2025-01-05T18:53:45.620477Z","iopub.status.idle":"2025-01-05T18:54:56.740176Z","shell.execute_reply":"2025-01-05T18:54:56.738756Z"},"papermill":{"duration":71.156086,"end_time":"2025-01-05T18:54:56.752341","exception":false,"start_time":"2025-01-05T18:53:45.596255","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n\ndef generate_text(prompt, model):\n    \"\"\"\n    Generate text from the model based on a given prompt.\n    \"\"\"\n    sampler = keras_nlp.samplers.TopKSampler(k=15, seed=2)\n    model.compile(sampler=sampler)\n    output = model.generate(prompt, max_length=512)\n    return output","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:54:56.857318Z","iopub.status.busy":"2025-01-05T18:54:56.857045Z","iopub.status.idle":"2025-01-05T18:54:56.861551Z","shell.execute_reply":"2025-01-05T18:54:56.860511Z"},"papermill":{"duration":0.031981,"end_time":"2025-01-05T18:54:56.863020","exception":false,"start_time":"2025-01-05T18:54:56.831039","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Evaluate Model Performance Before Fine-Tuning\nBefore training, test the model on a set of prompts to benchmark its initial performance. This helps us compare improvements after fine-tuning.\n","metadata":{}},{"cell_type":"code","source":"test_prompts_multilingual = [\n    # English (American)\n    \"What's a fun fact about the Grand Canyon?\",  # US English\n    \"Write a short story about a talking dog.\", # US English\n    # Arabic (Modern Standard)\n    \"ما هي أقدم مدينة في العالم؟\", # What is the oldest city in the world?\n    \"اكتب جملة عن أهمية القراءة.\", # Write a sentence about the importance of reading.\n    # Chinese (Simplified)\n    \"长城有多长？\", # How long is the Great Wall?\n    \"写一个关于猫的短篇故事。\", # Write a short story about a cat.\n    # Dutch\n    \"Wat is de hoofdstad van Nederland?\", # What is the capital of the Netherlands?\n    \"Schrijf een korte beschrijving van een molen.\", # Write a short description of a windmill.\n    # French (European)\n    \"Quelle est la capitale de la France ?\", # What is the capital of France?\n    \"Écris une courte description de la Tour Eiffel.\", # Write a short description of the Eiffel Tower.\n    # German\n    \"Was ist die Hauptstadt von Deutschland?\", # What is the capital of Germany?\n    \"Schreibe eine kurze Beschreibung des Brandenburger Tors.\", # Write a short description of the Brandenburg Gate.\n    # Italian\n    \"Qual è la capitale d'Italia?\", # What is the capital of Italy?\n    \"Scrivi una breve descrizione del Colosseo.\", # Write a short description of the Colosseum.\n    # Japanese\n    \"日本の首都はどこですか？\", # What is the capital of Japan?\n    \"桜について短い文章を書いてください。\", # Please write a short sentence about cherry blossoms.\n    # Korean\n    \"한국의 수도는 어디입니까?\", # What is the capital of Korea?\n    \"한국 음식에 대해 간단히 설명해 주세요.\", # Please briefly explain about Korean food.\n    # Polish\n    \"Jaka jest stolica Polski?\", # What is the capital of Poland?\n    \"Napisz krótkie opowiadanie o smoku.\", # Write a short story about a dragon.\n    # Portuguese (Brazilian)\n    \"Qual é a capital do Brasil?\", # What is the capital of Brazil?\n    \"Escreva uma breve descrição do Cristo Redentor.\", # Write a short description of Christ the Redeemer.\n    # Russian\n    \"Какая столица России?\", # What is the capital of Russia?\n    \"Напишите короткий рассказ о медведе.\", # Write a short story about a bear.\n    # Spanish (European)\n    \"¿Cuál es la capital de España?\", # What is the capital of Spain?\n    \"Escribe una breve descripción de la Sagrada Familia.\", # Write a short description of the Sagrada Familia.\n    # Thai\n    \"ประเทศไทยมีเมืองหลวงชื่ออะไร\", # What is the capital of Thailand?\n    \"เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\", # Write a short sentence about Thai temples.\n    # Turkish\n    \"Türkiye'nin başkenti neresidir?\", # What is the capital of Turkey?\n    \"Kapadokya hakkında kısa bir açıklama yazın.\", # Write a short description about Cappadocia.\n    # Ukrainian\n    \"Яка столиця України?\", # What is the capital of Ukraine?\n    \"Напишіть коротку розповідь про кота.\", # Write a short story about a cat.\n    #Vietnamese\n    \"Thủ đô của Việt Nam là gì?\", # What is the capital of Vietnam?\n    \"Hãy viết một câu ngắn về Vịnh Hạ Long.\", # Please write a short sentence about Ha Long Bay.\n    #Persian\n    \"پایتخت ایران کجاست؟\", #Where is the capital of Iran?\n    \"یک جمله کوتاه درباره حافظ بنویسید\", #Write a short sentence about Hafez\n]\n\nfor prompt in test_prompts_multilingual:\n    print(f\"\\n--- Model Output Before Fine-tuning for prompt: {prompt} ---\") \n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T18:54:56.912171Z","iopub.status.busy":"2025-01-05T18:54:56.911887Z","iopub.status.idle":"2025-01-05T19:17:44.796783Z","shell.execute_reply":"2025-01-05T19:17:44.795685Z"},"papermill":{"duration":1367.938414,"end_time":"2025-01-05T19:17:44.825081","exception":false,"start_time":"2025-01-05T18:54:56.886667","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Fine-Tuning the Gemma Model with LoRA\nWe apply LoRA to enable efficient parameter updates during fine-tuning. Key configurations include:\n- Optimizer: AdamW with weight decay for transformer models.\n- Metrics: Sparse Categorical Accuracy.\n- LoRA Rank: Defines the dimensionality of updates.\n\nWe use Weights & Biases to monitor training progress and metrics.\n","metadata":{}},{"cell_type":"code","source":"LoRA_rank = 8 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2025-01-05T19:17:44.879492Z","iopub.status.busy":"2025-01-05T19:17:44.879219Z","iopub.status.idle":"2025-01-05T19:17:45.659472Z","shell.execute_reply":"2025-01-05T19:17:45.658573Z"},"papermill":{"duration":0.809494,"end_time":"2025-01-05T19:17:45.660844","exception":false,"start_time":"2025-01-05T19:17:44.851350","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.02,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nconfigs = dict(\n    shuffle_buffer = 1000,\n    batch_size = 4,\n    learning_rate = 5e-5,\n    weight_decay = 0.02,\n    sequence_length = 512,\n    epochs = 16\n)\n\nwandb.init(project = \"fine-tuning-gemma2_2b_instruct_Polyglot\",\n    config=configs\n)","metadata":{"execution":{"iopub.execute_input":"2025-01-05T19:17:45.717627Z","iopub.status.busy":"2025-01-05T19:17:45.717367Z","iopub.status.idle":"2025-01-05T19:17:47.279970Z","shell.execute_reply":"2025-01-05T19:17:47.278815Z"},"papermill":{"duration":1.593543,"end_time":"2025-01-05T19:17:47.281959","exception":false,"start_time":"2025-01-05T19:17:45.688416","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 8: Training the gemma model:\nwe train the gemma language model on our ```train_data``` and evaluate it on our ```val_data```, to save time and computation lets use small epochs like 20, If you have more time and computation available, go ahead and increase this!","metadata":{}},{"cell_type":"code","source":"# Fit the model\nhistory = gemma_lm.fit(train_data, validation_data=val_data, epochs=16, verbose=0, callbacks=[WandbMetricsLogger()])\nprint(\"Training finished....\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T19:17:47.342596Z","iopub.status.busy":"2025-01-05T19:17:47.342214Z","iopub.status.idle":"2025-01-05T22:09:10.676817Z","shell.execute_reply":"2025-01-05T22:09:10.675223Z"},"papermill":{"duration":10283.398842,"end_time":"2025-01-05T22:09:10.710312","exception":false,"start_time":"2025-01-05T19:17:47.311470","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Evaluate Model Performance After Fine-Tuning\nFinally, evaluate the fine-tuned model using the same prompts as earlier. Compare the responses to assess improvements in quality and relevance.\n","metadata":{}},{"cell_type":"code","source":"test_prompts_multilingual = [\n    # English (American)\n    \"What's a fun fact about the Grand Canyon?\",  # US English\n    \"Write a short story about a talking dog.\", # US English\n    # Arabic (Modern Standard)\n    \"ما هي أقدم مدينة في العالم؟\", # What is the oldest city in the world?\n    \"اكتب جملة عن أهمية القراءة.\", # Write a sentence about the importance of reading.\n    # Chinese (Simplified)\n    \"长城有多长？\", # How long is the Great Wall?\n    \"写一个关于猫的短篇故事。\", # Write a short story about a cat.\n    # Dutch\n    \"Wat is de hoofdstad van Nederland?\", # What is the capital of the Netherlands?\n    \"Schrijf een korte beschrijving van een molen.\", # Write a short description of a windmill.\n    # French (European)\n    \"Quelle est la capitale de la France ?\", # What is the capital of France?\n    \"Écris une courte description de la Tour Eiffel.\", # Write a short description of the Eiffel Tower.\n    # German\n    \"Was ist die Hauptstadt von Deutschland?\", # What is the capital of Germany?\n    \"Schreibe eine kurze Beschreibung des Brandenburger Tors.\", # Write a short description of the Brandenburg Gate.\n    # Italian\n    \"Qual è la capitale d'Italia?\", # What is the capital of Italy?\n    \"Scrivi una breve descrizione del Colosseo.\", # Write a short description of the Colosseum.\n    # Japanese\n    \"日本の首都はどこですか？\", # What is the capital of Japan?\n    \"桜について短い文章を書いてください。\", # Please write a short sentence about cherry blossoms.\n    # Korean\n    \"한국의 수도는 어디입니까?\", # What is the capital of Korea?\n    \"한국 음식에 대해 간단히 설명해 주세요.\", # Please briefly explain about Korean food.\n    # Polish\n    \"Jaka jest stolica Polski?\", # What is the capital of Poland?\n    \"Napisz krótkie opowiadanie o smoku.\", # Write a short story about a dragon.\n    # Portuguese (Brazilian)\n    \"Qual é a capital do Brasil?\", # What is the capital of Brazil?\n    \"Escreva uma breve descrição do Cristo Redentor.\", # Write a short description of Christ the Redeemer.\n    # Russian\n    \"Какая столица России?\", # What is the capital of Russia?\n    \"Напишите короткий рассказ о медведе.\", # Write a short story about a bear.\n    # Spanish (European)\n    \"¿Cuál es la capital de España?\", # What is the capital of Spain?\n    \"Escribe una breve descripción de la Sagrada Familia.\", # Write a short description of the Sagrada Familia.\n    # Thai\n    \"ประเทศไทยมีเมืองหลวงชื่ออะไร\", # What is the capital of Thailand?\n    \"เขียนประโยคสั้นๆ เกี่ยวกับวัดไทย\", # Write a short sentence about Thai temples.\n    # Turkish\n    \"Türkiye'nin başkenti neresidir?\", # What is the capital of Turkey?\n    \"Kapadokya hakkında kısa bir açıklama yazın.\", # Write a short description about Cappadocia.\n    # Ukrainian\n    \"Яка столиця України?\", # What is the capital of Ukraine?\n    \"Напишіть коротку розповідь про кота.\", # Write a short story about a cat.\n    #Vietnamese\n    \"Thủ đô của Việt Nam là gì?\", # What is the capital of Vietnam?\n    \"Hãy viết một câu ngắn về Vịnh Hạ Long.\", # Please write a short sentence about Ha Long Bay.\n    #Persian\n    \"پایتخت ایران کجاست؟\", #Where is the capital of Iran?\n    \"یک جمله کوتاه درباره حافظ بنویسید\", #Write a short sentence about Hafez the iranian poet \n]\n\nfor prompt in test_prompts_multilingual:\n    print(f\"\\n--- Model Output After Fine-tuning for prompt: {prompt} ---\") \n    print(generate_text(template.format(instruction=prompt, response=\"\"), gemma_lm))\n    print(\"\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T22:09:11.260808Z","iopub.status.busy":"2025-01-05T22:09:11.260549Z","iopub.status.idle":"2025-01-05T22:34:22.103558Z","shell.execute_reply":"2025-01-05T22:34:22.101737Z"},"papermill":{"duration":1510.909733,"end_time":"2025-01-05T22:34:22.140313","exception":false,"start_time":"2025-01-05T22:09:11.230580","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### If you look into our examples and compare it, you can see the models generation has improved for our languages, Some more than others.\nNote: since this is a fine-tuned model of a base gemma(fine-tuned for 54 languages) model and used instruct and response text in target language, we can expect some randomness and other things from its answers, as it has been fine-tuned on a small instruct datasets(less than 10000 examples) and for saving computation we limit the LoRA rank and epochs.\n","metadata":{}},{"cell_type":"markdown","source":"### Step 10: Uploading the fine-tuned model to kaggle:\nHere we upload the final fine-tuned model to kaggle models so every one can use it!.\nwe use /kaggle/tmp to save the model, as the model size is larger than kaggle notebooks output directory size.","metadata":{}},{"cell_type":"code","source":"tmp_model_dir = \"/kaggle/tmp/gemma2_2b_instruct_Polyglot\"  # Use /kaggle/tmp\npreset_dir = \"gemma2_2b_instruct_Polyglot\"\nos.makedirs(tmp_model_dir, exist_ok=True)\ngemma_lm.save_to_preset(tmp_model_dir)\n\nprint(f\"Model saved to: {tmp_model_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-01-05T22:34:22.220364Z","iopub.status.busy":"2025-01-05T22:34:22.220041Z","iopub.status.idle":"2025-01-05T22:34:44.252942Z","shell.execute_reply":"2025-01-05T22:34:44.251394Z"},"papermill":{"duration":22.075748,"end_time":"2025-01-05T22:34:44.254350","exception":false,"start_time":"2025-01-05T22:34:22.178602","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport keras_hub\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nkaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\nkeras_hub.upload_preset(kaggle_uri, tmp_model_dir)\nprint(\"Done!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\nHere we talk about how we can load the fine-tuned model from kaggle and use it:","metadata":{}},{"cell_type":"markdown","source":"**For inference we just need to load the fine-tuned model from kaggle to our notebook in the following way:**\n\nfor more info check out [here](https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/)\n\nspecificly:\n\nA preset is a directory of configs, weights and other file assets used to save and load a pre-trained model. The preset can be passed as one of:\n* 1. \na built-in preset identifier like 'bert_base_e\n* 2. '\na Kaggle Models handle like 'kaggle://user/bert/keras/bert_base_\n* 3. n'\na Hugging Face handle like 'hf://user/bert_base\n* 4. en'\na path to a local preset directory like './bert_base_en'","metadata":{}},{"cell_type":"markdown","source":"**Infrence step by step:**\n* 1. Load the fine-tuned model from kaggle models\n* 2. After the model is succesfuly loaded, You can use it to generate text in the targeted language\n* Good luck:)","metadata":{}},{"cell_type":"code","source":"final_model_id = \"kaggle://mahdiseddigh/gemma2/keras/gemma2_2b_instruct_Polyglot\"\nfinetuned_gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(final_model_id)\nfinetuned_gemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_prompt = #define your prompt...\nprint(\"\\n--- Fine-tuned Models Output ---\")\nprint(generate_text(template.format(instruction=test_prompt, response=\"\"), finetuned_gemma_lm))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nThis notebook showcased the complete workflow for fine-tuning the (fine-tuned)Gemma model for 14 Instruct datasets. We highlighted:\n- Dataset preparation\n- Model architecture and parallelism\n- Fine-tuning with LoRA\n- Performance evaluation pre- and post-training","metadata":{}}]}