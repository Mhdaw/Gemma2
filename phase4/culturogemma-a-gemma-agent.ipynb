{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"793c8b9a0fce491ea6d171d4eea6fcfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_923a03fd4e2e4c7f86679ec7d14b4f03","IPY_MODEL_952da06b5d0b434fb472cf6b24d35fa2","IPY_MODEL_a66a95e27caf417ebfdb0a8a21c6ff59"],"layout":"IPY_MODEL_692ee8e5e9cf433f95d2654f7e04564c"}},"923a03fd4e2e4c7f86679ec7d14b4f03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_823653d2b7e84bc591230ad758bd2885","placeholder":"​","style":"IPY_MODEL_7b114ff533004424a3089dcd7ec27489","value":"Loading checkpoint shards: 100%"}},"952da06b5d0b434fb472cf6b24d35fa2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfe102e840d54c0a8749ce23ad584769","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_831627e5e9d44775b22252e07bdf011b","value":2}},"a66a95e27caf417ebfdb0a8a21c6ff59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_481ae9c8086f4cf2b006bb4427398b0c","placeholder":"​","style":"IPY_MODEL_84aec54737d042d9bdfadee727669a58","value":" 2/2 [00:22&lt;00:00,  9.42s/it]"}},"692ee8e5e9cf433f95d2654f7e04564c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823653d2b7e84bc591230ad758bd2885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b114ff533004424a3089dcd7ec27489":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfe102e840d54c0a8749ce23ad584769":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"831627e5e9d44775b22252e07bdf011b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"481ae9c8086f4cf2b006bb4427398b0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84aec54737d042d9bdfadee727669a58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277},{"sourceId":229250,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":193148,"modelId":163613}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CulturoGemma A Gemma agent\nThis notebook details the journey of creating an AI chat agent powered by the Gemma model, designed to engage in conversations across multiple languages and with cultural sensitivity. We'll walk through the code, explaining each component and the reasoning behind the design choices.","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nThe goal of this project is to build an AI assistant capable of understanding and responding to users in various languages, while also being mindful of cultural nuances. This involves leveraging a powerful language model like Gemma and integrating external tools for translation and information retrieval.\nThis will enable us to give the models like Gemma for `Unlocking Global Communication`","metadata":{}},{"cell_type":"markdown","source":"## Setting Up the Environment\nBefore we begin, let's ensure our environment is set up correctly. You'll need to install the following Python libraries:\n```bash\n!pip install transformers torch tavily-py google-cloud-translate wikipedia streamlit\n```","metadata":{}},{"cell_type":"markdown","source":"* **`transformers`**: Provides access to pre-trained language models like Gemma.\n* **`torch`**:  A fundamental library for tensor computation, essential for deep learning models.\n* **`tavily-py`**: A library for interacting with the Tavily API, which allows us to perform web searches and extract content from URLs.\n* **`google-cloud-translate`**:  The Google Cloud Translation API client library for translating text between languages.\n* **`wikipedia`**: A library to easily access and search Wikipedia content.\n* **`streamlit`**: A framework for building interactive web applications with Python.","metadata":{}},{"cell_type":"markdown","source":"You'll also need API keys for Tavily(its free) and Google Cloud Translation. It's recommended to store these securely as environment variables rather than hardcoding them in your script(kaggle, colab secrets).","metadata":{}},{"cell_type":"code","source":"!pip install -q wikipedia tavily-python","metadata":{"id":"HfDUWOFndeDY","outputId":"339d5111-3f3e-4dd8-b1d2-3fd1684a9927"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport io\nimport re\nimport json\nimport torch\nimport wikipedia\nfrom tavily import TavilyClient\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"id":"0jdqZWGkdju4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the environment variables for Kaggle and tavily search.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"TAVILY_KEY\"] = \"tavily-api-key\" # or UserSecretsClient().get_secret(TAVILY_KEY) or userdata.get(TAVILY_KEY) or getpass.getpass(\"Enter your TAVILY_KEY: \")","metadata":{"id":"NsaywHOXdl35"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize API clients using environment variables\ntry:\n    tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_KEY\"])\nexcept KeyError:\n    print(\"Error: TAVILY_KEY environment variable not set.\")\n    tavily_client = None","metadata":{"id":"M_QiHHJWeWTa"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\n* We import necessary libraries.\n* We initialize the Tavily and Google Cloud Translate clients using API keys retrieved from environment variables. **Remember to replace `'path/to/your/service_account_key.json'` with the actual path to your Google Cloud service account key file.**","metadata":{}},{"cell_type":"markdown","source":"## Defining Function Calls\n\nOur agent needs to interact with the outside world to provide comprehensive answers. We achieve this through function calls, which allow the agent to trigger specific actions.","metadata":{}},{"cell_type":"markdown","source":"**Functions and why we use them**\n\nOk, So our functions can be anything, I mean anything but as we want to use this model for **expanding global impact of gemma** we go with these:\n-  1- Web search and URL fetching with tavily search api: This enables the model to search the entire web and gather information about the topic in the conversation\n-  2- wikipedia: This act like our search functions but only limited to wikipedia as its source of information, Still very useful\n-  3- Google translate functions with google cloud api(Not tested): These functions can help if the used language is so far from models knowledge.","metadata":{}},{"cell_type":"markdown","source":"```python\ndef get_answer_from_tavily(query):\n    \"\"\"Searches the web using Tavily and returns the answer.\"\"\"\n    if tavily_client is None:\n        return {\"error\": \"Tavily client not initialized.\"}\n    try:\n        answer = tavily_client.qna_search(query=query)\n        return {\"web_result\": answer}\n    except Exception as e:\n        logging.error(f\"Error fetching answer from Tavily: {e}\")\n        return {\"error\": f\"Error fetching answer: {str(e)}\"}\n\ndef fetch_url(urls):\n    \"\"\"Fetches content from a given URL using Tavily.\"\"\"\n    if tavily_client is None:\n        return {\"error\": \"Tavily client not initialized.\"}\n    try:\n        extract_response = tavily_client.extract(urls=urls)\n        for result in extract_response.get(\"results\", []):\n            return {\"web_url\": result[\"url\"], \"extracted_content\": result[\"raw_content\"]}\n        return {\"error\": \"No results found for the given URL\"}\n    except Exception as e:\n        logging.error(f\"Error fetching URL content: {e}\")\n        return {\"error\": f\"Error fetching URL: {str(e)}\"}\n\ndef search_wikipedia(query):\n    \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n    try:\n        return {\"wikipedia_summary\": wikipedia.summary(query, sentences=3)}\n    except wikipedia.exceptions.PageError:\n        return {\"error\": f\"Wikipedia page not found for query: {query}\"}\n    except wikipedia.exceptions.DisambiguationError as e:\n        return {\"error\": f\"Multiple Wikipedia pages found for query: {query}. Be more specific.\", \"possible_titles\": e.options}\n    except Exception as e:\n        logging.error(f\"Error searching Wikipedia: {e}\")\n        return {\"error\": f\"Error searching Wikipedia: {str(e)}\"}\n\ndef translate_to_english(prompt):\n  \"\"\"Translates the given prompt to English, handling potential errors.\"\"\"\n  try:\n    if use_google_translate:\n      result = translate_client.translate(prompt, target_language='en')\n      return result['translatedText']\n    else:\n      return None  # Return None if translation is disabled\n  except GoogleAPIError as e:\n    print(f\"Error translating to English: {e}\")\n    return prompt  # Return the original prompt on failure\n\ndef translate_to_target(prompt):\n  \"\"\"Translates the given prompt to the previously detected target language, handling errors.\"\"\"\n  global detected_language\n  try:\n    if use_google_translate and detected_language:\n      result = translate_client.translate(prompt, target_language=detected_language)\n      return result['translatedText']\n    else:\n      return \"Target language not detected Or google cloud api not provided. Please call detect_language() first and `use_google_translate=True`.\"\n  except GoogleAPIError as e:\n    print(f\"Error translating to target language: {e}\")\n    return prompt  # Return the original prompt on failure\n```\n","metadata":{}},{"cell_type":"code","source":"def get_answer_from_tavily(query):\n    \"\"\" This function gets query as str and passes the extracted answer as str\"\"\"\n    try:\n        answer = tavily_client.qna_search(query=query)\n        # Assuming the response has an 'answer' key\n        return answer\n    except Exception as e:\n        print(f\"Error fetching answer from Tavily: {e}\")\n        return f\"Error fetching answer: {str(e)}\"\n\ndef fetch_url(urls):\n    \"\"\" this function fetches the user url and returns the extracted content\"\"\"\n    try:\n        extract_response = tavily_client.extract(urls=urls)\n        for result in extract_response.get(\"results\", []):\n            return result[\"url\"], result[\"raw_content\"]\n        return \"No results found\", \"No content found\"\n    except Exception as e:\n        print(f\"Error fetching URL content: {e}\")\n        return f\"Error fetching URL: {str(e)}\", f\"Error fetching content: {str(e)}\"","metadata":{"id":"SrJg1iEwdy0X"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_google_translate = False\nif use_google_translate:\n    from google.cloud import translate_v2 as translate\n    from google.api_core.exceptions import GoogleAPIError\n    # Create a client with your credentials (replace with your service account key file)\n    translate_client = translate.Client.from_service_account_json('path/to/your/service_account_key.json')\n\ndetected_language = None  # Initialize a global variable to store the detected language\n\ndef detect_language(prompt):\n    \"\"\"Detects the language of the given prompt, handling potential errors.\"\"\"\n    global detected_language\n    try:\n        # Check if translation functionality is enabled\n        if use_google_translate:\n            result = translate_client.detect_language(prompt)\n            detected_language = result['language']\n            return detected_language\n        else:\n            # Return None or a default language if translation is disabled\n            return None\n    except GoogleAPIError as e:\n        print(f\"Error detecting language: {e}\")\n        return None\n\ndef translate_to_english(prompt):\n  \"\"\"Translates the given prompt to English, handling potential errors.\"\"\"\n  try:\n    if use_google_translate:\n      result = translate_client.translate(prompt, target_language='en')\n      return result['translatedText']\n    else:\n      return None  # Return None if translation is disabled\n  except GoogleAPIError as e:\n    print(f\"Error translating to English: {e}\")\n    return prompt  # Return the original prompt on failure\n\ndef translate_to_target(prompt):\n  \"\"\"Translates the given prompt to the previously detected target language, handling errors.\"\"\"\n  global detected_language\n  try:\n    if use_google_translate and detected_language:\n      result = translate_client.translate(prompt, target_language=detected_language)\n      return result['translatedText']\n    else:\n      return \"Target language not detected Or google cloud api not provided. Please call detect_language() first and `use_google_translate=True`.\"\n  except GoogleAPIError as e:\n    print(f\"Error translating to target language: {e}\")\n    return prompt  # Return the original prompt on failure","metadata":{"id":"Icb6k7eqd3TU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\n* We define functions for each specific task:\n    * `get_answer_from_tavily`: Searches the web using the Tavily API.\n    * `fetch_url`: Extracts content from a given URL using the Tavily API.\n    * `search_wikipedia`: Searches for information on Wikipedia.\n    * `translate_to_english`: Translates text to English using Google Cloud Translate.\n    * `translate_to_target`: Translates text to a specified target language using Google Cloud Translate.\n* Each function includes error handling to gracefully manage potential issues (e.g., API errors, network problems).\n* We check if the API clients are initialized before attempting to use them.","metadata":{}},{"cell_type":"markdown","source":"## Crafting the System Prompt\n\nThe system prompt guides the behavior of the language model. It tells the model its role, how to interact with users, and how to use the function calls.\n\n```python\n\"\"\"Your task is to assist users by searching for information and providing accurate responses.\n\nFor questions requiring current or factual information, you must first search using this format:\nFUNCTION_CALL: get_web_result(\"search query\")\n\nEXAMPLES:\nUser: What is Thanksgiving?\nAssistant: FUNCTION_CALL: get_web_result(\"What is Thanksgiving holiday history traditions\")\n\nUser: What's happening in Iran?\nAssistant: FUNCTION_CALL: get_web_result(\"current news Iran\")\n\nOther functions that you have access and you can use if needed:\nFor searching Wikipedia: FUNCTION_CALL: search_wikipedia(\"topic to search on wikipedia\")\nEXAMPLE:\nuser: What is Chaharshanbe Suri\nAssistant: FUNCTION_CALL: search_wikipedia(\"Chaharshanbe Suri\")\n\nFor fetching content from a specific URL(from user if provided): FUNCTION_CALL: fetch_url(\"URL\")\nFor translating user input into English (for your better understanding): FUNCTION_CALL: get_translation(\"user's input in their language\")\nFor translating your response into the user's language: FUNCTION_CALL: translate(\"your response in English\")\n\nCurrent conversation:\n\"\"\"\n```\nKeep in mind we make it in this format so we don't get to see the system prompt at the beggining of each response.\nAnd we do all these things because gemma dose not accept system role.","metadata":{}},{"cell_type":"markdown","source":"**Explanation:**\n\n* The system prompt clearly defines the AI's role as a helpful, multilingual, and culturally aware assistant.\n* It explicitly instructs the AI on how to use the defined function calls.\n* It emphasizes cultural sensitivity, respect for different languages.","metadata":{}},{"cell_type":"markdown","source":"Note: You can play with the system prompt to see what works better, This version works well after some trail and errors I made this.","metadata":{}},{"cell_type":"markdown","source":"## Building the `CulturoGemma` Class\n\nThe `CulturoGemma` class encapsulates the logic for interacting with the language model, managing conversations, and executing function calls.","metadata":{}},{"cell_type":"markdown","source":"**The model:**\n* I used the multilingual DPO fine-tuned version of Gemma2 model family, More information can be found on the fine-tuning [notebook](https://www.kaggle.com/code/mahdiseddigh/multilingual-dpo-fine-tuning-gemma)","metadata":{}},{"cell_type":"code","source":"# If you want to use it outside of development you can clear the code by commenting the prints, How ever \n#I recomand adding logging to keep track of models responses and behavior\nclass CulturoGemma:\n    def __init__(self, model_name:str=\"/kaggle/input/gemma2/transformers/gemma2_2b_mulitlingual_dpo/2\", tavily_api:str=\"your-tavily-key\", max_new_token:int=256):\n        # You can use all variations of Gemma model family, You can add quantization, You can also modify the LLM loading\n        # and response generation for using other frameworks like keras, Vllm,...\n        # I also tested with functions like getting stock data and calculating simple math terms, Its works well\n        # You can modify the function calling to use langchain or llamaindex or...\n        try:\n            self.model_name = model_name\n            self.tavily_api = os.environ.get(\"TAVILY_KEY\") or tavily_api\n\n            if not self.model_name or not self.tavily_api:\n                raise ValueError(\"Missing Model name Or API credentials.\")\n\n            self.model, self.tokenizer, error_message = self.load_model_and_tokenizer(model_name=self.model_name)\n            if error_message:\n              print(f\"model loading error:{error_message}\")\n              raise Exception(error_message)\n            self.max_new_token = max_new_token\n            self.conversations = []\n\n            # Define available functions\n            self.available_functions = {\n                \"get_translation\": self.get_translation,\n                \"get_web_result\": self.get_web_result,\n                \"search_wikipedia\": self.search_wikipedia,\n                \"translate\": self.translate,\n                \"get_translation\": self.get_translation,\n                \"fetch_url\": self.get_url\n            }\n\n        except Exception as e:\n            print(f\"Chat Service initialization error: {e}\")\n            raise\n\n    def set_system_instruction(self, instruction: str):\n        \"\"\"Allow updating the system instruction\"\"\"\n        self.system_instruction = instruction\n\n    def clean_response(self, response: str) -> str:\n        \"\"\"Clean the model's response by removing any system prompt repetition\"\"\"\n        # Remove any system instruction repetition\n        if \"Your task is to assist users\" in response:\n            response = response.split(\"Current conversation:\")[-1].strip()\n\n        # Remove any role prefixes that might have been repeated\n        response = re.sub(r'^(Assistant|User):', '', response).strip()\n\n        return response\n    def load_model_and_tokenizer(self, model_name:str):\n        try:\n            # Check if a GPU is available\n            if not torch.cuda.is_available():\n                return None, None, \"GPU is not available. Please switch to GPU.\"\n\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=\"auto\"\n            )\n            print(\"Model And Tokenizer Loaded Successfully...\")\n            return model, tokenizer, None\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            return None, None, \"Failed to load the model.\"\n\n    def get_model_response(self, prompt:str) -> str:\n      try:\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        with torch.no_grad():\n          outputs = self.model.generate(**input_ids, max_new_tokens=self.max_new_token)\n          response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response\n      except Exception as e:\n        print(f\"Error generating response: {e}\")\n        return \"Error generating response with the model.\"\n\n    def get_translation(self, prompt:str) -> dict:\n        \"\"\"Get the translation of users prompt with google translate api\"\"\"\n        try:\n            translation = translate_to_english(prompt)\n            return {\"translation\":translation}\n        except Exception as e:\n            return {\"error\": f\"could not get the english translation: {str(e)}\"}\n\n    def translate(self, prompt:str) -> dict:\n        \"\"\"Get the translation from english prompt to detected target language\"\"\"\n        try:\n            translation = translate_to_target(prompt)\n            return {\"translation\":translation}\n        except Exception as e:\n            return {\"error\": f\"could not get the target translation: {str(e)}\"}\n\n    def get_web_result(self, query:str) -> dict:\n        \"\"\"Searches The web for a given query and returns an answer.\"\"\"\n        try:\n            answer = get_answer_from_tavily(query)\n            print(f\"got answer from web for:{query}\")\n            return {\"web_result\": answer}\n        except Exception as e:\n            return {\"error\": f\"could not get web answer: {str(e)}\"}\n\n    def get_url(self, urls):\n        \"\"\"fetches URLs.\"\"\"\n        try:\n            url, content = fetch_url(urls)\n            print(f\"fetched for url:{urls}\")\n            return {\"web_url\":url, \"extracted_content\":content}\n        except Exception as e:\n            return {\"error\": f\"could not fetch url: {str(e)}\"}\n\n    def search_wikipedia(self, query:str) -> dict:\n        \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n        try:\n            # Attempt to get the summary of the page directly\n            return {\"wikipedia_summary\": wikipedia.summary(query, sentences=3)}\n        except wikipedia.exceptions.PageError:\n            return {\"error\": f\"Wikipedia page not found for query: {query}\"}\n        except wikipedia.exceptions.DisambiguationError as e:\n            return {\"error\": f\"Multiple Wikipedia pages found for query: {query}. Be more specific.\", \"possible_titles\": e.options}\n        except Exception as e:\n            return {\"error\": f\"Error searching Wikipedia: {e}\"}\n\n    def execute_function(self, function_text: str) -> str:\n        \"\"\"Execute a function based on the text command\"\"\"\n        try:\n            # Extract function name and parameters using regex\n            match = re.match(r'FUNCTION_CALL:\\s*(\\w+)\\(\"([^\"]*)\"\\)', function_text.strip())\n            if not match:\n                print(f\"Failed to parse function call: {function_text}\")\n                return \"Error: Invalid function call format. Please try again.\"\n\n            function_name, param = match.groups()\n            print(f\"Executing function: {function_name} with param: {param}\")\n\n            if function_name not in self.available_functions:\n                return f\"Error: Unknown function {function_name}\"\n\n            result = self.available_functions[function_name](param)\n            print(f\"Function result: {result}\")\n\n            if isinstance(result, dict):\n                if \"error\" in result:\n                    return f\"Error: {result['error']}\"\n                return str(result.get(\"web_result\", result.get(\"extracted_content\", result.get(\"translation\", str(result)))))\n\n            return str(result)\n\n        except Exception as e:\n            print(f\"Function execution error: {e}\")\n            return f\"Error executing function: {str(e)}\"\n\n\n    def format_conversation_history(self, messages):\n        \"\"\"Format conversation history with clear separation\"\"\"\n        # Fixed instructions at the start of every prompt since Gemma dose not accept system role!\n        formatted_prompt = \"\"\"Your task is to assist users by searching for information and providing accurate responses.\n\nFor questions requiring current or factual information, you must first search using this format:\nFUNCTION_CALL: get_web_result(\"search query\")\n\nEXAMPLES:\nUser: What is Thanksgiving?\nAssistant: FUNCTION_CALL: get_web_result(\"What is Thanksgiving holiday history traditions\")\n\nUser: What's happening in Iran?\nAssistant: FUNCTION_CALL: get_web_result(\"current news Iran\")\n\nOther functions that you have access and you can use if needed:\nFor searching Wikipedia: FUNCTION_CALL: search_wikipedia(\"topic to search on wikipedia\")\nEXAMPLE:\nuser: What is Chaharshanbe Suri\nAssistant: FUNCTION_CALL: search_wikipedia(\"Chaharshanbe Suri\")\n\nFor fetching content from a specific URL(from user if provided): FUNCTION_CALL: fetch_url(\"URL\")\nFor translating user input into English (for your better understanding): FUNCTION_CALL: get_translation(\"user's input in their language\")\nFor translating your response into the user's language: FUNCTION_CALL: translate(\"your response in English\")\n\nCurrent conversation:\n\"\"\"\n        # Add conversation history\n        for msg in messages:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n\n            if role == \"system\":\n                continue\n\n            if role == \"function\":\n                formatted_prompt += f\"[Search Result: {content}]\\n\"\n                continue\n\n            role_prefix = \"User\" if role == \"user\" else \"Assistant\"\n            formatted_prompt += f\"{role_prefix}: {content}\\n\"\n\n        # Add final prompt\n        formatted_prompt += \"Assistant: \"\n        return formatted_prompt\n\n    def get_response(self, message):\n        \"\"\"Get a response from the assistant\"\"\"\n        try:\n            # Add user message\n            self.conversations.append({\"role\": \"user\", \"content\": message})\n\n            # Format conversation to avoid problems.\n            formatted_prompt = self.format_conversation_history(self.conversations)\n            print(f\"\\nFormatted prompt:\\n{formatted_prompt}\\n\")\n\n            # Get initial response(pre-function call)\n            assistant_response = self.get_model_response(formatted_prompt)\n            assistant_response = self.clean_response(assistant_response)\n            print(f\"\\nInitial response:\\n{assistant_response}\\n\")\n\n            # Handle function calls\n            if \"FUNCTION_CALL:\" in assistant_response:\n                # Extract function call - take the first line that contains FUNCTION_CALL\n                function_call = next(line for line in assistant_response.split('\\n')\n                                  if \"FUNCTION_CALL:\" in line).strip()\n                print(f\"\\nDetected function call:\\n{function_call}\\n\")\n\n                # Execute function\n                function_result = self.execute_function(function_call)\n                print(f\"\\nFunction result:\\n{function_result}\\n\")\n\n                # Add to conversation history\n                self.conversations.extend([\n                    {\"role\": \"assistant\", \"content\": function_call},\n                    {\"role\": \"function\", \"content\": function_result}\n                ])\n\n                # Get final response\n                formatted_prompt = self.format_conversation_history(self.conversations)\n                assistant_response = self.get_model_response(formatted_prompt)\n                assistant_response = self.clean_response(assistant_response)\n                print(f\"\\nFinal response:\\n{assistant_response}\\n\")\n\n            # Store final response\n            self.conversations.append({\"role\": \"assistant\", \"content\": assistant_response})\n            return assistant_response\n\n        except Exception as e:\n            print(f\"Error in get_response: {e}\")\n            return f\"I apologize, but I encountered an error: {str(e)}\"# simple error handdling","metadata":{"id":"RgtJX0yqd9xr"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\n* **`__init__`**: Initializes the model, tokenizer, conversation history, and sets the system prompt.\n* **`get_model_response`**: Sends the formatted prompt to the Gemma model and returns the raw text response.\n* **`execute_function`**: Parses the function call from the model's response and executes the corresponding function. It uses a `function_mapping` dictionary to link function names to their implementations.\n* **`format_conversation_history`**: Formats the conversation history into a single string that can be fed to the model.\n* **`get_response`**: This is the core method for handling user input:\n    1. Appends the user's message to the conversation history.\n    2. Translates the user's message to English for internal processing(if needed and choosed by the model).\n    3. Detects the user's language.\n    4. Formats the conversation history.\n    5. Gets the initial response from the Gemma model.\n    6. If the response contains a function call, it executes the function and gets an updated response.\n    7. Translates the final response back to the user's language(if needed and choosed by the model).\n    8. Appends the assistant's response to the conversation history.\n","metadata":{}},{"cell_type":"markdown","source":"## Initialzing the `CulturoGemma`\nBy running the cell below we initialize the `CulturoGemma`.","metadata":{}},{"cell_type":"code","source":"try:\n  culturo_gemma = CulturoGemma()\n  print(\"Culturo Gemma is ready to go...\")\nexcept Exception as e:\n  print(f\"Error initializing Culturo Gemma: {e}\")","metadata":{"id":"MwuCS_ibeE_p","outputId":"c8d1d85b-a646-4038-d51d-661293c5d859"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = culturo_gemma.get_response(\"What is Greenery Day?\")\nprint(f\"response: \\n {response}\")","metadata":{"id":"NyJSGVY3otg-","outputId":"8cc9fb52-b0f3-481f-ac12-a161ec0892b6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a Simple Command-Line Interface\n\nTo test our agent, we can create a simple command-line interface using Jupyter Notebook.","metadata":{}},{"cell_type":"code","source":"print(\"Welcome to the Culturo Gemma! Type 'EnD' to exit.\")\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == \"end\":\n        print(\"Chat ended.\")\n        break\n\n    try:\n        response = culturo_gemma.get_response(user_input)\n        print(f\"AI: {response}\")\n        print(\"\\n ------------------------ \\n\")\n    except Exception as e:\n        print(f\"Error processing your request: {e}\")","metadata":{"id":"t5MhMiS_h_lH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\n* We create an instance of the `CulturoGemma`.\n* A `while` loop allows for continuous interaction until the user types \"EnD\".\n* The `input()` function gets the user's message.\n* The `culturo_gemma.get_response()` method processes the input and generates the AI's response.\n* The AI's response is printed to the console.\n* Error handling is included to catch any issues during the process.","metadata":{}},{"cell_type":"markdown","source":"## Building a Streamlit Interface\n\nFor a more user-friendly experience, we can create a web interface using Streamlit.","metadata":{}},{"cell_type":"code","source":"%%writefile streamlit_app.py\nimport os\nimport io\nimport re\nimport json\nimport torch\nimport wikipedia\nimport streamlit as st\nfrom tavily import TavilyClient\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Set the environment variables for Kaggle and tavily search.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your  KAGGLE_KEY: \")\nos.environ[\"TAVILY_KEY\"] = \"tavily-api-key\" # or UserSecretsClient().get_secret(TAVILY_KEY) or userdata.get(TAVILY_KEY) or getpass.getpass(\"Enter your TAVILY_KEY: \")\n\n# Initialize API clients using environment variables\ntry:\n    tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_KEY\"])\nexcept KeyError:\n    print(\"Error: TAVILY_KEY environment variable not set.\")\n    tavily_client = None\n\ndef get_answer_from_tavily(query):\n    \"\"\" This function gets query as str and passes the extracted answer as str\"\"\"\n    try:\n        answer = tavily_client.qna_search(query=query)\n        # Assuming the response has an 'answer' key\n        return answer\n    except Exception as e:\n        print(f\"Error fetching answer from Tavily: {e}\")\n        return f\"Error fetching answer: {str(e)}\"\n\ndef fetch_url(urls):\n    \"\"\" this function fetches the user url and returns the extracted content\"\"\"\n    try:\n        extract_response = tavily_client.extract(urls=urls)\n        for result in extract_response.get(\"results\", []):\n            return result[\"url\"], result[\"raw_content\"]\n        return \"No results found\", \"No content found\"\n    except Exception as e:\n        print(f\"Error fetching URL content: {e}\")\n        return f\"Error fetching URL: {str(e)}\", f\"Error fetching content: {str(e)}\"\nuse_google_translate = False\nif use_google_translate:\n    from google.cloud import translate_v2 as translate\n    from google.api_core.exceptions import GoogleAPIError\n    # Create a client with your credentials (replace with your service account key file)\n    translate_client = translate.Client.from_service_account_json('path/to/your/service_account_key.json')\n\ndetected_language = None  # Initialize a global variable to store the detected language\n\ndef detect_language(prompt):\n    \"\"\"Detects the language of the given prompt, handling potential errors.\"\"\"\n    global detected_language\n    try:\n        # Check if translation functionality is enabled\n        if use_google_translate:\n            result = translate_client.detect_language(prompt)\n            detected_language = result['language']\n            return detected_language\n        else:\n            # Return None or a default language if translation is disabled\n            return None\n    except GoogleAPIError as e:\n        print(f\"Error detecting language: {e}\")\n        return None\n\ndef translate_to_english(prompt):\n  \"\"\"Translates the given prompt to English, handling potential errors.\"\"\"\n  try:\n    if use_google_translate:\n      result = translate_client.translate(prompt, target_language='en')\n      return result['translatedText']\n    else:\n      return None  # Return None if translation is disabled\n  except GoogleAPIError as e:\n    print(f\"Error translating to English: {e}\")\n    return prompt  # Return the original prompt on failure\n\ndef translate_to_target(prompt):\n  \"\"\"Translates the given prompt to the previously detected target language, handling errors.\"\"\"\n  global detected_language\n  try:\n    if use_google_translate and detected_language:\n      result = translate_client.translate(prompt, target_language=detected_language)\n      return result['translatedText']\n    else:\n      return \"Target language not detected Or google cloud api not provided. Please call detect_language() first and `use_google_translate=True`.\"\n  except GoogleAPIError as e:\n    print(f\"Error translating to target language: {e}\")\n    return prompt  # Return the original prompt on failure\n      \n# If you want to use it outside of development you can clear the code by commenting the prints, How ever \n#I recomand adding logging to keep track of models responses and behavior\nclass CulturoGemma:\n    def __init__(self, model_name:str=\"/kaggle/input/gemma2/transformers/gemma2_2b_mulitlingual_dpo/2\", tavily_api:str=\"your-tavily-key\", max_new_token:int=256):\n        # You can use all variations of Gemma model family, You can add quantization, You can also modify the LLM loading\n        # and response generation for using other frameworks like keras, Vllm,...\n        # I also tested with functions like getting stock data and calculating simple math terms, Its works well\n        # You can modify the function calling to use langchain or llamaindex or...\n        try:\n            self.model_name = model_name\n            self.tavily_api = os.environ.get(\"TAVILY_KEY\") or tavily_api\n\n            if not self.model_name or not self.tavily_api:\n                raise ValueError(\"Missing Model name Or API credentials.\")\n\n            self.model, self.tokenizer, error_message = self.load_model_and_tokenizer(model_name=self.model_name)\n            if error_message:\n              print(f\"model loading error:{error_message}\")\n              raise Exception(error_message)\n            self.max_new_token = max_new_token\n            self.conversations = []\n\n            # Define available functions\n            self.available_functions = {\n                \"get_translation\": self.get_translation,\n                \"get_web_result\": self.get_web_result,\n                \"search_wikipedia\": self.search_wikipedia,\n                \"translate\": self.translate,\n                \"get_translation\": self.get_translation,\n                \"fetch_url\": self.get_url\n            }\n\n        except Exception as e:\n            print(f\"Chat Service initialization error: {e}\")\n            raise\n\n    def set_system_instruction(self, instruction: str):\n        \"\"\"Allow updating the system instruction\"\"\"\n        self.system_instruction = instruction\n\n    def clean_response(self, response: str) -> str:\n        \"\"\"Clean the model's response by removing any system prompt repetition\"\"\"\n        # Remove any system instruction repetition\n        if \"Your task is to assist users\" in response:\n            response = response.split(\"Current conversation:\")[-1].strip()\n\n        # Remove any role prefixes that might have been repeated\n        response = re.sub(r'^(Assistant|User):', '', response).strip()\n\n        return response\n    def load_model_and_tokenizer(self, model_name:str):\n        try:\n            # Check if a GPU is available\n            if not torch.cuda.is_available():\n                return None, None, \"GPU is not available. Please switch to GPU.\"\n\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=\"auto\"\n            )\n            print(\"Model And Tokenizer Loaded Successfully...\")\n            return model, tokenizer, None\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            return None, None, \"Failed to load the model.\"\n\n    def get_model_response(self, prompt:str) -> str:\n      try:\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        with torch.no_grad():\n          outputs = self.model.generate(**input_ids, max_new_tokens=self.max_new_token)\n          response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response\n      except Exception as e:\n        print(f\"Error generating response: {e}\")\n        return \"Error generating response with the model.\"\n\n    def get_translation(self, prompt:str) -> dict:\n        \"\"\"Get the translation of users prompt with google translate api\"\"\"\n        try:\n            translation = translate_to_english(prompt)\n            return {\"translation\":translation}\n        except Exception as e:\n            return {\"error\": f\"could not get the english translation: {str(e)}\"}\n\n    def translate(self, prompt:str) -> dict:\n        \"\"\"Get the translation from english prompt to detected target language\"\"\"\n        try:\n            translation = translate_to_target(prompt)\n            return {\"translation\":translation}\n        except Exception as e:\n            return {\"error\": f\"could not get the target translation: {str(e)}\"}\n\n    def get_web_result(self, query:str) -> dict:\n        \"\"\"Searches The web for a given query and returns an answer.\"\"\"\n        try:\n            answer = get_answer_from_tavily(query)\n            print(f\"got answer from web for:{query}\")\n            return {\"web_result\": answer}\n        except Exception as e:\n            return {\"error\": f\"could not get web answer: {str(e)}\"}\n\n    def get_url(self, urls):\n        \"\"\"fetches URLs.\"\"\"\n        try:\n            url, content = fetch_url(urls)\n            print(f\"fetched for url:{urls}\")\n            return {\"web_url\":url, \"extracted_content\":content}\n        except Exception as e:\n            return {\"error\": f\"could not fetch url: {str(e)}\"}\n\n    def search_wikipedia(self, query:str) -> dict:\n        \"\"\"Searches Wikipedia for a given query and returns a summary.\"\"\"\n        try:\n            # Attempt to get the summary of the page directly\n            return {\"wikipedia_summary\": wikipedia.summary(query, sentences=3)}\n        except wikipedia.exceptions.PageError:\n            return {\"error\": f\"Wikipedia page not found for query: {query}\"}\n        except wikipedia.exceptions.DisambiguationError as e:\n            return {\"error\": f\"Multiple Wikipedia pages found for query: {query}. Be more specific.\", \"possible_titles\": e.options}\n        except Exception as e:\n            return {\"error\": f\"Error searching Wikipedia: {e}\"}\n\n    def execute_function(self, function_text: str) -> str:\n        \"\"\"Execute a function based on the text command\"\"\"\n        try:\n            # Extract function name and parameters using regex\n            match = re.match(r'FUNCTION_CALL:\\s*(\\w+)\\(\"([^\"]*)\"\\)', function_text.strip())\n            if not match:\n                print(f\"Failed to parse function call: {function_text}\")\n                return \"Error: Invalid function call format. Please try again.\"\n\n            function_name, param = match.groups()\n            print(f\"Executing function: {function_name} with param: {param}\")\n\n            if function_name not in self.available_functions:\n                return f\"Error: Unknown function {function_name}\"\n\n            result = self.available_functions[function_name](param)\n            print(f\"Function result: {result}\")\n\n            if isinstance(result, dict):\n                if \"error\" in result:\n                    return f\"Error: {result['error']}\"\n                return str(result.get(\"web_result\", result.get(\"extracted_content\", result.get(\"translation\", str(result)))))\n\n            return str(result)\n\n        except Exception as e:\n            print(f\"Function execution error: {e}\")\n            return f\"Error executing function: {str(e)}\"\n\n\n    def format_conversation_history(self, messages):\n        \"\"\"Format conversation history with clear separation\"\"\"\n        # Fixed instructions at the start of every prompt since Gemma dose not accept system role!\n        formatted_prompt = \"\"\"Your task is to assist users by searching for information and providing accurate responses.\n\nFor questions requiring current or factual information, you must first search using this format:\nFUNCTION_CALL: get_web_result(\"search query\")\n\nEXAMPLES:\nUser: What is Thanksgiving?\nAssistant: FUNCTION_CALL: get_web_result(\"What is Thanksgiving holiday history traditions\")\n\nUser: What's happening in Iran?\nAssistant: FUNCTION_CALL: get_web_result(\"current news Iran\")\n\nOther functions that you have access and you can use if needed:\nFor searching Wikipedia: FUNCTION_CALL: search_wikipedia(\"topic to search on wikipedia\")\nEXAMPLE:\nuser: What is Chaharshanbe Suri\nAssistant: FUNCTION_CALL: search_wikipedia(\"Chaharshanbe Suri\")\n\nFor fetching content from a specific URL(from user if provided): FUNCTION_CALL: fetch_url(\"URL\")\nFor translating user input into English (for your better understanding): FUNCTION_CALL: get_translation(\"user's input in their language\")\nFor translating your response into the user's language: FUNCTION_CALL: translate(\"your response in English\")\n\nCurrent conversation:\n\"\"\"\n        # Add conversation history\n        for msg in messages:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n\n            if role == \"system\":\n                continue\n\n            if role == \"function\":\n                formatted_prompt += f\"[Search Result: {content}]\\n\"\n                continue\n\n            role_prefix = \"User\" if role == \"user\" else \"Assistant\"\n            formatted_prompt += f\"{role_prefix}: {content}\\n\"\n\n        # Add final prompt\n        formatted_prompt += \"Assistant: \"\n        return formatted_prompt\n\n    def get_response(self, message):\n        \"\"\"Get a response from the assistant\"\"\"\n        try:\n            # Add user message\n            self.conversations.append({\"role\": \"user\", \"content\": message})\n\n            # Format conversation to avoid problems.\n            formatted_prompt = self.format_conversation_history(self.conversations)\n            print(f\"\\nFormatted prompt:\\n{formatted_prompt}\\n\")\n\n            # Get initial response(pre-function call)\n            assistant_response = self.get_model_response(formatted_prompt)\n            assistant_response = self.clean_response(assistant_response)\n            print(f\"\\nInitial response:\\n{assistant_response}\\n\")\n\n            # Handle function calls\n            if \"FUNCTION_CALL:\" in assistant_response:\n                # Extract function call - take the first line that contains FUNCTION_CALL\n                function_call = next(line for line in assistant_response.split('\\n')\n                                  if \"FUNCTION_CALL:\" in line).strip()\n                print(f\"\\nDetected function call:\\n{function_call}\\n\")\n\n                # Execute function\n                function_result = self.execute_function(function_call)\n                print(f\"\\nFunction result:\\n{function_result}\\n\")\n\n                # Add to conversation history\n                self.conversations.extend([\n                    {\"role\": \"assistant\", \"content\": function_call},\n                    {\"role\": \"function\", \"content\": function_result}\n                ])\n\n                # Get final response\n                formatted_prompt = self.format_conversation_history(self.conversations)\n                assistant_response = self.get_model_response(formatted_prompt)\n                assistant_response = self.clean_response(assistant_response)\n                print(f\"\\nFinal response:\\n{assistant_response}\\n\")\n\n            # Store final response\n            self.conversations.append({\"role\": \"assistant\", \"content\": assistant_response})\n            return assistant_response\n\n        except Exception as e:\n            print(f\"Error in get_response: {e}\")\n            return f\"I apologize, but I encountered an error: {str(e)}\"# simple error handdling\n\n\n# Initialize ChatService in session state\nif 'culturo_gemma' not in st.session_state:\n    st.session_state.culturo_gemma = CulturoGemma()\n\n# Initialize chat history in session state\nif 'messages' not in st.session_state:\n    st.session_state.messages = []\n\nst.title(\"Multilingual & Culturally Aware AI Agent\")\n\nuser_input = st.text_input(\"You:\", key=\"user_input\")\n\nif st.button(\"Send\"):\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n        try:\n            ai_response = st.session_state.culturo_gemma.get_response(user_input)\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": ai_response})\n        except Exception as e:\n            st.session_state.messages.append({\"role\": \"error\", \"content\": f\"Error: {e}\"})\n        st.session_state[\"user_input\"] = \"\"\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\n* We import the necessary libraries, including `streamlit`.\n* We initialize the `ChatService` and the chat `messages` list in Streamlit's `session_state`. This ensures that the state of the application is preserved across interactions.\n* `st.title()` sets the title of the web application.\n* `st.text_input()` creates a text input field for the user.\n* `st.button()` creates a \"Send\" button.\n* When the button is clicked, the user's input is processed by the `chat_service.get_response()` method, and the response is added to the chat history.\n* `st.chat_message()` is used to display the chat messages with different styling based on the role (user or assistant).\n\nTo run the Streamlit application, save this code as a Python file (e.g., `streamlit_app.py`) and run the following command in your terminal:\n\n```bash\nstreamlit run streamlit_app.py\n```","metadata":{}},{"cell_type":"markdown","source":"## Naming the Agent: CulturoGemma\n\nI've poured effort into building an AI agent that's not just intelligent but also adept at navigating the complexities of language and culture. To give our creation a fitting identity, I've chosen the name **CulturoGemma**.\n\nThis name is a deliberate blend of two key aspects of our agent:\n\n* **Culturo:** This prefix directly emphasizes the agent's **cultural awareness** and sensitivity. It signifies the agent's ability to understand and respect diverse customs, traditions, and perspectives. The \"Culturo\" element highlights our focus on building an AI that can engage thoughtfully and respectfully with users from various backgrounds.\n\n* **Gemma:** This suffix clearly identifies the underlying **power and intelligence** of the agent, stemming from the Gemma large language model. By including \"Gemma,\" we acknowledge the technological foundation that enables the agent's sophisticated language processing and generation capabilities.","metadata":{}},{"cell_type":"markdown","source":"## Future path:\n**What else can we do?**\n\nSo we can do anything but I suggest these(I would do them if I have the time):\n* 1- a sort of ReACT agent, So it can call multiple functions in a single response\n* 2- adding more functions\n* 3- fine-tuning the model for multi-lingual instruct and tool calling\n* 4- adding voice featuers(But this is out of our current scope since we deal with the model not the web or app, It can be achieved easily, Take a look at one of my projects, [repo](https://github.com/Mhdaw/All-chat)\n* 5- incorprating RAG as well, You can see an example in one of my notebooks published.\n* 6- using langchain and other libraries to make the progress more reliable","metadata":{}},{"cell_type":"markdown","source":"### My Gemma cookbook:\nI made this repo and uploaded all of my projects:\nhttps://github.com/Mhdaw/Gemma2","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis notebook has demonstrated the process of building a sophisticated AI chat agent capable of engaging in multilingual and culturally sensitive conversations. By leveraging powerful language models, external APIs, and careful design, we can create AI assistants that are more inclusive and effective in a global context. Further improvements could include:\n\n* **More sophisticated error handling and user feedback.**\n* **Integration with more diverse information sources.**\n* **Fine-tuning the Gemma model for specific cultural contexts.**\n* **Implementing user authentication and personalized experiences.**\n\nThis project provides a solid foundation for building advanced AI agents that can communicate and understand the world in a richer and more nuanced way.","metadata":{}}]}