{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Gemma Weaver: weaving knowledge from different languages","metadata":{}},{"cell_type":"markdown","source":"## Introduction: Weaving Knowledge with Gemma 2B: A Multilingual RAG System\n\nThis notebook details the creation of a Retrieval-Augmented Generation (RAG) system, leveraging the power of Google's Gemma 2B language model, fine-tuned on a multilingual dataset from Project Gutenberg. This approach combines the generative capabilities of LLMs with the ability to retrieve relevant information from a knowledge base, resulting in more accurate and contextually rich responses.\n\nThe notebook is divided into three key parts:\n\n1. **Fine-tuning:** In this section, we adapt the pre-trained Gemma 2B model to better understand and generate text from the multilingual Gutenberg dataset. We utilize Parameter-Efficient Fine-Tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA), to efficiently train the model. Quantization is also employed to manage memory usage during training.\n\n2. **Making the RAG Database:** Here, we construct a vector database from the same Gutenberg dataset. This involves chunking the text data, generating sentence embeddings using a transformer model, and indexing these embeddings using FAISS for efficient similarity search. This database will serve as the knowledge source for our RAG system.\n\n3. **Putting it Together:**  This final section integrates the fine-tuned Gemma 2B model with the RAG database. When a query is posed, the system first retrieves relevant text chunks from the database based on semantic similarity. This retrieved context is then used to augment the prompt given to the language model, allowing it to generate more informed and grounded answers.\n\nEach cell in the notebook is explained in detail to provide a clear understanding of the code and the underlying processes involved in building this multilingual RAG system. By following this notebook, you will gain a comprehensive understanding of how to build a RAG system from scratch using state-of-the-art models and techniques.\n","metadata":{}},{"cell_type":"markdown","source":"## Part 1: Fine-tuning a Language Model\n\nThis section focuses on fine-tuning a pre-trained language model, specifically Google's Gemma 2B, on a multilingual text dataset. The goal is to adapt the model to better understand and generate text relevant to the content it's trained on.","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets trl accelerate peft bitsandbytes kagglehub sentence-transformers faiss-gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell installs the necessary Python libraries for fine-tuning and working with large language models. Here's a breakdown:\n\n*   `datasets`: Provides access to various datasets, including those from Hugging Face.\n*   `trl` (Transformers Reinforcement Learning): Contains tools and utilities for training language models, including supervised fine-tuning.\n*   `accelerate`: Facilitates distributed training and handles hardware abstraction.\n*   `peft` (Parameter-Efficient Fine-Tuning): Enables efficient fine-tuning of large models by only training a small subset of parameters, like LoRA (Low-Rank Adaptation).\n*   `bitsandbytes`: Allows for efficient memory usage by quantizing model weights (e.g., loading in 4-bit).\n*   `kagglehub`: Enables interaction with Kaggle models and datasets.\n*   `sentence-transformers`: A library for creating sentence embeddings. While not directly used in the fine-tuning, it's used later for the RAG database.\n*   `faiss-gpu`: A library for efficient similarity search in high-dimensional spaces, used later for the RAG database. The `-gpu` version indicates GPU support.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import SFTConfig, SFTTrainer\nfrom transformers import GemmaTokenizerFast\nfrom peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nimport numpy as np\nimport os\nimport pickle\nimport re\nfrom tqdm import tqdm\nimport concurrent.futures\nimport cProfile\nimport pandas as pd\nimport shutil\nimport kagglehub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell imports the specific modules and classes needed from the installed libraries. Key imports include:\n\n*   `torch`: The fundamental PyTorch library for tensor operations and neural networks.\n*   `transformers`: Hugging Face's core library for working with pre-trained language models, including tokenizers and model classes.\n*   `trl`: Specifically `SFTConfig` for setting up supervised fine-tuning configurations and `SFTTrainer` for performing the fine-tuning.\n*   `peft`: For using LoRA (`LoraConfig`, `get_peft_model`) and managing PEFT models.\n*   `accelerate`: For using the `Accelerator` class, although it's not explicitly used in this snippet.\n*   `datasets`: To load the training dataset.\n*   Standard Python libraries like `os`, `pickle`, `re`, `tqdm` for file operations, data serialization, regular expressions, and progress bars.\n","metadata":{}},{"cell_type":"code","source":"# Set the environment variables for Kaggle and huggingface.\n# from kaggle_secrets import UserSecretsClient if you use kaggle\n# from google.colab import userdata if you use google colab\n#import getpass if you use jupyter notebook\nos.environ[\"KAGGLE_USERNAME\"] = \"your-username\"# or UserSecretsClient().get_secret(KAGGLE_USERNAME) or userdata.get(KAGGLE_USERNAME) or getpass.getpass(\"Enter your KAGGLE_USERNAME: \")\nos.environ[\"KAGGLE_KEY\"] = \"kaggle-api-key\" # or UserSecretsClient().get_secret(KAGGLE_KEY) or userdata.get(KAGGLE_KEY) or getpass.getpass(\"Enter your KAGGLE_KEY: \")\nos.environ[\"HF_TOKEN\"] = \"huggingface-api-key\" # or UserSecretsClient().get_secret(HF_TOKEN) or userdata.get(HF_TOKEN) or getpass.getpass(\"Enter your HF_TOKEN: \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell sets environment variables required for authentication with Kaggle and Hugging Face.\n\n*   `KAGGLE_USERNAME` and `KAGGLE_KEY`: Your Kaggle API credentials, needed for uploading the fine-tuned model later. **Remember to replace `\"your-username\"` and `\"kaggle-api-key\"` with your actual credentials.** The commented-out code shows alternative ways to securely manage secrets depending on your environment (Kaggle Secrets, Google Colab userdata, or direct input).\n*   `HF_TOKEN`: Your Hugging Face API token, which might be required for downloading certain models or pushing to the Hugging Face Hub. **Replace `\"huggingface-api-key\"` with your actual token.**","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"sedthh/gutenberg_multilang\", split=\"train\")\ndataset = dataset.shuffle(seed=65).select(range(3000)) # Only use 3000 samples for quick demo\ndataset = dataset.rename_column('TEXT', 'text')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell loads and prepares the dataset for fine-tuning.\n\n*   `load_dataset(\"sedthh/gutenberg_multilang\", split=\"train\")`: Loads the \"sedthh/gutenberg_multilang\" dataset from Hugging Face Datasets. This dataset contains multilingual text extracted from Project Gutenberg. The `split=\"train\"` argument specifies that we want the training portion of the dataset.\n*   `.shuffle(seed=65)`: Shuffles the dataset with a fixed random seed (65) for reproducibility.\n*   `.select(range(3000))`: Selects the first 3000 samples from the shuffled dataset. This is done for demonstration purposes to speed up the fine-tuning process. In a real-world scenario, you would likely use a larger portion of the dataset.\n*   `.rename_column('TEXT', 'text')`: Renames the column containing the text data from 'TEXT' to 'text'. This is often necessary as the `SFTTrainer` expects the text column to be named 'text' by default.\nfor more information take a look at dataset [page](https://huggingface.co/datasets/sedthh/gutenberg_multilang).\n\nwe used 7 languages for fine-tuning which are mentioned in the dataset page.","metadata":{}},{"cell_type":"code","source":"model_name = \"google/gemma-2-2b-it\" \n\nquantizationConfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantizationConfig\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = 'right' # To avoid warnings\nlora_config = LoraConfig(\n    r=4,  # Adjust this value to control the number of trainable parameters\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Specify target modules to apply LoRA(linear)\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nprint(\"Model loaded\")\nmodel.enable_input_require_grads()# This is necessary for training, If not you will face errors.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell loads the pre-trained Gemma 2B model and configures it for fine-tuning with LoRA and quantization.\n\n*   `model_name = \"google/gemma-2-2b-it\"`: Specifies the pre-trained model to use, in this case, the instruction-tuned version of Gemma 2B from Google.\n*   `quantizationConfig = BitsAndBytesConfig(...)`: Configures the `bitsandbytes` library for 4-bit quantization.\n    *   `load_in_4bit=True`: Loads the model weights in 4-bit precision, reducing memory usage.\n    *   `bnb_4bit_compute_dtype=torch.float16`: Specifies the data type for computation within the 4-bit quantized layers (using half-precision floating-point for potential speedups).\n    *   `bnb_4bit_quant_type=\"nf4\"`: Uses the Normal Float 4 (NF4) quantization type.\n*   `model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantizationConfig)`: Loads the pre-trained Gemma model with the specified quantization configuration. `AutoModelForCausalLM` is used because Gemma is a causal language model (predicting the next token).\n*   `tokenizer = AutoTokenizer.from_pretrained(model_name)`: Loads the tokenizer associated with the Gemma model. Tokenizers are used to convert text into numerical representations that the model can understand.\n*   `tokenizer.padding_side = 'right'`: Sets the padding side for tokenization. Padding is used to make sequences in a batch have the same length. Setting it to 'right' is a common practice to avoid issues with certain models.\n*   `lora_config = LoraConfig(...)`: Configures the LoRA (Low-Rank Adaptation) method for parameter-efficient fine-tuning.\n    *   `r=4`: The rank of the low-rank matrices used in LoRA. A smaller value reduces the number of trainable parameters.\n    *   `lora_alpha=8`: A scaling factor for the LoRA updates.\n    *   `target_modules=[\"q_proj\", \"v_proj\"]`: Specifies the linear layers within the transformer blocks where LoRA will be applied (query and value projection layers in the attention mechanism).\n    *   `lora_dropout=0.1`: Dropout probability for the LoRA layers.\n    *   `bias=\"none\"`: Specifies that no bias terms should be added in the LoRA layers.\n    *   `task_type=\"CAUSAL_LM\"`: Indicates that the task is causal language modeling.\n*   `model = get_peft_model(model, lora_config)`: Wraps the base Gemma model with the LoRA adapters, making only the LoRA parameters trainable.\n*   `print(\"Model loaded\")`: Prints a confirmation message.\n*   `model.enable_input_require_grads()`: Enables gradient computation for the input embeddings. This is crucial for training. If not enabled, you will encounter errors during the training process.\n","metadata":{}},{"cell_type":"code","source":"training_args = SFTConfig(\n    per_device_train_batch_size=1,\n    torch_empty_cache_steps=5,\n    max_steps=500,\n    warmup_steps=200,\n    logging_steps=1,\n    save_strategy=\"no\",\n    gradient_checkpointing=True,\n    max_seq_length=512,\n    output_dir=\"gemma2_2b\",\n    report_to=\"none\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset,\n        processing_class=tokenizer,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell sets up the training configuration and initializes the `SFTTrainer`.\n\n*   `training_args = SFTConfig(...)`: Creates a configuration object for supervised fine-tuning.\n    *   `per_device_train_batch_size=1`: The batch size for training on each GPU or CPU.\n    *   `torch_empty_cache_steps=5`: How often to empty the PyTorch CUDA cache to free up GPU memory.\n    *   `max_steps=500`: The total number of training steps.\n    *   `warmup_steps=200`: The number of steps for the learning rate warmup phase.\n    *   `logging_steps=1`: How often to log training information.\n    *   `save_strategy=\"no\"`: Disables saving checkpoints during training.\n    *   `gradient_checkpointing=True`: Enables gradient checkpointing to reduce memory usage during backpropagation, potentially at the cost of some computation time.\n    *   `max_seq_length=512`: The maximum length of input sequences the model will process.\n    *   `output_dir=\"gemma2_2b\"`: The directory where training outputs (like logs) will be saved.\n    *   `report_to=\"none\"`: Disables reporting training metrics to platforms like Weights & Biases.\n*   `trainer = SFTTrainer(...)`: Initializes the `SFTTrainer` with the configured settings.\n    *   `model=model`: The fine-tuned model (Gemma with LoRA).\n    *   `args=training_args`: The training configuration.\n    *   `train_dataset=dataset`: The dataset to use for training.\n    *   `processing_class=tokenizer`: The tokenizer to use for preprocessing the data.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell starts the fine-tuning process. The `trainer.train()` method will iterate through the training dataset, update the model's weights (specifically the LoRA adapters in this case), and log the training progress according to the `training_args`.","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/gemma2_2b\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load the base model\nmodel_name_or_path = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2\"#\"google/gemma-2-2b-it\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n\n# 2. Load the PEFT adapter\nadapter_model_name_or_path = \"/kaggle/working/gemma2_2b\"\nmodel = PeftModel.from_pretrained(model, adapter_model_name_or_path)\n\n# 3. Merge the adapter into the base model\nmerged_model = model.merge_and_unload()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/gemma2-2b-gutenberg-merged\", exist_ok=True)\nmerged_model.save_pretrained(\"/kaggle/working/gemma2-2b-gutenberg-merged\")\nprint(\"Merged model saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These cells saves the fine-tuned LoRA adapters and then merges them back into the original base model.\n\n*   `trainer.save_model(\"/kaggle/working/gemma2_2b\")`: Saves the trained LoRA adapters to the specified directory. This saves the changes made during fine-tuning.\n*   The subsequent lines load the base Gemma model again:\n    *   `model_name_or_path = ...`: Specifies the path to the original Gemma model. Note that it's loading from a Kaggle input directory, indicating the base model was likely downloaded and placed there previously.\n    *   `model = AutoModelForCausalLM.from_pretrained(model_name_or_path)`: Loads the base model.\n*   Then, it loads the saved LoRA adapters:\n    *   `adapter_model_name_or_path = \"/kaggle/working/gemma2_2b\"`: Specifies the directory where the LoRA adapters were saved.\n    *   `model = PeftModel.from_pretrained(model, adapter_model_name_or_path)`: Loads the LoRA adapters and applies them to the base model.\n*   Finally, it merges the adapters into the base model:\n    *   `merged_model = model.merge_and_unload()`: Merges the LoRA weights into the base model's weights, creating a standalone fine-tuned model. The `unload()` part releases the LoRA adapters from memory.\n    *   `os.makedirs(...)`: Creates the directory to save the merged model if it doesn't exist.\n    *   `merged_model.save_pretrained(...)`: Saves the fully merged fine-tuned model to the specified directory.\n    *   `print(\"Merged model saved.\")`: Prints a confirmation message.","metadata":{}},{"cell_type":"code","source":"# List of files to copy\nfiles_to_copy = ['/kaggle/working/gemma2_2b/tokenizer_config.json',\n                '/kaggle/working/gemma2_2b/tokenizer.model', '/kaggle/working/gemma2_2b/tokenizer.json',\n                '/kaggle/working/gemma2_2b/special_tokens_map.json']\ndestination_directory = '/kaggle/working/gemma2-2b-gutenberg-merged'\n\n# Ensure the destination directory exists\nif not os.path.exists(destination_directory):\n    os.makedirs(destination_directory)\n\n# Copy each file\nfor file in files_to_copy:\n    shutil.copy(file, destination_directory)\n    print(f\"File '{file}' copied to '{destination_directory}'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell copies necessary tokenizer files from the LoRA adapter's directory to the merged model's directory. The tokenizer configuration is essential for correctly processing text with the fine-tuned model.\n\n*   `files_to_copy = [...]`: Lists the specific tokenizer files needed. These files contain information about the vocabulary, tokenization rules, and special tokens used by the tokenizer.\n*   `destination_directory = ...`: Specifies the directory where the merged model is saved.\n*   The code then checks if the destination directory exists and creates it if it doesn't.\n*   Finally, it iterates through the `files_to_copy` list and uses `shutil.copy()` to copy each file to the destination directory.\n","metadata":{}},{"cell_type":"code","source":"if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()\n\nmodel_version = 1\nkaggle_username = kagglehub.whoami()[\"username\"]\nfine_tuned_model_name = \"gemma2_2b_gutneberg\"\nhandle = f'{kaggle_username}/gemma2/transformers/{fine_tuned_model_name}'\nprint(f\"Handle: {handle}\\n\")\nlocal_model_dir = \"/kaggle/working/gemma2-2b-gutenberg-merged\"\nkagglehub.model_upload(handle, local_model_dir)\nprint(\"Done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell uploads the fine-tuned model to Kaggle Models.\n\n*   `if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:`: Checks if the Kaggle API credentials are set as environment variables. If not, it calls `kagglehub.login()` to prompt for login.\n*   `model_version = 1`: Sets the version of the model being uploaded.\n*   `kaggle_username = kagglehub.whoami()[\"username\"]`: Retrieves your Kaggle username.\n*   `fine_tuned_model_name = \"gemma2_2b_gutneberg\"`: Defines the name you want to give to your fine-tuned model on Kaggle.\n*   `handle = f'{kaggle_username}/gemma2/transformers/{fine_tuned_model_name}'`: Creates the unique identifier (handle) for your model on Kaggle. The format is typically `username/model_group/model_type/model_name`.\n*   `print(f\"Handle: {handle}\\n\")`: Prints the model handle.\n*   `local_model_dir = \"/kaggle/working/gemma2-2b-gutenberg-merged\"`: Specifies the local directory where the merged fine-tuned model is saved.\n*   `kagglehub.model_upload(handle, local_model_dir)`: Uploads the model from the local directory to Kaggle Models using the specified handle.\n*   `print(\"Done!\")`: Prints a confirmation message.\n","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Making the RAG Database\n\nThis section focuses on creating a vector database that will be used for retrieving relevant information to augment the language model's responses. This involves processing a collection of text documents, creating embeddings for text chunks, and indexing these embeddings for efficient similarity search.","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport faiss\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple, Optional, Dict\nimport textwrap\nimport json\nimport pickle\nimport kagglehub\nimport concurrent\nimport gc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell imports the necessary libraries for building the RAG database. Many of these libraries were also used in the fine-tuning section, but some are specific to this part:\n\n*   `sentence_transformers`: Used to create embeddings of text chunks.\n*   `faiss`: The library for efficient similarity search in high-dimensional spaces, which will be used to build and query the vector database.\n*   `typing`: For type hinting, making the code more readable and maintainable.\n*   `concurrent`: For parallel processing of files.\n*   `gc`: For garbage collection, useful for managing memory when dealing with large datasets.","metadata":{}},{"cell_type":"code","source":"paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input/gutenberg-over-70000'):\n    for idx, filename in enumerate(filenames):\n        paths.append(os.path.join(dirname, filename))\n\n    if len(paths) > 150: #only 150 Books for a quik demo, The actuall full data takes a lot of time\n        break\n        \nprint(f\"data lenght:{len(paths)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_path = paths[0]#this is a .csv file so we can exclude it\npaths = paths[1:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unpickle_file(path):\n    \"\"\"Unpickle a single file and process its content.\"\"\"\n    with open(path, \"rb\") as f:\n        try:\n            file = pickle.load(f)\n        except (pickle.UnpicklingError, EOFError, ImportError) as e:\n            print(f\"Error unpickling file at {path}: {e}\")\n            return None\n\n    # Ensure file is a string\n    if isinstance(file, str):\n        file = ' '.join(file.split())\n        pattern = r'[^a-zA-Z0-9 ]'\n        file = re.sub(pattern, ' ', file)\n        file = re.sub(r'\\s+', ' ', file).strip()\n        return file  # Return the processed file\n    else:\n        print(f\"Warning: The file at {path} did not contain a string.\")\n        return None\n\ndef process_files(paths):\n    \"\"\"Process files in parallel using a generator.\"\"\"\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        for processed_file in tqdm(executor.map(unpickle_file, paths), desc=\"Processing files\", unit=\"file\"):\n            if processed_file is not None:\n                yield processed_file  # Yield each processed file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_files = list(process_files(paths))\nprint(\"Books are ready...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These cells loads and preprocesses the text data from the Gutenberg dataset.\n\n*   The first part of the cell uses `os.walk` to recursively go through the directory `/kaggle/input/gutenberg-over-70000` and collect the paths of all files within it. It limits the number of processed files to 150 for a quick demo.\n*   `print(f\"data lenght:{len(paths)}\")`: Prints the number of files found.\n*   `meta_path = paths[0]`: Assumes the first file is a metadata file (likely a CSV) and stores its path.\n*   `paths = paths[1:]`: Removes the metadata file path from the list of text file paths.\n*   The `unpickle_file` function takes a file path, attempts to unpickle its contents (assuming the files are pickled objects), and then preprocesses the text if it's a string. Preprocessing includes:\n    *   Removing non-alphanumeric characters and spaces using regular expressions.\n    *   Removing extra whitespace.\n*   The `process_files` function uses `concurrent.futures.ProcessPoolExecutor` to process the files in parallel, speeding up the process. It uses a generator to yield processed files one by one, which is memory-efficient.\n*   `processed_files = list(process_files(paths))`: Executes the parallel processing and stores the processed text content of the books in a list.\n*   `print(\"Books are ready...\")`: Prints a confirmation message.\nfor more info about the dataset see the dataset [page](https://www.kaggle.com/datasets/jasonheesanglee/gutenberg-over-70000)","metadata":{}},{"cell_type":"code","source":"def chunk_text(text: str, chunk_size: int = 256, overlap: int = 32) -> List[str]:\n    \"\"\"Chunk text with overlap.\"\"\"\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunks.append(text[i:i + chunk_size])\n    return chunks\n\ndef create_embeddings(text_list: List[str]) -> np.ndarray:\n    \"\"\"Create embeddings.\"\"\"\n    embeddings = embedding_model.encode(text_list)\n    return np.array(embeddings).astype('float32')\n\ndef create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n    \"\"\"Create FAISS index.\"\"\"\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n    return index\n\ndef initialize_rag_system(full_text_list: List[str], \n                         chunk_size: int = 256, \n                         overlap: int = 32) -> Tuple[faiss.IndexFlatL2, List[str], Dict[int, int], AutoModelForCausalLM, AutoTokenizer]:\n    \"\"\"\n    Initialize the RAG system by creating embeddings, vector database, and loading models.\n    Should be called once at startup.\n    \n    Args:\n        full_text_list: List of all text documents\n        chunk_size: Size of text chunks\n        overlap: Overlap between chunks\n        \n    Returns:\n        Tuple containing (faiss_index, all_chunks, chunk_to_doc_map, model, tokenizer)\n    \"\"\"\n\n    # Process all documents\n    all_chunks = []\n    chunk_to_doc_map = {}  # Maps chunk index to original document index\n    \n    for doc_idx, text in enumerate(full_text_list):\n        chunks = chunk_text(text, chunk_size, overlap)\n        for chunk in chunks:\n            chunk_to_doc_map[len(all_chunks)] = doc_idx\n            all_chunks.append(chunk)\n    print(\"Chunks loaded successfully...\")\n    print(f\"chunk length:{len(all_chunks)}\")\n    # Create embeddings for all chunks\n    chunk_embeddings = create_embeddings(all_chunks)\n    print(\"Embeddings created successfully...\")\n    # Create FAISS index\n    index = create_faiss_index(chunk_embeddings)\n    print(\"Vector database created successfully...\")\n    \n    return index, all_chunks, chunk_to_doc_map\n\ndef save_rag_system(faiss_index: faiss.IndexFlatL2, \n                   all_chunks: list, \n                   chunk_to_doc_map: dict,\n                   save_dir: str = \"rag_system\"):\n    \"\"\"\n    Save the RAG system components to disk.\n    \n    Args:\n        faiss_index: The FAISS index\n        all_chunks: List of text chunks\n        chunk_to_doc_map: Mapping from chunk index to document index\n        save_dir: Directory to save the components\n    \"\"\"\n    # Create directory if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save FAISS index\n    faiss.write_index(faiss_index, f\"{save_dir}/faiss_index.bin\")\n    \n    # Save chunks and mapping\n    with open(os.path.join(save_dir, \"chunks_and_mapping.pkl\"), \"wb\") as f:\n        pickle.dump({\n            \"all_chunks\": all_chunks,\n            \"chunk_to_doc_map\": chunk_to_doc_map\n        }, f)\n    \n    print(f\"RAG system saved to {save_dir}\")\n\ndef load_rag_system(save_dir: str = \"rag_system\") -> tuple:\n    \"\"\"\n    Load the RAG system components from disk.\n    \n    Args:\n        save_dir: Directory containing the saved components\n        \n    Returns:\n        tuple: (faiss_index, all_chunks, chunk_to_doc_map)\n    \"\"\"\n    # Load FAISS index\n    faiss_index = faiss.read_index(f\"{save_dir}/faiss_index.bin\")\n    \n    # Load chunks and mapping\n    with open(os.path.join(save_dir, \"chunks_and_mapping.pkl\"), \"rb\") as f:\n        data = pickle.load(f)\n        all_chunks = data[\"all_chunks\"]\n        chunk_to_doc_map = data[\"chunk_to_doc_map\"]\n    \n    print(f\"RAG system loaded from {save_dir}\")\n    return faiss_index, all_chunks, chunk_to_doc_map\n\n\n\ndef process_rag_query(\n    query: str,\n    faiss_index: faiss.IndexFlatL2,\n    all_chunks: List[str],\n    chunk_to_doc_map: Dict[int, int],\n    model,\n    tokenizer: AutoTokenizer,\n    embedding_model,\n    top_k: int = 5\n) -> Dict:\n    \"\"\"\n    Process a query using the pre-initialized RAG system with memory optimization.\n    \"\"\"\n    # Get query embedding\n    query_embedding = create_embeddings([query])[0].reshape(1, -1)\n    \n    # Search for relevant chunks\n    distances, chunk_indices = faiss_index.search(query_embedding, top_k)\n    \n    # Clean up query embedding for memory\n    del query_embedding\n    gc.collect()  \n    \n    # Retrieve relevant chunks and their source documents\n    relevant_chunks = [all_chunks[idx] for idx in chunk_indices[0]]\n    source_docs = [chunk_to_doc_map[idx] for idx in chunk_indices[0]]\n    \n    # Create augmented prompt with context\n    context = \"\\n\\n\".join(relevant_chunks)\n    augmented_prompt = f\"\"\"Based on the following context, please answer the question:\n\nContext:\n{context}\n\nQuestion: {query}\"\"\"\n    \n    # Get model response\n    response = get_llm_response(augmented_prompt, model, tokenizer)\n    \n    # Clear any remaining temporary variables\n    #del context, augmented_prompt, relevant_chunks, query, source_docs, distances, chunk_indices\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return {\n        \"query\": query,\n        \"response\": response,\n        \"relevant_chunks\": relevant_chunks,\n        \"source_documents\": source_docs,\n        \"distances\": distances[0].tolist()\n    }\n\ndef get_llm_response(prompt: str, model, tokenizer) -> str:\n    \"\"\"\n    Get response from the language model.\n    \"\"\"\n    system_instructions = \"You are an inteligent assistant, With access to the provided content try to answer using them as well. Note: Try to anwer in the same languages as users message.\"\n    message = f\"{system_instructions}. {prompt}\"\n    messages = [\n        {\"role\": \"user\", \"content\": message},\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    #print(text)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n    \n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    \n    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell defines several functions that are crucial for creating and using the RAG system.\n\n*   `chunk_text(text, chunk_size=256, overlap=32)`: Splits a given text into smaller chunks of a specified `chunk_size`, with a defined `overlap` between consecutive chunks. This overlap helps to maintain context across chunks.\n*   `create_embeddings(text_list)`: Takes a list of text chunks and generates their embeddings using the `embedding_model` (loaded later). It returns the embeddings as a NumPy array of type float32.\n*   `create_faiss_index(embeddings)`: Creates a FAISS index from the given embeddings. `faiss.IndexFlatL2` creates a simple L2 distance-based index. The index is used for efficient nearest neighbor search.\n*   `initialize_rag_system(full_text_list, chunk_size=256, overlap=32)`: This function orchestrates the initial setup of the RAG system.\n    *   It takes a list of full text documents, chunking parameters, and pre-loads the embedding model and the language model.\n    *   It iterates through each document, chunks it using `chunk_text`, and keeps track of which chunk belongs to which original document using `chunk_to_doc_map`.\n    *   It then calls `create_embeddings` to generate embeddings for all the chunks and `create_faiss_index` to build the vector database.\n    *   It returns the FAISS index, the list of all chunks, and the mapping from chunk index to document index.\n*   `save_rag_system(faiss_index, all_chunks, chunk_to_doc_map, save_dir=\"rag_system\")`: Saves the created RAG components (FAISS index, text chunks, and the chunk-to-document mapping) to disk using `faiss.write_index` and `pickle.dump`.\n*   `load_rag_system(save_dir=\"rag_system\")`: Loads the saved RAG components from disk.\n*   `process_rag_query(query, faiss_index, all_chunks, chunk_to_doc_map, model, tokenizer, embedding_model, top_k=5)`: This function processes a user query by:\n    *   Generating an embedding for the query.\n    *   Searching the FAISS index for the `top_k` most similar chunk embeddings.\n    *   Retrieving the corresponding text chunks and the indices of their source documents.\n    *   Constructing an augmented prompt by including the retrieved context along with the original query.\n    *   Generating a response from the language model using the augmented prompt.\n    *   It includes memory management techniques like deleting the query embedding and running garbage collection.\n*   `get_llm_response(prompt, model, tokenizer)`: This function takes a prompt and generates a response from the language model. It formats the prompt as a chat message and uses the tokenizer to prepare the input for the model. It then generates text and decodes the output tokens back into a string.","metadata":{}},{"cell_type":"code","source":"# Load sentence transformer model and LLM and make the vector database once\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nprint(\"Embedding model loaded...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"faiss_index, all_chunks, chunk_to_doc_map = initialize_rag_system(processed_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving the database and chunks so we don't have to do this again.\nsave_rag_system(faiss_index, all_chunks, chunk_to_doc_map, save_dir=\"/kaggle/temp/rag_system\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These cells instantiates the sentence transformer model and initializes and saves the RAG database.\n\n*   `embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")`: Loads the `all-MiniLM-L6-v2` pre-trained sentence embedding model from the `sentence-transformers` library. This model will be used to create vector representations of the text chunks.\n*   `print(\"Embedding model loaded...\")`: Prints a confirmation message.\n*   `faiss_index, all_chunks, chunk_to_doc_map = initialize_rag_system(processed_files)`: Calls the `initialize_rag_system` function, passing the list of processed book texts. This creates the text chunks, generates their embeddings, and builds the FAISS index. The returned values are the FAISS index, the list of all text chunks, and the mapping from chunk index to the original document index.\n*   `save_rag_system(faiss_index, all_chunks, chunk_to_doc_map, save_dir=\"/kaggle/temp/rag_system\")`: Calls the `save_rag_system` function to save the created FAISS index, the list of text chunks, and the chunk-to-document mapping to the `/kaggle/temp/rag_system` directory. This allows you to load the database later without recomputing everything.\n","metadata":{}},{"cell_type":"code","source":"kaggle_username = kagglehub.whoami()[\"username\"]\nhandle = f'{kaggle_username}/Sample_Gutenverg_vector_database_for_Rag'\nlocal_dataset_dir = '/kaggle/temp/rag_system'\n\n# Create a new dataset\nkagglehub.dataset_upload(handle, local_dataset_dir)\n\nprint(\"Database uploaded to kaggle datasets...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell uploads the created RAG database to Kaggle Datasets.\n\n*   `kaggle_username = kagglehub.whoami()[\"username\"]`: Retrieves your Kaggle username using the `kagglehub` library.\n*   `handle = f'{kaggle_username}/Sample_Gutenverg_vector_database_for_Rag'`: Defines the handle (unique identifier) for your dataset on Kaggle. The format is typically `username/dataset_name`.\n*   `local_dataset_dir = '/kaggle/temp/rag_system'`: Specifies the local directory where the saved RAG database components are located.\n*   `kagglehub.dataset_upload(handle, local_dataset_dir)`: Uploads the contents of the local directory to Kaggle Datasets under the specified handle. This makes the RAG database accessible to others or for use in other Kaggle notebooks.\n*   `print(\"Database uploaded to kaggle datasets...\")`: Prints a confirmation message.","metadata":{}},{"cell_type":"markdown","source":"## Part 3: Putting It Together\n\nThis section demonstrates how to combine the fine-tuned language model and the RAG database to answer a query. It loads the fine-tuned model, and then uses the RAG system to retrieve relevant context and generate an informed response.","metadata":{}},{"cell_type":"code","source":"model_name = \"/kaggle/input/gemma2/transformers/gemma2_2b_gutneberg/2\" #The fine-tuned model, I fine-tuned it in another notebook and pushed it to kaggle\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell loads the previously fine-tuned Gemma model and its tokenizer.\n\n*   `model_name = \"/kaggle/input/gemma2/transformers/gemma2_2b_gutneberg/2\"`: Specifies the path to the directory containing the saved fine-tuned Gemma model. The comment indicates that this model was fine-tuned in another notebook and uploaded to Kaggle.\n*   `model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")`: Loads the fine-tuned Gemma model.\n    *   `torch_dtype=\"auto\"`: Automatically determines the appropriate data type for the model weights (e.g., float16, bfloat16) based on hardware availability.\n    *   `device_map=\"auto\"`: Automatically places the model's layers on the available devices (GPU if available, otherwise CPU).\n*   `tokenizer = AutoTokenizer.from_pretrained(model_name)`: Loads the tokenizer associated with the fine-tuned Gemma model. It's crucial to use the same tokenizer that was used during fine-tuning.","metadata":{}},{"cell_type":"code","source":"query = \"Descrivi l'importanza della famiglia nella cultura italiana.\"#Describe the importance of family in Italian culture.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = process_rag_query(\n            query=query,\n            faiss_index=faiss_index,\n            all_chunks=all_chunks,\n            chunk_to_doc_map=chunk_to_doc_map,\n            model=model,\n            tokenizer=tokenizer,\n            embedding_model=embedding_model\n        )\nresponse = results[\"response\"]\nrelevant_chunks = results[\"relevant_chunks\"]\nsource_documents = results[\"source_documents\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"response:{response}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell demonstrates how to use the RAG system to answer a specific query.\n\n*   `query = \"Descrivi l'importanza della famiglia nella cultura italiana.\"`: Defines the query in Italian (with an English translation in the comment).\n*   `results = process_rag_query(...)`: Calls the `process_rag_query` function with the following arguments:\n    *   `query`: The user's question.\n    *   `faiss_index`: The loaded FAISS index containing the embeddings of the text chunks.\n    *   `all_chunks`: The list of all text chunks extracted from the Gutenberg books.\n    *   `chunk_to_doc_map`: The mapping from chunk index to the original document index.\n    *   `model`: The loaded fine-tuned Gemma model.\n    *   `tokenizer`: The tokenizer for the Gemma model.\n    *   `embedding_model`: The loaded sentence transformer model used to create the embeddings.\n*   `response = results[\"response\"]`: Extracts the generated answer from the `results` dictionary.\n*   `relevant_chunks = results[\"relevant_chunks\"]`: Extracts the list of text chunks that were deemed most relevant to the query.\n*   `source_documents = results[\"source_documents\"]`: Extracts the indices of the original documents from which the relevant chunks were retrieved.","metadata":{}},{"cell_type":"markdown","source":"## Future Path\n\n*   **Improving the RAG system:**\n    *   Experimenting with different embedding models for better semantic representation.\n    *   Trying different FAISS index types for improved search efficiency or accuracy.\n    *   Implementing more sophisticated chunking strategies.\n    *   Adding mechanisms for re-ranking retrieved chunks.\n*   **Enhancing the fine-tuning process:**\n    *   Training for more steps or epochs.\n    *   Using a larger and more diverse dataset for fine-tuning.\n    *   Experimenting with different LoRA configurations or other PEFT methods.\n*   **Improving the prompt engineering:**\n    *   Trying different prompt formats or instructions to guide the model's response.\n*   **Adding evaluation metrics:**\n    *   Implementing metrics to quantitatively assess the quality of the generated responses.\n*   **Deploying the system:**\n    *   Packaging the model and RAG database for deployment in a web application or API.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis section summarizes the work done in the notebook and highlights the key achievements. It often reiterates the goal of the project and briefly describes the steps taken to achieve it. It might also mention any limitations of the current implementation and the potential impact or applications of the work. For example:\n\n*   The notebook successfully demonstrates the creation of a Retrieval-Augmented Generation (RAG) system using a fine-tuned Gemma 2B model and a vector database built from Project Gutenberg texts.\n*   The fine-tuning process adapts the base model to better understand and generate text related to the training data.\n*   The RAG database allows the model to access and incorporate relevant information from a large corpus of text, leading to more informed and contextually appropriate responses.\n*   The system can be further improved by exploring different techniques in embedding, indexing, and fine-tuning.\n*   This type of system can be valuable for various applications, such as question answering, information retrieval, and content generation.","metadata":{}}]}